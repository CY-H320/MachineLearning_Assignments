{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n44rMWcLS-BY"
   },
   "source": [
    "# Week 10: Colab Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCMvaDkMTJkT"
   },
   "source": [
    "# I. Introduction\n",
    "In this exercise, we apply CNN to MNIST data to classify the hand written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "id": "2jxq00nbuCwt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdXkw_9pkfn5"
   },
   "source": [
    "# Data Loading\n",
    "Load the data from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "id": "PoUAesyDuL0n"
   },
   "outputs": [],
   "source": [
    "# Run this once to load the train and test data straight into a dataloader class\n",
    "# that will provide the batches\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRXOY0Tzkfn6"
   },
   "source": [
    "# Visualize dataset sample\n",
    "Show some sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "id": "boEAxlB5uPZx"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's draw some of the training data\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_94InEd1TkC6"
   },
   "source": [
    "# II. Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "id": "y_kwOzXQuWNK"
   },
   "outputs": [],
   "source": [
    "from os import X_OK\n",
    "\n",
    "# This class implements a minimal network (which still does okay)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Valid convolution, 1 channel in, 2 channels out, stride 1, kernel size = 3\n",
    "        self.conv1 = nn.Conv2d(1, 2, kernel_size=3)\n",
    "        # Dropout for convolutions\n",
    "        self.drop = nn.Dropout2d()\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(338, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = F.relu(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "id": "d4Ue45Pnf8gZ"
   },
   "outputs": [],
   "source": [
    "# TODO: Change above Net to Net2 class to implement\n",
    "\n",
    "# 1. A valid convolution with kernel size 5, 1 input channel, and 10 output channels\n",
    "# 2. A max pooling operation over a 2x2 area\n",
    "# 3. A Relu\n",
    "# 4. A valid convolution with kernel size 5, 10 input channels, and 20 output channels\n",
    "# 5. A 2D Dropout layer\n",
    "# 6. A max pooling operation over a 2x2 area\n",
    "# 7. A relu\n",
    "# 8. A flattening operation\n",
    "# 9. A fully connected layer mapping from (whatever dimensions we are at-- find out using .shape) to 50\n",
    "# 10. A ReLU\n",
    "# 11. A fully connected layer mapping from 50 to 10 dimensions\n",
    "# 12. A softmax function.\n",
    "\n",
    "\n",
    "# New class implementing the specified architecture\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        # First convolutional block: 1 input channel, 10 output channels, kernel size 5\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)  # 2x2 max pooling\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Second convolutional block: 10 input channels, 20 output channels, kernel size 5\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.drop = nn.Dropout2d(p=0.4)  # Dropout layer after the second convolution\n",
    "\n",
    "        # I added a third convolutional block for better performance\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size=3)\n",
    "        self.conv3_bn = nn.BatchNorm2d(30)\n",
    "        \n",
    "        # Flattening and fully connected layers\n",
    "        # We'll determine input size for the first fully connected layer dynamically in forward\n",
    "        self.fc1 = None  # Placeholder, to be initialized in the forward method\n",
    "        self.fc2 = nn.Linear(50, 10)  # Final output to 10 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block: Conv -> ReLU -> Max Pool\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Second conv block: Conv -> ReLU -> Max Pool -> Dropout\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # Third conv block: Conv -> BatchNorm -> ReLU\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        \n",
    "        # Flattening operation\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Initialize the first fully connected layer dynamically (if not already done)\n",
    "        if self.fc1 is None:\n",
    "            input_dim = x.size(1)\n",
    "            self.fc1 = nn.Linear(input_dim, 50)\n",
    "            nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')  # Optional weight initialization\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "id": "9sN5hsK2uan8"
   },
   "outputs": [],
   "source": [
    "# He initialization of weights\n",
    "def weights_init(layer_in):\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "    nn.init.kaiming_uniform_(layer_in.weight)\n",
    "    layer_in.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "id": "2pBDgYp2ufUi"
   },
   "outputs": [],
   "source": [
    "# Main training routine\n",
    "# TODO: Read it and understand what it does, you would need to implement it in the next colab HW\n",
    "\n",
    "# define the training of the model for a single epoch\n",
    "def train(epoch, model):\n",
    "  model.train() # set the model to training mode\n",
    "  # Get each, iterate over batches of data from the train_loader\n",
    "  for batch_idx, (data, target) in enumerate(train_loader): \n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    output = model(data) # data is passed through the model to get predictions\n",
    "    loss = F.nll_loss(output, target)  # using the negative log-likelihood\n",
    "    loss.backward() # compute the gradients of the model’s parameters with respect to the loss using backpropagation\n",
    "    optimizer.step() # update the model's parameters (weights and biases) using the optimizer\n",
    "    # Store results\n",
    "    if batch_idx % 10 == 0:\n",
    "      # Every 10th batch, print the current loss and training progress\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct = pred.eq(target.data.view_as(pred)).sum()\n",
    "      print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "id": "Xr6yXzWduhbU"
   },
   "outputs": [],
   "source": [
    "# Run on test data\n",
    "# TODO: Read it and understand what it does, you would need to implement it in the next colab HW\n",
    "\n",
    "def test(model):\n",
    "  model.eval() # set the model to evaluation mode, dropout is disabled, and batch normalization uses the running statistics rather than batch statistics\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad(): # prevent PyTorch from calculating gradients during inference\n",
    "    for data, target in test_loader:\n",
    "      output = model(data)\n",
    "      # compute the negative log-likelihood loss between the model’s output (output) and the true labels (target)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      # select the class with the highest score for each sample in the batch\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      # count correct predictions\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  # average loss per example\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "  return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "id": "EVUrbYiamki8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/03rd9m156sx7chkdv0lzxt240000gn/T/ipykernel_12892/1265220663.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3277, Accuracy: 959/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/60000]\tLoss: 2.397596\n",
      "Train Epoch: 1 [640/60000]\tLoss: 2.314329\n",
      "Train Epoch: 1 [1280/60000]\tLoss: 2.313947\n",
      "Train Epoch: 1 [1920/60000]\tLoss: 2.287558\n",
      "Train Epoch: 1 [2560/60000]\tLoss: 2.202006\n",
      "Train Epoch: 1 [3200/60000]\tLoss: 2.182694\n",
      "Train Epoch: 1 [3840/60000]\tLoss: 2.251300\n",
      "Train Epoch: 1 [4480/60000]\tLoss: 2.221143\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 2.041233\n",
      "Train Epoch: 1 [5760/60000]\tLoss: 2.026867\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 1.910887\n",
      "Train Epoch: 1 [7040/60000]\tLoss: 1.915868\n",
      "Train Epoch: 1 [7680/60000]\tLoss: 1.655506\n",
      "Train Epoch: 1 [8320/60000]\tLoss: 1.730156\n",
      "Train Epoch: 1 [8960/60000]\tLoss: 1.589088\n",
      "Train Epoch: 1 [9600/60000]\tLoss: 1.595941\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 1.646502\n",
      "Train Epoch: 1 [10880/60000]\tLoss: 1.620031\n",
      "Train Epoch: 1 [11520/60000]\tLoss: 1.555584\n",
      "Train Epoch: 1 [12160/60000]\tLoss: 1.482594\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 1.740577\n",
      "Train Epoch: 1 [13440/60000]\tLoss: 1.460530\n",
      "Train Epoch: 1 [14080/60000]\tLoss: 1.320450\n",
      "Train Epoch: 1 [14720/60000]\tLoss: 1.379568\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 1.483611\n",
      "Train Epoch: 1 [16000/60000]\tLoss: 1.428045\n",
      "Train Epoch: 1 [16640/60000]\tLoss: 1.295487\n",
      "Train Epoch: 1 [17280/60000]\tLoss: 1.392124\n",
      "Train Epoch: 1 [17920/60000]\tLoss: 1.336647\n",
      "Train Epoch: 1 [18560/60000]\tLoss: 1.485596\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 1.336313\n",
      "Train Epoch: 1 [19840/60000]\tLoss: 1.536413\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 1.480452\n",
      "Train Epoch: 1 [21120/60000]\tLoss: 1.279289\n",
      "Train Epoch: 1 [21760/60000]\tLoss: 1.545602\n",
      "Train Epoch: 1 [22400/60000]\tLoss: 1.308126\n",
      "Train Epoch: 1 [23040/60000]\tLoss: 1.254732\n",
      "Train Epoch: 1 [23680/60000]\tLoss: 1.425135\n",
      "Train Epoch: 1 [24320/60000]\tLoss: 1.404511\n",
      "Train Epoch: 1 [24960/60000]\tLoss: 1.361461\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 1.268047\n",
      "Train Epoch: 1 [26240/60000]\tLoss: 1.571826\n",
      "Train Epoch: 1 [26880/60000]\tLoss: 1.280354\n",
      "Train Epoch: 1 [27520/60000]\tLoss: 1.359819\n",
      "Train Epoch: 1 [28160/60000]\tLoss: 1.261041\n",
      "Train Epoch: 1 [28800/60000]\tLoss: 1.349852\n",
      "Train Epoch: 1 [29440/60000]\tLoss: 1.407607\n",
      "Train Epoch: 1 [30080/60000]\tLoss: 1.412494\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 1.480536\n",
      "Train Epoch: 1 [31360/60000]\tLoss: 1.476069\n",
      "Train Epoch: 1 [32000/60000]\tLoss: 1.288327\n",
      "Train Epoch: 1 [32640/60000]\tLoss: 1.380821\n",
      "Train Epoch: 1 [33280/60000]\tLoss: 1.522667\n",
      "Train Epoch: 1 [33920/60000]\tLoss: 1.110130\n",
      "Train Epoch: 1 [34560/60000]\tLoss: 1.141881\n",
      "Train Epoch: 1 [35200/60000]\tLoss: 1.488034\n",
      "Train Epoch: 1 [35840/60000]\tLoss: 1.257423\n",
      "Train Epoch: 1 [36480/60000]\tLoss: 0.895752\n",
      "Train Epoch: 1 [37120/60000]\tLoss: 1.520676\n",
      "Train Epoch: 1 [37760/60000]\tLoss: 1.357782\n",
      "Train Epoch: 1 [38400/60000]\tLoss: 1.472277\n",
      "Train Epoch: 1 [39040/60000]\tLoss: 1.321719\n",
      "Train Epoch: 1 [39680/60000]\tLoss: 1.492197\n",
      "Train Epoch: 1 [40320/60000]\tLoss: 1.231010\n",
      "Train Epoch: 1 [40960/60000]\tLoss: 1.515968\n",
      "Train Epoch: 1 [41600/60000]\tLoss: 1.425685\n",
      "Train Epoch: 1 [42240/60000]\tLoss: 1.387338\n",
      "Train Epoch: 1 [42880/60000]\tLoss: 1.620343\n",
      "Train Epoch: 1 [43520/60000]\tLoss: 1.522348\n",
      "Train Epoch: 1 [44160/60000]\tLoss: 1.367133\n",
      "Train Epoch: 1 [44800/60000]\tLoss: 1.302413\n",
      "Train Epoch: 1 [45440/60000]\tLoss: 1.213323\n",
      "Train Epoch: 1 [46080/60000]\tLoss: 1.318906\n",
      "Train Epoch: 1 [46720/60000]\tLoss: 1.147672\n",
      "Train Epoch: 1 [47360/60000]\tLoss: 1.412199\n",
      "Train Epoch: 1 [48000/60000]\tLoss: 1.345219\n",
      "Train Epoch: 1 [48640/60000]\tLoss: 1.176267\n",
      "Train Epoch: 1 [49280/60000]\tLoss: 1.278370\n",
      "Train Epoch: 1 [49920/60000]\tLoss: 1.237326\n",
      "Train Epoch: 1 [50560/60000]\tLoss: 1.252987\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 1.124288\n",
      "Train Epoch: 1 [51840/60000]\tLoss: 1.075293\n",
      "Train Epoch: 1 [52480/60000]\tLoss: 1.447951\n",
      "Train Epoch: 1 [53120/60000]\tLoss: 1.324571\n",
      "Train Epoch: 1 [53760/60000]\tLoss: 1.414881\n",
      "Train Epoch: 1 [54400/60000]\tLoss: 1.323467\n",
      "Train Epoch: 1 [55040/60000]\tLoss: 1.137133\n",
      "Train Epoch: 1 [55680/60000]\tLoss: 1.338251\n",
      "Train Epoch: 1 [56320/60000]\tLoss: 1.321047\n",
      "Train Epoch: 1 [56960/60000]\tLoss: 1.285709\n",
      "Train Epoch: 1 [57600/60000]\tLoss: 1.450892\n",
      "Train Epoch: 1 [58240/60000]\tLoss: 1.478906\n",
      "Train Epoch: 1 [58880/60000]\tLoss: 1.189751\n",
      "Train Epoch: 1 [59520/60000]\tLoss: 1.284945\n",
      "Train Epoch: 2 [0/60000]\tLoss: 1.386023\n",
      "Train Epoch: 2 [640/60000]\tLoss: 1.225837\n",
      "Train Epoch: 2 [1280/60000]\tLoss: 1.444272\n",
      "Train Epoch: 2 [1920/60000]\tLoss: 1.446242\n",
      "Train Epoch: 2 [2560/60000]\tLoss: 1.576784\n",
      "Train Epoch: 2 [3200/60000]\tLoss: 1.529716\n",
      "Train Epoch: 2 [3840/60000]\tLoss: 1.326583\n",
      "Train Epoch: 2 [4480/60000]\tLoss: 1.262314\n",
      "Train Epoch: 2 [5120/60000]\tLoss: 1.877827\n",
      "Train Epoch: 2 [5760/60000]\tLoss: 1.130113\n",
      "Train Epoch: 2 [6400/60000]\tLoss: 1.063616\n",
      "Train Epoch: 2 [7040/60000]\tLoss: 1.189099\n",
      "Train Epoch: 2 [7680/60000]\tLoss: 1.259756\n",
      "Train Epoch: 2 [8320/60000]\tLoss: 1.184816\n",
      "Train Epoch: 2 [8960/60000]\tLoss: 1.541699\n",
      "Train Epoch: 2 [9600/60000]\tLoss: 1.425544\n",
      "Train Epoch: 2 [10240/60000]\tLoss: 0.958014\n",
      "Train Epoch: 2 [10880/60000]\tLoss: 1.524896\n",
      "Train Epoch: 2 [11520/60000]\tLoss: 0.992038\n",
      "Train Epoch: 2 [12160/60000]\tLoss: 1.476621\n",
      "Train Epoch: 2 [12800/60000]\tLoss: 1.343493\n",
      "Train Epoch: 2 [13440/60000]\tLoss: 1.359618\n",
      "Train Epoch: 2 [14080/60000]\tLoss: 1.191073\n",
      "Train Epoch: 2 [14720/60000]\tLoss: 1.006467\n",
      "Train Epoch: 2 [15360/60000]\tLoss: 1.182106\n",
      "Train Epoch: 2 [16000/60000]\tLoss: 0.942328\n",
      "Train Epoch: 2 [16640/60000]\tLoss: 1.164981\n",
      "Train Epoch: 2 [17280/60000]\tLoss: 1.428728\n",
      "Train Epoch: 2 [17920/60000]\tLoss: 1.039760\n",
      "Train Epoch: 2 [18560/60000]\tLoss: 1.027856\n",
      "Train Epoch: 2 [19200/60000]\tLoss: 0.968151\n",
      "Train Epoch: 2 [19840/60000]\tLoss: 0.997098\n",
      "Train Epoch: 2 [20480/60000]\tLoss: 0.955860\n",
      "Train Epoch: 2 [21120/60000]\tLoss: 0.768362\n",
      "Train Epoch: 2 [21760/60000]\tLoss: 0.807092\n",
      "Train Epoch: 2 [22400/60000]\tLoss: 0.668509\n",
      "Train Epoch: 2 [23040/60000]\tLoss: 1.024185\n",
      "Train Epoch: 2 [23680/60000]\tLoss: 0.994228\n",
      "Train Epoch: 2 [24320/60000]\tLoss: 0.984294\n",
      "Train Epoch: 2 [24960/60000]\tLoss: 1.301103\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 0.816496\n",
      "Train Epoch: 2 [26240/60000]\tLoss: 1.285725\n",
      "Train Epoch: 2 [26880/60000]\tLoss: 0.900834\n",
      "Train Epoch: 2 [27520/60000]\tLoss: 0.931342\n",
      "Train Epoch: 2 [28160/60000]\tLoss: 1.193097\n",
      "Train Epoch: 2 [28800/60000]\tLoss: 1.170685\n",
      "Train Epoch: 2 [29440/60000]\tLoss: 0.882655\n",
      "Train Epoch: 2 [30080/60000]\tLoss: 1.095673\n",
      "Train Epoch: 2 [30720/60000]\tLoss: 1.221097\n",
      "Train Epoch: 2 [31360/60000]\tLoss: 1.094121\n",
      "Train Epoch: 2 [32000/60000]\tLoss: 0.996491\n",
      "Train Epoch: 2 [32640/60000]\tLoss: 1.003186\n",
      "Train Epoch: 2 [33280/60000]\tLoss: 1.120885\n",
      "Train Epoch: 2 [33920/60000]\tLoss: 1.347654\n",
      "Train Epoch: 2 [34560/60000]\tLoss: 0.812063\n",
      "Train Epoch: 2 [35200/60000]\tLoss: 0.794359\n",
      "Train Epoch: 2 [35840/60000]\tLoss: 1.026898\n",
      "Train Epoch: 2 [36480/60000]\tLoss: 0.508919\n",
      "Train Epoch: 2 [37120/60000]\tLoss: 1.007845\n",
      "Train Epoch: 2 [37760/60000]\tLoss: 0.737861\n",
      "Train Epoch: 2 [38400/60000]\tLoss: 0.936188\n",
      "Train Epoch: 2 [39040/60000]\tLoss: 0.659931\n",
      "Train Epoch: 2 [39680/60000]\tLoss: 1.041129\n",
      "Train Epoch: 2 [40320/60000]\tLoss: 0.979984\n",
      "Train Epoch: 2 [40960/60000]\tLoss: 1.012164\n",
      "Train Epoch: 2 [41600/60000]\tLoss: 0.973963\n",
      "Train Epoch: 2 [42240/60000]\tLoss: 1.055764\n",
      "Train Epoch: 2 [42880/60000]\tLoss: 0.725238\n",
      "Train Epoch: 2 [43520/60000]\tLoss: 0.673388\n",
      "Train Epoch: 2 [44160/60000]\tLoss: 0.991399\n",
      "Train Epoch: 2 [44800/60000]\tLoss: 0.992569\n",
      "Train Epoch: 2 [45440/60000]\tLoss: 0.936208\n",
      "Train Epoch: 2 [46080/60000]\tLoss: 0.812483\n",
      "Train Epoch: 2 [46720/60000]\tLoss: 0.779413\n",
      "Train Epoch: 2 [47360/60000]\tLoss: 1.049713\n",
      "Train Epoch: 2 [48000/60000]\tLoss: 1.206995\n",
      "Train Epoch: 2 [48640/60000]\tLoss: 0.903669\n",
      "Train Epoch: 2 [49280/60000]\tLoss: 1.185985\n",
      "Train Epoch: 2 [49920/60000]\tLoss: 0.998061\n",
      "Train Epoch: 2 [50560/60000]\tLoss: 1.081303\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.775962\n",
      "Train Epoch: 2 [51840/60000]\tLoss: 0.761366\n",
      "Train Epoch: 2 [52480/60000]\tLoss: 1.070236\n",
      "Train Epoch: 2 [53120/60000]\tLoss: 1.002080\n",
      "Train Epoch: 2 [53760/60000]\tLoss: 0.739340\n",
      "Train Epoch: 2 [54400/60000]\tLoss: 0.875085\n",
      "Train Epoch: 2 [55040/60000]\tLoss: 0.868746\n",
      "Train Epoch: 2 [55680/60000]\tLoss: 0.856648\n",
      "Train Epoch: 2 [56320/60000]\tLoss: 0.780108\n",
      "Train Epoch: 2 [56960/60000]\tLoss: 0.895437\n",
      "Train Epoch: 2 [57600/60000]\tLoss: 0.850002\n",
      "Train Epoch: 2 [58240/60000]\tLoss: 0.834839\n",
      "Train Epoch: 2 [58880/60000]\tLoss: 0.868017\n",
      "Train Epoch: 2 [59520/60000]\tLoss: 0.899481\n",
      "Train Epoch: 3 [0/60000]\tLoss: 0.963101\n",
      "Train Epoch: 3 [640/60000]\tLoss: 0.705618\n",
      "Train Epoch: 3 [1280/60000]\tLoss: 0.583203\n",
      "Train Epoch: 3 [1920/60000]\tLoss: 0.720926\n",
      "Train Epoch: 3 [2560/60000]\tLoss: 0.893482\n",
      "Train Epoch: 3 [3200/60000]\tLoss: 0.807539\n",
      "Train Epoch: 3 [3840/60000]\tLoss: 1.145228\n",
      "Train Epoch: 3 [4480/60000]\tLoss: 1.117886\n",
      "Train Epoch: 3 [5120/60000]\tLoss: 0.836429\n",
      "Train Epoch: 3 [5760/60000]\tLoss: 1.120923\n",
      "Train Epoch: 3 [6400/60000]\tLoss: 0.695508\n",
      "Train Epoch: 3 [7040/60000]\tLoss: 0.774149\n",
      "Train Epoch: 3 [7680/60000]\tLoss: 0.771787\n",
      "Train Epoch: 3 [8320/60000]\tLoss: 0.655439\n",
      "Train Epoch: 3 [8960/60000]\tLoss: 0.945414\n",
      "Train Epoch: 3 [9600/60000]\tLoss: 0.805466\n",
      "Train Epoch: 3 [10240/60000]\tLoss: 0.735815\n",
      "Train Epoch: 3 [10880/60000]\tLoss: 0.963901\n",
      "Train Epoch: 3 [11520/60000]\tLoss: 0.718027\n",
      "Train Epoch: 3 [12160/60000]\tLoss: 0.616326\n",
      "Train Epoch: 3 [12800/60000]\tLoss: 0.694563\n",
      "Train Epoch: 3 [13440/60000]\tLoss: 0.851360\n",
      "Train Epoch: 3 [14080/60000]\tLoss: 0.785477\n",
      "Train Epoch: 3 [14720/60000]\tLoss: 1.009785\n",
      "Train Epoch: 3 [15360/60000]\tLoss: 0.688336\n",
      "Train Epoch: 3 [16000/60000]\tLoss: 0.758222\n",
      "Train Epoch: 3 [16640/60000]\tLoss: 1.134126\n",
      "Train Epoch: 3 [17280/60000]\tLoss: 0.756604\n",
      "Train Epoch: 3 [17920/60000]\tLoss: 0.719483\n",
      "Train Epoch: 3 [18560/60000]\tLoss: 0.897288\n",
      "Train Epoch: 3 [19200/60000]\tLoss: 1.061001\n",
      "Train Epoch: 3 [19840/60000]\tLoss: 0.737751\n",
      "Train Epoch: 3 [20480/60000]\tLoss: 0.988863\n",
      "Train Epoch: 3 [21120/60000]\tLoss: 0.748760\n",
      "Train Epoch: 3 [21760/60000]\tLoss: 0.821607\n",
      "Train Epoch: 3 [22400/60000]\tLoss: 0.785065\n",
      "Train Epoch: 3 [23040/60000]\tLoss: 1.163881\n",
      "Train Epoch: 3 [23680/60000]\tLoss: 0.816571\n",
      "Train Epoch: 3 [24320/60000]\tLoss: 0.940342\n",
      "Train Epoch: 3 [24960/60000]\tLoss: 1.012276\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 0.720374\n",
      "Train Epoch: 3 [26240/60000]\tLoss: 0.638911\n",
      "Train Epoch: 3 [26880/60000]\tLoss: 0.928250\n",
      "Train Epoch: 3 [27520/60000]\tLoss: 0.760327\n",
      "Train Epoch: 3 [28160/60000]\tLoss: 1.005918\n",
      "Train Epoch: 3 [28800/60000]\tLoss: 0.970788\n",
      "Train Epoch: 3 [29440/60000]\tLoss: 0.582694\n",
      "Train Epoch: 3 [30080/60000]\tLoss: 0.982376\n",
      "Train Epoch: 3 [30720/60000]\tLoss: 0.934166\n",
      "Train Epoch: 3 [31360/60000]\tLoss: 0.883536\n",
      "Train Epoch: 3 [32000/60000]\tLoss: 0.934391\n",
      "Train Epoch: 3 [32640/60000]\tLoss: 0.942877\n",
      "Train Epoch: 3 [33280/60000]\tLoss: 0.683404\n",
      "Train Epoch: 3 [33920/60000]\tLoss: 0.697643\n",
      "Train Epoch: 3 [34560/60000]\tLoss: 0.823249\n",
      "Train Epoch: 3 [35200/60000]\tLoss: 1.152308\n",
      "Train Epoch: 3 [35840/60000]\tLoss: 1.189673\n",
      "Train Epoch: 3 [36480/60000]\tLoss: 0.710387\n",
      "Train Epoch: 3 [37120/60000]\tLoss: 0.854698\n",
      "Train Epoch: 3 [37760/60000]\tLoss: 0.809064\n",
      "Train Epoch: 3 [38400/60000]\tLoss: 0.844145\n",
      "Train Epoch: 3 [39040/60000]\tLoss: 0.456060\n",
      "Train Epoch: 3 [39680/60000]\tLoss: 0.868881\n",
      "Train Epoch: 3 [40320/60000]\tLoss: 0.731135\n",
      "Train Epoch: 3 [40960/60000]\tLoss: 0.682219\n",
      "Train Epoch: 3 [41600/60000]\tLoss: 0.951468\n",
      "Train Epoch: 3 [42240/60000]\tLoss: 0.892090\n",
      "Train Epoch: 3 [42880/60000]\tLoss: 0.942439\n",
      "Train Epoch: 3 [43520/60000]\tLoss: 0.893267\n",
      "Train Epoch: 3 [44160/60000]\tLoss: 0.724332\n",
      "Train Epoch: 3 [44800/60000]\tLoss: 0.702940\n",
      "Train Epoch: 3 [45440/60000]\tLoss: 0.765333\n",
      "Train Epoch: 3 [46080/60000]\tLoss: 0.862574\n",
      "Train Epoch: 3 [46720/60000]\tLoss: 1.053518\n",
      "Train Epoch: 3 [47360/60000]\tLoss: 1.088219\n",
      "Train Epoch: 3 [48000/60000]\tLoss: 0.762335\n",
      "Train Epoch: 3 [48640/60000]\tLoss: 0.909031\n",
      "Train Epoch: 3 [49280/60000]\tLoss: 0.817530\n",
      "Train Epoch: 3 [49920/60000]\tLoss: 0.878465\n",
      "Train Epoch: 3 [50560/60000]\tLoss: 0.777773\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.713342\n",
      "Train Epoch: 3 [51840/60000]\tLoss: 0.874221\n",
      "Train Epoch: 3 [52480/60000]\tLoss: 0.912612\n",
      "Train Epoch: 3 [53120/60000]\tLoss: 0.810063\n",
      "Train Epoch: 3 [53760/60000]\tLoss: 0.966095\n",
      "Train Epoch: 3 [54400/60000]\tLoss: 0.818370\n",
      "Train Epoch: 3 [55040/60000]\tLoss: 1.218127\n",
      "Train Epoch: 3 [55680/60000]\tLoss: 0.752203\n",
      "Train Epoch: 3 [56320/60000]\tLoss: 0.689356\n",
      "Train Epoch: 3 [56960/60000]\tLoss: 0.613646\n",
      "Train Epoch: 3 [57600/60000]\tLoss: 0.852597\n",
      "Train Epoch: 3 [58240/60000]\tLoss: 0.934373\n",
      "Train Epoch: 3 [58880/60000]\tLoss: 0.642072\n",
      "Train Epoch: 3 [59520/60000]\tLoss: 1.027461\n",
      "Train Epoch: 4 [0/60000]\tLoss: 0.881362\n",
      "Train Epoch: 4 [640/60000]\tLoss: 0.750627\n",
      "Train Epoch: 4 [1280/60000]\tLoss: 1.000898\n",
      "Train Epoch: 4 [1920/60000]\tLoss: 1.055902\n",
      "Train Epoch: 4 [2560/60000]\tLoss: 0.929436\n",
      "Train Epoch: 4 [3200/60000]\tLoss: 0.526358\n",
      "Train Epoch: 4 [3840/60000]\tLoss: 1.147899\n",
      "Train Epoch: 4 [4480/60000]\tLoss: 0.813360\n",
      "Train Epoch: 4 [5120/60000]\tLoss: 0.853035\n",
      "Train Epoch: 4 [5760/60000]\tLoss: 0.961081\n",
      "Train Epoch: 4 [6400/60000]\tLoss: 0.772923\n",
      "Train Epoch: 4 [7040/60000]\tLoss: 0.796366\n",
      "Train Epoch: 4 [7680/60000]\tLoss: 1.037531\n",
      "Train Epoch: 4 [8320/60000]\tLoss: 0.816848\n",
      "Train Epoch: 4 [8960/60000]\tLoss: 1.009919\n",
      "Train Epoch: 4 [9600/60000]\tLoss: 0.816473\n",
      "Train Epoch: 4 [10240/60000]\tLoss: 0.649756\n",
      "Train Epoch: 4 [10880/60000]\tLoss: 0.999916\n",
      "Train Epoch: 4 [11520/60000]\tLoss: 0.849419\n",
      "Train Epoch: 4 [12160/60000]\tLoss: 0.893560\n",
      "Train Epoch: 4 [12800/60000]\tLoss: 0.940973\n",
      "Train Epoch: 4 [13440/60000]\tLoss: 1.124010\n",
      "Train Epoch: 4 [14080/60000]\tLoss: 0.701703\n",
      "Train Epoch: 4 [14720/60000]\tLoss: 0.726999\n",
      "Train Epoch: 4 [15360/60000]\tLoss: 0.815226\n",
      "Train Epoch: 4 [16000/60000]\tLoss: 0.841498\n",
      "Train Epoch: 4 [16640/60000]\tLoss: 0.783849\n",
      "Train Epoch: 4 [17280/60000]\tLoss: 0.948018\n",
      "Train Epoch: 4 [17920/60000]\tLoss: 0.986721\n",
      "Train Epoch: 4 [18560/60000]\tLoss: 0.821940\n",
      "Train Epoch: 4 [19200/60000]\tLoss: 0.786023\n",
      "Train Epoch: 4 [19840/60000]\tLoss: 0.915473\n",
      "Train Epoch: 4 [20480/60000]\tLoss: 0.781768\n",
      "Train Epoch: 4 [21120/60000]\tLoss: 0.965517\n",
      "Train Epoch: 4 [21760/60000]\tLoss: 0.763750\n",
      "Train Epoch: 4 [22400/60000]\tLoss: 0.795655\n",
      "Train Epoch: 4 [23040/60000]\tLoss: 0.716124\n",
      "Train Epoch: 4 [23680/60000]\tLoss: 0.865267\n",
      "Train Epoch: 4 [24320/60000]\tLoss: 1.006397\n",
      "Train Epoch: 4 [24960/60000]\tLoss: 0.918241\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.698927\n",
      "Train Epoch: 4 [26240/60000]\tLoss: 0.973525\n",
      "Train Epoch: 4 [26880/60000]\tLoss: 0.837844\n",
      "Train Epoch: 4 [27520/60000]\tLoss: 0.853207\n",
      "Train Epoch: 4 [28160/60000]\tLoss: 0.735450\n",
      "Train Epoch: 4 [28800/60000]\tLoss: 0.766839\n",
      "Train Epoch: 4 [29440/60000]\tLoss: 1.162688\n",
      "Train Epoch: 4 [30080/60000]\tLoss: 0.927932\n",
      "Train Epoch: 4 [30720/60000]\tLoss: 0.943461\n",
      "Train Epoch: 4 [31360/60000]\tLoss: 0.905448\n",
      "Train Epoch: 4 [32000/60000]\tLoss: 0.882962\n",
      "Train Epoch: 4 [32640/60000]\tLoss: 0.845142\n",
      "Train Epoch: 4 [33280/60000]\tLoss: 0.969953\n",
      "Train Epoch: 4 [33920/60000]\tLoss: 1.092629\n",
      "Train Epoch: 4 [34560/60000]\tLoss: 0.714498\n",
      "Train Epoch: 4 [35200/60000]\tLoss: 0.693806\n",
      "Train Epoch: 4 [35840/60000]\tLoss: 0.837259\n",
      "Train Epoch: 4 [36480/60000]\tLoss: 0.821124\n",
      "Train Epoch: 4 [37120/60000]\tLoss: 0.878354\n",
      "Train Epoch: 4 [37760/60000]\tLoss: 0.838434\n",
      "Train Epoch: 4 [38400/60000]\tLoss: 0.943928\n",
      "Train Epoch: 4 [39040/60000]\tLoss: 0.727013\n",
      "Train Epoch: 4 [39680/60000]\tLoss: 0.796022\n",
      "Train Epoch: 4 [40320/60000]\tLoss: 0.966894\n",
      "Train Epoch: 4 [40960/60000]\tLoss: 0.986493\n",
      "Train Epoch: 4 [41600/60000]\tLoss: 0.762221\n",
      "Train Epoch: 4 [42240/60000]\tLoss: 0.691214\n",
      "Train Epoch: 4 [42880/60000]\tLoss: 0.752560\n",
      "Train Epoch: 4 [43520/60000]\tLoss: 1.142243\n",
      "Train Epoch: 4 [44160/60000]\tLoss: 1.076781\n",
      "Train Epoch: 4 [44800/60000]\tLoss: 0.528669\n",
      "Train Epoch: 4 [45440/60000]\tLoss: 0.917593\n",
      "Train Epoch: 4 [46080/60000]\tLoss: 0.858223\n",
      "Train Epoch: 4 [46720/60000]\tLoss: 0.501568\n",
      "Train Epoch: 4 [47360/60000]\tLoss: 1.156801\n",
      "Train Epoch: 4 [48000/60000]\tLoss: 0.650457\n",
      "Train Epoch: 4 [48640/60000]\tLoss: 0.851045\n",
      "Train Epoch: 4 [49280/60000]\tLoss: 1.017775\n",
      "Train Epoch: 4 [49920/60000]\tLoss: 0.921663\n",
      "Train Epoch: 4 [50560/60000]\tLoss: 0.843217\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 0.786105\n",
      "Train Epoch: 4 [51840/60000]\tLoss: 0.700366\n",
      "Train Epoch: 4 [52480/60000]\tLoss: 0.745201\n",
      "Train Epoch: 4 [53120/60000]\tLoss: 0.823412\n",
      "Train Epoch: 4 [53760/60000]\tLoss: 0.892240\n",
      "Train Epoch: 4 [54400/60000]\tLoss: 0.763069\n",
      "Train Epoch: 4 [55040/60000]\tLoss: 0.753504\n",
      "Train Epoch: 4 [55680/60000]\tLoss: 1.058964\n",
      "Train Epoch: 4 [56320/60000]\tLoss: 0.797503\n",
      "Train Epoch: 4 [56960/60000]\tLoss: 0.942479\n",
      "Train Epoch: 4 [57600/60000]\tLoss: 0.587354\n",
      "Train Epoch: 4 [58240/60000]\tLoss: 0.812095\n",
      "Train Epoch: 4 [58880/60000]\tLoss: 0.649945\n",
      "Train Epoch: 4 [59520/60000]\tLoss: 0.809763\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.799997\n",
      "Train Epoch: 5 [640/60000]\tLoss: 0.688507\n",
      "Train Epoch: 5 [1280/60000]\tLoss: 0.793649\n",
      "Train Epoch: 5 [1920/60000]\tLoss: 0.733781\n",
      "Train Epoch: 5 [2560/60000]\tLoss: 0.728600\n",
      "Train Epoch: 5 [3200/60000]\tLoss: 0.767988\n",
      "Train Epoch: 5 [3840/60000]\tLoss: 0.898204\n",
      "Train Epoch: 5 [4480/60000]\tLoss: 0.789759\n",
      "Train Epoch: 5 [5120/60000]\tLoss: 0.977025\n",
      "Train Epoch: 5 [5760/60000]\tLoss: 0.875082\n",
      "Train Epoch: 5 [6400/60000]\tLoss: 0.650699\n",
      "Train Epoch: 5 [7040/60000]\tLoss: 1.013027\n",
      "Train Epoch: 5 [7680/60000]\tLoss: 0.719266\n",
      "Train Epoch: 5 [8320/60000]\tLoss: 0.758457\n",
      "Train Epoch: 5 [8960/60000]\tLoss: 0.977701\n",
      "Train Epoch: 5 [9600/60000]\tLoss: 0.796134\n",
      "Train Epoch: 5 [10240/60000]\tLoss: 0.662618\n",
      "Train Epoch: 5 [10880/60000]\tLoss: 0.714006\n",
      "Train Epoch: 5 [11520/60000]\tLoss: 0.803506\n",
      "Train Epoch: 5 [12160/60000]\tLoss: 0.752278\n",
      "Train Epoch: 5 [12800/60000]\tLoss: 1.014247\n",
      "Train Epoch: 5 [13440/60000]\tLoss: 0.779054\n",
      "Train Epoch: 5 [14080/60000]\tLoss: 0.626027\n",
      "Train Epoch: 5 [14720/60000]\tLoss: 0.941952\n",
      "Train Epoch: 5 [15360/60000]\tLoss: 0.729575\n",
      "Train Epoch: 5 [16000/60000]\tLoss: 0.544513\n",
      "Train Epoch: 5 [16640/60000]\tLoss: 0.869460\n",
      "Train Epoch: 5 [17280/60000]\tLoss: 0.861949\n",
      "Train Epoch: 5 [17920/60000]\tLoss: 0.766090\n",
      "Train Epoch: 5 [18560/60000]\tLoss: 0.914435\n",
      "Train Epoch: 5 [19200/60000]\tLoss: 0.598391\n",
      "Train Epoch: 5 [19840/60000]\tLoss: 0.826292\n",
      "Train Epoch: 5 [20480/60000]\tLoss: 0.629207\n",
      "Train Epoch: 5 [21120/60000]\tLoss: 0.719287\n",
      "Train Epoch: 5 [21760/60000]\tLoss: 1.097840\n",
      "Train Epoch: 5 [22400/60000]\tLoss: 0.772803\n",
      "Train Epoch: 5 [23040/60000]\tLoss: 0.781147\n",
      "Train Epoch: 5 [23680/60000]\tLoss: 0.906802\n",
      "Train Epoch: 5 [24320/60000]\tLoss: 0.760704\n",
      "Train Epoch: 5 [24960/60000]\tLoss: 0.668623\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.738173\n",
      "Train Epoch: 5 [26240/60000]\tLoss: 0.920460\n",
      "Train Epoch: 5 [26880/60000]\tLoss: 1.143439\n",
      "Train Epoch: 5 [27520/60000]\tLoss: 0.815023\n",
      "Train Epoch: 5 [28160/60000]\tLoss: 0.503777\n",
      "Train Epoch: 5 [28800/60000]\tLoss: 0.792570\n",
      "Train Epoch: 5 [29440/60000]\tLoss: 0.691016\n",
      "Train Epoch: 5 [30080/60000]\tLoss: 0.839337\n",
      "Train Epoch: 5 [30720/60000]\tLoss: 0.641418\n",
      "Train Epoch: 5 [31360/60000]\tLoss: 0.603152\n",
      "Train Epoch: 5 [32000/60000]\tLoss: 0.981961\n",
      "Train Epoch: 5 [32640/60000]\tLoss: 0.792121\n",
      "Train Epoch: 5 [33280/60000]\tLoss: 0.856365\n",
      "Train Epoch: 5 [33920/60000]\tLoss: 0.887051\n",
      "Train Epoch: 5 [34560/60000]\tLoss: 0.637803\n",
      "Train Epoch: 5 [35200/60000]\tLoss: 0.577482\n",
      "Train Epoch: 5 [35840/60000]\tLoss: 1.265159\n",
      "Train Epoch: 5 [36480/60000]\tLoss: 0.817752\n",
      "Train Epoch: 5 [37120/60000]\tLoss: 0.768963\n",
      "Train Epoch: 5 [37760/60000]\tLoss: 0.889854\n",
      "Train Epoch: 5 [38400/60000]\tLoss: 0.932345\n",
      "Train Epoch: 5 [39040/60000]\tLoss: 0.784825\n",
      "Train Epoch: 5 [39680/60000]\tLoss: 0.752728\n",
      "Train Epoch: 5 [40320/60000]\tLoss: 0.914277\n",
      "Train Epoch: 5 [40960/60000]\tLoss: 0.875982\n",
      "Train Epoch: 5 [41600/60000]\tLoss: 0.732920\n",
      "Train Epoch: 5 [42240/60000]\tLoss: 1.012087\n",
      "Train Epoch: 5 [42880/60000]\tLoss: 0.689387\n",
      "Train Epoch: 5 [43520/60000]\tLoss: 0.851161\n",
      "Train Epoch: 5 [44160/60000]\tLoss: 0.982252\n",
      "Train Epoch: 5 [44800/60000]\tLoss: 1.099484\n",
      "Train Epoch: 5 [45440/60000]\tLoss: 0.697408\n",
      "Train Epoch: 5 [46080/60000]\tLoss: 1.089010\n",
      "Train Epoch: 5 [46720/60000]\tLoss: 0.684341\n",
      "Train Epoch: 5 [47360/60000]\tLoss: 0.768321\n",
      "Train Epoch: 5 [48000/60000]\tLoss: 0.764479\n",
      "Train Epoch: 5 [48640/60000]\tLoss: 0.798520\n",
      "Train Epoch: 5 [49280/60000]\tLoss: 0.825868\n",
      "Train Epoch: 5 [49920/60000]\tLoss: 0.619981\n",
      "Train Epoch: 5 [50560/60000]\tLoss: 1.006048\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.413404\n",
      "Train Epoch: 5 [51840/60000]\tLoss: 0.874245\n",
      "Train Epoch: 5 [52480/60000]\tLoss: 1.135215\n",
      "Train Epoch: 5 [53120/60000]\tLoss: 0.858296\n",
      "Train Epoch: 5 [53760/60000]\tLoss: 1.019785\n",
      "Train Epoch: 5 [54400/60000]\tLoss: 0.757151\n",
      "Train Epoch: 5 [55040/60000]\tLoss: 1.161180\n",
      "Train Epoch: 5 [55680/60000]\tLoss: 0.919991\n",
      "Train Epoch: 5 [56320/60000]\tLoss: 1.204235\n",
      "Train Epoch: 5 [56960/60000]\tLoss: 0.760751\n",
      "Train Epoch: 5 [57600/60000]\tLoss: 0.884385\n",
      "Train Epoch: 5 [58240/60000]\tLoss: 0.920974\n",
      "Train Epoch: 5 [58880/60000]\tLoss: 0.908412\n",
      "Train Epoch: 5 [59520/60000]\tLoss: 0.920167\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.985182\n",
      "Train Epoch: 6 [640/60000]\tLoss: 1.111423\n",
      "Train Epoch: 6 [1280/60000]\tLoss: 0.878062\n",
      "Train Epoch: 6 [1920/60000]\tLoss: 0.692302\n",
      "Train Epoch: 6 [2560/60000]\tLoss: 0.908035\n",
      "Train Epoch: 6 [3200/60000]\tLoss: 0.737052\n",
      "Train Epoch: 6 [3840/60000]\tLoss: 0.857047\n",
      "Train Epoch: 6 [4480/60000]\tLoss: 0.947971\n",
      "Train Epoch: 6 [5120/60000]\tLoss: 0.716454\n",
      "Train Epoch: 6 [5760/60000]\tLoss: 0.961423\n",
      "Train Epoch: 6 [6400/60000]\tLoss: 0.776616\n",
      "Train Epoch: 6 [7040/60000]\tLoss: 0.888510\n",
      "Train Epoch: 6 [7680/60000]\tLoss: 0.656793\n",
      "Train Epoch: 6 [8320/60000]\tLoss: 0.665800\n",
      "Train Epoch: 6 [8960/60000]\tLoss: 0.642210\n",
      "Train Epoch: 6 [9600/60000]\tLoss: 0.652450\n",
      "Train Epoch: 6 [10240/60000]\tLoss: 0.865927\n",
      "Train Epoch: 6 [10880/60000]\tLoss: 0.731555\n",
      "Train Epoch: 6 [11520/60000]\tLoss: 0.739166\n",
      "Train Epoch: 6 [12160/60000]\tLoss: 0.798512\n",
      "Train Epoch: 6 [12800/60000]\tLoss: 0.839234\n",
      "Train Epoch: 6 [13440/60000]\tLoss: 0.979502\n",
      "Train Epoch: 6 [14080/60000]\tLoss: 0.875870\n",
      "Train Epoch: 6 [14720/60000]\tLoss: 0.806494\n",
      "Train Epoch: 6 [15360/60000]\tLoss: 0.757609\n",
      "Train Epoch: 6 [16000/60000]\tLoss: 0.461347\n",
      "Train Epoch: 6 [16640/60000]\tLoss: 0.674145\n",
      "Train Epoch: 6 [17280/60000]\tLoss: 0.501400\n",
      "Train Epoch: 6 [17920/60000]\tLoss: 0.815131\n",
      "Train Epoch: 6 [18560/60000]\tLoss: 0.733640\n",
      "Train Epoch: 6 [19200/60000]\tLoss: 0.739600\n",
      "Train Epoch: 6 [19840/60000]\tLoss: 0.863705\n",
      "Train Epoch: 6 [20480/60000]\tLoss: 0.627510\n",
      "Train Epoch: 6 [21120/60000]\tLoss: 0.982414\n",
      "Train Epoch: 6 [21760/60000]\tLoss: 0.728502\n",
      "Train Epoch: 6 [22400/60000]\tLoss: 0.911846\n",
      "Train Epoch: 6 [23040/60000]\tLoss: 0.865307\n",
      "Train Epoch: 6 [23680/60000]\tLoss: 0.797664\n",
      "Train Epoch: 6 [24320/60000]\tLoss: 0.724072\n",
      "Train Epoch: 6 [24960/60000]\tLoss: 0.961883\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.709427\n",
      "Train Epoch: 6 [26240/60000]\tLoss: 1.062886\n",
      "Train Epoch: 6 [26880/60000]\tLoss: 1.012196\n",
      "Train Epoch: 6 [27520/60000]\tLoss: 0.767931\n",
      "Train Epoch: 6 [28160/60000]\tLoss: 0.663823\n",
      "Train Epoch: 6 [28800/60000]\tLoss: 0.700305\n",
      "Train Epoch: 6 [29440/60000]\tLoss: 0.828687\n",
      "Train Epoch: 6 [30080/60000]\tLoss: 0.952579\n",
      "Train Epoch: 6 [30720/60000]\tLoss: 0.896752\n",
      "Train Epoch: 6 [31360/60000]\tLoss: 0.687235\n",
      "Train Epoch: 6 [32000/60000]\tLoss: 0.692106\n",
      "Train Epoch: 6 [32640/60000]\tLoss: 0.618162\n",
      "Train Epoch: 6 [33280/60000]\tLoss: 0.590146\n",
      "Train Epoch: 6 [33920/60000]\tLoss: 0.980306\n",
      "Train Epoch: 6 [34560/60000]\tLoss: 0.525991\n",
      "Train Epoch: 6 [35200/60000]\tLoss: 1.296810\n",
      "Train Epoch: 6 [35840/60000]\tLoss: 0.780335\n",
      "Train Epoch: 6 [36480/60000]\tLoss: 1.146261\n",
      "Train Epoch: 6 [37120/60000]\tLoss: 0.726665\n",
      "Train Epoch: 6 [37760/60000]\tLoss: 0.783987\n",
      "Train Epoch: 6 [38400/60000]\tLoss: 0.676121\n",
      "Train Epoch: 6 [39040/60000]\tLoss: 0.807126\n",
      "Train Epoch: 6 [39680/60000]\tLoss: 0.948305\n",
      "Train Epoch: 6 [40320/60000]\tLoss: 0.902315\n",
      "Train Epoch: 6 [40960/60000]\tLoss: 0.957665\n",
      "Train Epoch: 6 [41600/60000]\tLoss: 0.702820\n",
      "Train Epoch: 6 [42240/60000]\tLoss: 0.668451\n",
      "Train Epoch: 6 [42880/60000]\tLoss: 0.851244\n",
      "Train Epoch: 6 [43520/60000]\tLoss: 1.080657\n",
      "Train Epoch: 6 [44160/60000]\tLoss: 1.116182\n",
      "Train Epoch: 6 [44800/60000]\tLoss: 0.677514\n",
      "Train Epoch: 6 [45440/60000]\tLoss: 0.653597\n",
      "Train Epoch: 6 [46080/60000]\tLoss: 0.758588\n",
      "Train Epoch: 6 [46720/60000]\tLoss: 0.656009\n",
      "Train Epoch: 6 [47360/60000]\tLoss: 0.832149\n",
      "Train Epoch: 6 [48000/60000]\tLoss: 0.673324\n",
      "Train Epoch: 6 [48640/60000]\tLoss: 0.663386\n",
      "Train Epoch: 6 [49280/60000]\tLoss: 1.045313\n",
      "Train Epoch: 6 [49920/60000]\tLoss: 0.673243\n",
      "Train Epoch: 6 [50560/60000]\tLoss: 0.754025\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 1.037824\n",
      "Train Epoch: 6 [51840/60000]\tLoss: 0.857594\n",
      "Train Epoch: 6 [52480/60000]\tLoss: 0.835847\n",
      "Train Epoch: 6 [53120/60000]\tLoss: 0.856212\n",
      "Train Epoch: 6 [53760/60000]\tLoss: 0.610529\n",
      "Train Epoch: 6 [54400/60000]\tLoss: 0.707277\n",
      "Train Epoch: 6 [55040/60000]\tLoss: 0.995593\n",
      "Train Epoch: 6 [55680/60000]\tLoss: 0.753330\n",
      "Train Epoch: 6 [56320/60000]\tLoss: 0.750546\n",
      "Train Epoch: 6 [56960/60000]\tLoss: 0.785572\n",
      "Train Epoch: 6 [57600/60000]\tLoss: 0.834670\n",
      "Train Epoch: 6 [58240/60000]\tLoss: 1.006669\n",
      "Train Epoch: 6 [58880/60000]\tLoss: 0.807567\n",
      "Train Epoch: 6 [59520/60000]\tLoss: 0.739851\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.778454\n",
      "Train Epoch: 7 [640/60000]\tLoss: 1.068427\n",
      "Train Epoch: 7 [1280/60000]\tLoss: 0.716092\n",
      "Train Epoch: 7 [1920/60000]\tLoss: 0.614210\n",
      "Train Epoch: 7 [2560/60000]\tLoss: 0.956332\n",
      "Train Epoch: 7 [3200/60000]\tLoss: 0.685839\n",
      "Train Epoch: 7 [3840/60000]\tLoss: 0.838100\n",
      "Train Epoch: 7 [4480/60000]\tLoss: 0.537473\n",
      "Train Epoch: 7 [5120/60000]\tLoss: 0.977489\n",
      "Train Epoch: 7 [5760/60000]\tLoss: 0.833395\n",
      "Train Epoch: 7 [6400/60000]\tLoss: 0.865623\n",
      "Train Epoch: 7 [7040/60000]\tLoss: 0.817716\n",
      "Train Epoch: 7 [7680/60000]\tLoss: 0.604534\n",
      "Train Epoch: 7 [8320/60000]\tLoss: 0.803855\n",
      "Train Epoch: 7 [8960/60000]\tLoss: 0.815154\n",
      "Train Epoch: 7 [9600/60000]\tLoss: 0.818805\n",
      "Train Epoch: 7 [10240/60000]\tLoss: 0.889175\n",
      "Train Epoch: 7 [10880/60000]\tLoss: 0.735626\n",
      "Train Epoch: 7 [11520/60000]\tLoss: 0.711309\n",
      "Train Epoch: 7 [12160/60000]\tLoss: 0.871253\n",
      "Train Epoch: 7 [12800/60000]\tLoss: 0.804703\n",
      "Train Epoch: 7 [13440/60000]\tLoss: 0.553008\n",
      "Train Epoch: 7 [14080/60000]\tLoss: 1.139069\n",
      "Train Epoch: 7 [14720/60000]\tLoss: 1.245372\n",
      "Train Epoch: 7 [15360/60000]\tLoss: 1.087008\n",
      "Train Epoch: 7 [16000/60000]\tLoss: 0.950591\n",
      "Train Epoch: 7 [16640/60000]\tLoss: 0.961021\n",
      "Train Epoch: 7 [17280/60000]\tLoss: 0.908846\n",
      "Train Epoch: 7 [17920/60000]\tLoss: 0.830051\n",
      "Train Epoch: 7 [18560/60000]\tLoss: 1.127587\n",
      "Train Epoch: 7 [19200/60000]\tLoss: 0.988701\n",
      "Train Epoch: 7 [19840/60000]\tLoss: 0.979815\n",
      "Train Epoch: 7 [20480/60000]\tLoss: 0.798951\n",
      "Train Epoch: 7 [21120/60000]\tLoss: 0.795341\n",
      "Train Epoch: 7 [21760/60000]\tLoss: 0.621966\n",
      "Train Epoch: 7 [22400/60000]\tLoss: 0.885651\n",
      "Train Epoch: 7 [23040/60000]\tLoss: 0.606117\n",
      "Train Epoch: 7 [23680/60000]\tLoss: 0.743658\n",
      "Train Epoch: 7 [24320/60000]\tLoss: 0.507438\n",
      "Train Epoch: 7 [24960/60000]\tLoss: 0.816715\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.861007\n",
      "Train Epoch: 7 [26240/60000]\tLoss: 0.571935\n",
      "Train Epoch: 7 [26880/60000]\tLoss: 0.899792\n",
      "Train Epoch: 7 [27520/60000]\tLoss: 0.825307\n",
      "Train Epoch: 7 [28160/60000]\tLoss: 0.835561\n",
      "Train Epoch: 7 [28800/60000]\tLoss: 0.747176\n",
      "Train Epoch: 7 [29440/60000]\tLoss: 0.843698\n",
      "Train Epoch: 7 [30080/60000]\tLoss: 0.763783\n",
      "Train Epoch: 7 [30720/60000]\tLoss: 0.910835\n",
      "Train Epoch: 7 [31360/60000]\tLoss: 0.773801\n",
      "Train Epoch: 7 [32000/60000]\tLoss: 0.738828\n",
      "Train Epoch: 7 [32640/60000]\tLoss: 0.622835\n",
      "Train Epoch: 7 [33280/60000]\tLoss: 0.798936\n",
      "Train Epoch: 7 [33920/60000]\tLoss: 0.648207\n",
      "Train Epoch: 7 [34560/60000]\tLoss: 0.969108\n",
      "Train Epoch: 7 [35200/60000]\tLoss: 0.846388\n",
      "Train Epoch: 7 [35840/60000]\tLoss: 0.693924\n",
      "Train Epoch: 7 [36480/60000]\tLoss: 0.588144\n",
      "Train Epoch: 7 [37120/60000]\tLoss: 0.852525\n",
      "Train Epoch: 7 [37760/60000]\tLoss: 0.780915\n",
      "Train Epoch: 7 [38400/60000]\tLoss: 0.916371\n",
      "Train Epoch: 7 [39040/60000]\tLoss: 0.791607\n",
      "Train Epoch: 7 [39680/60000]\tLoss: 0.648005\n",
      "Train Epoch: 7 [40320/60000]\tLoss: 0.848851\n",
      "Train Epoch: 7 [40960/60000]\tLoss: 0.807798\n",
      "Train Epoch: 7 [41600/60000]\tLoss: 0.787437\n",
      "Train Epoch: 7 [42240/60000]\tLoss: 0.705791\n",
      "Train Epoch: 7 [42880/60000]\tLoss: 0.548645\n",
      "Train Epoch: 7 [43520/60000]\tLoss: 0.685808\n",
      "Train Epoch: 7 [44160/60000]\tLoss: 0.806653\n",
      "Train Epoch: 7 [44800/60000]\tLoss: 0.706769\n",
      "Train Epoch: 7 [45440/60000]\tLoss: 0.989055\n",
      "Train Epoch: 7 [46080/60000]\tLoss: 0.964657\n",
      "Train Epoch: 7 [46720/60000]\tLoss: 0.989603\n",
      "Train Epoch: 7 [47360/60000]\tLoss: 0.803184\n",
      "Train Epoch: 7 [48000/60000]\tLoss: 1.139665\n",
      "Train Epoch: 7 [48640/60000]\tLoss: 0.781267\n",
      "Train Epoch: 7 [49280/60000]\tLoss: 0.891636\n",
      "Train Epoch: 7 [49920/60000]\tLoss: 0.958117\n",
      "Train Epoch: 7 [50560/60000]\tLoss: 0.811054\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 0.633253\n",
      "Train Epoch: 7 [51840/60000]\tLoss: 1.063290\n",
      "Train Epoch: 7 [52480/60000]\tLoss: 0.685631\n",
      "Train Epoch: 7 [53120/60000]\tLoss: 0.775798\n",
      "Train Epoch: 7 [53760/60000]\tLoss: 0.934522\n",
      "Train Epoch: 7 [54400/60000]\tLoss: 0.571539\n",
      "Train Epoch: 7 [55040/60000]\tLoss: 0.732231\n",
      "Train Epoch: 7 [55680/60000]\tLoss: 0.906954\n",
      "Train Epoch: 7 [56320/60000]\tLoss: 0.755964\n",
      "Train Epoch: 7 [56960/60000]\tLoss: 0.869335\n",
      "Train Epoch: 7 [57600/60000]\tLoss: 0.606250\n",
      "Train Epoch: 7 [58240/60000]\tLoss: 0.732037\n",
      "Train Epoch: 7 [58880/60000]\tLoss: 0.907692\n",
      "Train Epoch: 7 [59520/60000]\tLoss: 0.822705\n",
      "Train Epoch: 8 [0/60000]\tLoss: 0.730645\n",
      "Train Epoch: 8 [640/60000]\tLoss: 0.710212\n",
      "Train Epoch: 8 [1280/60000]\tLoss: 0.944773\n",
      "Train Epoch: 8 [1920/60000]\tLoss: 0.765043\n",
      "Train Epoch: 8 [2560/60000]\tLoss: 0.906452\n",
      "Train Epoch: 8 [3200/60000]\tLoss: 0.666615\n",
      "Train Epoch: 8 [3840/60000]\tLoss: 0.869445\n",
      "Train Epoch: 8 [4480/60000]\tLoss: 0.964233\n",
      "Train Epoch: 8 [5120/60000]\tLoss: 0.977511\n",
      "Train Epoch: 8 [5760/60000]\tLoss: 0.998417\n",
      "Train Epoch: 8 [6400/60000]\tLoss: 0.854678\n",
      "Train Epoch: 8 [7040/60000]\tLoss: 0.738736\n",
      "Train Epoch: 8 [7680/60000]\tLoss: 0.797993\n",
      "Train Epoch: 8 [8320/60000]\tLoss: 0.891358\n",
      "Train Epoch: 8 [8960/60000]\tLoss: 0.652790\n",
      "Train Epoch: 8 [9600/60000]\tLoss: 0.967946\n",
      "Train Epoch: 8 [10240/60000]\tLoss: 1.148640\n",
      "Train Epoch: 8 [10880/60000]\tLoss: 0.853842\n",
      "Train Epoch: 8 [11520/60000]\tLoss: 0.812804\n",
      "Train Epoch: 8 [12160/60000]\tLoss: 0.780761\n",
      "Train Epoch: 8 [12800/60000]\tLoss: 0.806087\n",
      "Train Epoch: 8 [13440/60000]\tLoss: 0.639883\n",
      "Train Epoch: 8 [14080/60000]\tLoss: 0.745938\n",
      "Train Epoch: 8 [14720/60000]\tLoss: 0.921435\n",
      "Train Epoch: 8 [15360/60000]\tLoss: 0.797412\n",
      "Train Epoch: 8 [16000/60000]\tLoss: 0.704280\n",
      "Train Epoch: 8 [16640/60000]\tLoss: 0.829346\n",
      "Train Epoch: 8 [17280/60000]\tLoss: 0.716464\n",
      "Train Epoch: 8 [17920/60000]\tLoss: 0.725530\n",
      "Train Epoch: 8 [18560/60000]\tLoss: 1.020401\n",
      "Train Epoch: 8 [19200/60000]\tLoss: 0.860155\n",
      "Train Epoch: 8 [19840/60000]\tLoss: 0.751353\n",
      "Train Epoch: 8 [20480/60000]\tLoss: 0.690628\n",
      "Train Epoch: 8 [21120/60000]\tLoss: 1.111272\n",
      "Train Epoch: 8 [21760/60000]\tLoss: 0.869245\n",
      "Train Epoch: 8 [22400/60000]\tLoss: 0.933133\n",
      "Train Epoch: 8 [23040/60000]\tLoss: 0.745611\n",
      "Train Epoch: 8 [23680/60000]\tLoss: 0.914158\n",
      "Train Epoch: 8 [24320/60000]\tLoss: 0.637289\n",
      "Train Epoch: 8 [24960/60000]\tLoss: 0.839055\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.694477\n",
      "Train Epoch: 8 [26240/60000]\tLoss: 0.693273\n",
      "Train Epoch: 8 [26880/60000]\tLoss: 0.666226\n",
      "Train Epoch: 8 [27520/60000]\tLoss: 0.996664\n",
      "Train Epoch: 8 [28160/60000]\tLoss: 0.583797\n",
      "Train Epoch: 8 [28800/60000]\tLoss: 0.877848\n",
      "Train Epoch: 8 [29440/60000]\tLoss: 0.929029\n",
      "Train Epoch: 8 [30080/60000]\tLoss: 0.765381\n",
      "Train Epoch: 8 [30720/60000]\tLoss: 0.997769\n",
      "Train Epoch: 8 [31360/60000]\tLoss: 0.739770\n",
      "Train Epoch: 8 [32000/60000]\tLoss: 0.902244\n",
      "Train Epoch: 8 [32640/60000]\tLoss: 0.692297\n",
      "Train Epoch: 8 [33280/60000]\tLoss: 0.854791\n",
      "Train Epoch: 8 [33920/60000]\tLoss: 0.836517\n",
      "Train Epoch: 8 [34560/60000]\tLoss: 0.802154\n",
      "Train Epoch: 8 [35200/60000]\tLoss: 0.755719\n",
      "Train Epoch: 8 [35840/60000]\tLoss: 0.747201\n",
      "Train Epoch: 8 [36480/60000]\tLoss: 0.720624\n",
      "Train Epoch: 8 [37120/60000]\tLoss: 0.860105\n",
      "Train Epoch: 8 [37760/60000]\tLoss: 1.073753\n",
      "Train Epoch: 8 [38400/60000]\tLoss: 0.823019\n",
      "Train Epoch: 8 [39040/60000]\tLoss: 0.921914\n",
      "Train Epoch: 8 [39680/60000]\tLoss: 0.671406\n",
      "Train Epoch: 8 [40320/60000]\tLoss: 0.760369\n",
      "Train Epoch: 8 [40960/60000]\tLoss: 0.743661\n",
      "Train Epoch: 8 [41600/60000]\tLoss: 0.790626\n",
      "Train Epoch: 8 [42240/60000]\tLoss: 1.137861\n",
      "Train Epoch: 8 [42880/60000]\tLoss: 0.813290\n",
      "Train Epoch: 8 [43520/60000]\tLoss: 0.465810\n",
      "Train Epoch: 8 [44160/60000]\tLoss: 0.895478\n",
      "Train Epoch: 8 [44800/60000]\tLoss: 0.648124\n",
      "Train Epoch: 8 [45440/60000]\tLoss: 0.747227\n",
      "Train Epoch: 8 [46080/60000]\tLoss: 0.852396\n",
      "Train Epoch: 8 [46720/60000]\tLoss: 0.773423\n",
      "Train Epoch: 8 [47360/60000]\tLoss: 0.772728\n",
      "Train Epoch: 8 [48000/60000]\tLoss: 0.857152\n",
      "Train Epoch: 8 [48640/60000]\tLoss: 0.847448\n",
      "Train Epoch: 8 [49280/60000]\tLoss: 1.223762\n",
      "Train Epoch: 8 [49920/60000]\tLoss: 0.758707\n",
      "Train Epoch: 8 [50560/60000]\tLoss: 0.756287\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.820342\n",
      "Train Epoch: 8 [51840/60000]\tLoss: 0.892119\n",
      "Train Epoch: 8 [52480/60000]\tLoss: 1.001203\n",
      "Train Epoch: 8 [53120/60000]\tLoss: 0.672314\n",
      "Train Epoch: 8 [53760/60000]\tLoss: 0.664692\n",
      "Train Epoch: 8 [54400/60000]\tLoss: 0.456446\n",
      "Train Epoch: 8 [55040/60000]\tLoss: 0.661610\n",
      "Train Epoch: 8 [55680/60000]\tLoss: 1.129772\n",
      "Train Epoch: 8 [56320/60000]\tLoss: 0.992062\n",
      "Train Epoch: 8 [56960/60000]\tLoss: 0.912684\n",
      "Train Epoch: 8 [57600/60000]\tLoss: 0.957145\n",
      "Train Epoch: 8 [58240/60000]\tLoss: 0.947999\n",
      "Train Epoch: 8 [58880/60000]\tLoss: 0.646718\n",
      "Train Epoch: 8 [59520/60000]\tLoss: 0.774002\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.843283\n",
      "Train Epoch: 9 [640/60000]\tLoss: 0.945050\n",
      "Train Epoch: 9 [1280/60000]\tLoss: 0.637697\n",
      "Train Epoch: 9 [1920/60000]\tLoss: 0.597230\n",
      "Train Epoch: 9 [2560/60000]\tLoss: 0.730589\n",
      "Train Epoch: 9 [3200/60000]\tLoss: 1.025447\n",
      "Train Epoch: 9 [3840/60000]\tLoss: 0.941440\n",
      "Train Epoch: 9 [4480/60000]\tLoss: 1.022697\n",
      "Train Epoch: 9 [5120/60000]\tLoss: 1.076632\n",
      "Train Epoch: 9 [5760/60000]\tLoss: 0.601381\n",
      "Train Epoch: 9 [6400/60000]\tLoss: 0.911127\n",
      "Train Epoch: 9 [7040/60000]\tLoss: 0.967839\n",
      "Train Epoch: 9 [7680/60000]\tLoss: 0.841008\n",
      "Train Epoch: 9 [8320/60000]\tLoss: 0.596890\n",
      "Train Epoch: 9 [8960/60000]\tLoss: 0.881029\n",
      "Train Epoch: 9 [9600/60000]\tLoss: 0.845912\n",
      "Train Epoch: 9 [10240/60000]\tLoss: 0.942806\n",
      "Train Epoch: 9 [10880/60000]\tLoss: 0.920942\n",
      "Train Epoch: 9 [11520/60000]\tLoss: 0.565907\n",
      "Train Epoch: 9 [12160/60000]\tLoss: 0.898297\n",
      "Train Epoch: 9 [12800/60000]\tLoss: 0.922675\n",
      "Train Epoch: 9 [13440/60000]\tLoss: 0.699205\n",
      "Train Epoch: 9 [14080/60000]\tLoss: 0.839310\n",
      "Train Epoch: 9 [14720/60000]\tLoss: 0.611375\n",
      "Train Epoch: 9 [15360/60000]\tLoss: 0.958634\n",
      "Train Epoch: 9 [16000/60000]\tLoss: 0.697107\n",
      "Train Epoch: 9 [16640/60000]\tLoss: 0.857627\n",
      "Train Epoch: 9 [17280/60000]\tLoss: 1.083848\n",
      "Train Epoch: 9 [17920/60000]\tLoss: 1.064732\n",
      "Train Epoch: 9 [18560/60000]\tLoss: 0.697652\n",
      "Train Epoch: 9 [19200/60000]\tLoss: 0.753067\n",
      "Train Epoch: 9 [19840/60000]\tLoss: 0.625377\n",
      "Train Epoch: 9 [20480/60000]\tLoss: 0.604982\n",
      "Train Epoch: 9 [21120/60000]\tLoss: 1.029779\n",
      "Train Epoch: 9 [21760/60000]\tLoss: 0.890010\n",
      "Train Epoch: 9 [22400/60000]\tLoss: 0.669844\n",
      "Train Epoch: 9 [23040/60000]\tLoss: 0.712130\n",
      "Train Epoch: 9 [23680/60000]\tLoss: 0.737414\n",
      "Train Epoch: 9 [24320/60000]\tLoss: 0.696984\n",
      "Train Epoch: 9 [24960/60000]\tLoss: 0.732060\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.719323\n",
      "Train Epoch: 9 [26240/60000]\tLoss: 0.746598\n",
      "Train Epoch: 9 [26880/60000]\tLoss: 1.040184\n",
      "Train Epoch: 9 [27520/60000]\tLoss: 0.754390\n",
      "Train Epoch: 9 [28160/60000]\tLoss: 0.892664\n",
      "Train Epoch: 9 [28800/60000]\tLoss: 0.639960\n",
      "Train Epoch: 9 [29440/60000]\tLoss: 0.831017\n",
      "Train Epoch: 9 [30080/60000]\tLoss: 0.824879\n",
      "Train Epoch: 9 [30720/60000]\tLoss: 0.713751\n",
      "Train Epoch: 9 [31360/60000]\tLoss: 0.804172\n",
      "Train Epoch: 9 [32000/60000]\tLoss: 0.707166\n",
      "Train Epoch: 9 [32640/60000]\tLoss: 0.675332\n",
      "Train Epoch: 9 [33280/60000]\tLoss: 0.680331\n",
      "Train Epoch: 9 [33920/60000]\tLoss: 1.058366\n",
      "Train Epoch: 9 [34560/60000]\tLoss: 0.645714\n",
      "Train Epoch: 9 [35200/60000]\tLoss: 0.781067\n",
      "Train Epoch: 9 [35840/60000]\tLoss: 0.870049\n",
      "Train Epoch: 9 [36480/60000]\tLoss: 0.684430\n",
      "Train Epoch: 9 [37120/60000]\tLoss: 0.839806\n",
      "Train Epoch: 9 [37760/60000]\tLoss: 0.835283\n",
      "Train Epoch: 9 [38400/60000]\tLoss: 0.752646\n",
      "Train Epoch: 9 [39040/60000]\tLoss: 0.750684\n",
      "Train Epoch: 9 [39680/60000]\tLoss: 0.783587\n",
      "Train Epoch: 9 [40320/60000]\tLoss: 0.784229\n",
      "Train Epoch: 9 [40960/60000]\tLoss: 0.667528\n",
      "Train Epoch: 9 [41600/60000]\tLoss: 0.814630\n",
      "Train Epoch: 9 [42240/60000]\tLoss: 0.822455\n",
      "Train Epoch: 9 [42880/60000]\tLoss: 0.736347\n",
      "Train Epoch: 9 [43520/60000]\tLoss: 0.937107\n",
      "Train Epoch: 9 [44160/60000]\tLoss: 0.846773\n",
      "Train Epoch: 9 [44800/60000]\tLoss: 0.781100\n",
      "Train Epoch: 9 [45440/60000]\tLoss: 0.724992\n",
      "Train Epoch: 9 [46080/60000]\tLoss: 0.812796\n",
      "Train Epoch: 9 [46720/60000]\tLoss: 0.661043\n",
      "Train Epoch: 9 [47360/60000]\tLoss: 0.881844\n",
      "Train Epoch: 9 [48000/60000]\tLoss: 0.722168\n",
      "Train Epoch: 9 [48640/60000]\tLoss: 0.638079\n",
      "Train Epoch: 9 [49280/60000]\tLoss: 0.768694\n",
      "Train Epoch: 9 [49920/60000]\tLoss: 0.838741\n",
      "Train Epoch: 9 [50560/60000]\tLoss: 0.922262\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.577327\n",
      "Train Epoch: 9 [51840/60000]\tLoss: 0.786263\n",
      "Train Epoch: 9 [52480/60000]\tLoss: 0.807395\n",
      "Train Epoch: 9 [53120/60000]\tLoss: 1.028719\n",
      "Train Epoch: 9 [53760/60000]\tLoss: 0.792955\n",
      "Train Epoch: 9 [54400/60000]\tLoss: 0.903131\n",
      "Train Epoch: 9 [55040/60000]\tLoss: 0.934353\n",
      "Train Epoch: 9 [55680/60000]\tLoss: 0.949081\n",
      "Train Epoch: 9 [56320/60000]\tLoss: 0.813231\n",
      "Train Epoch: 9 [56960/60000]\tLoss: 0.621955\n",
      "Train Epoch: 9 [57600/60000]\tLoss: 0.876047\n",
      "Train Epoch: 9 [58240/60000]\tLoss: 0.906351\n",
      "Train Epoch: 9 [58880/60000]\tLoss: 0.799816\n",
      "Train Epoch: 9 [59520/60000]\tLoss: 0.679066\n",
      "Train Epoch: 10 [0/60000]\tLoss: 0.689065\n",
      "Train Epoch: 10 [640/60000]\tLoss: 0.968619\n",
      "Train Epoch: 10 [1280/60000]\tLoss: 0.897734\n",
      "Train Epoch: 10 [1920/60000]\tLoss: 0.902794\n",
      "Train Epoch: 10 [2560/60000]\tLoss: 0.917656\n",
      "Train Epoch: 10 [3200/60000]\tLoss: 0.956962\n",
      "Train Epoch: 10 [3840/60000]\tLoss: 0.658110\n",
      "Train Epoch: 10 [4480/60000]\tLoss: 0.861058\n",
      "Train Epoch: 10 [5120/60000]\tLoss: 0.925418\n",
      "Train Epoch: 10 [5760/60000]\tLoss: 0.477654\n",
      "Train Epoch: 10 [6400/60000]\tLoss: 0.756175\n",
      "Train Epoch: 10 [7040/60000]\tLoss: 0.830732\n",
      "Train Epoch: 10 [7680/60000]\tLoss: 0.990041\n",
      "Train Epoch: 10 [8320/60000]\tLoss: 0.685004\n",
      "Train Epoch: 10 [8960/60000]\tLoss: 0.824505\n",
      "Train Epoch: 10 [9600/60000]\tLoss: 0.861642\n",
      "Train Epoch: 10 [10240/60000]\tLoss: 0.639182\n",
      "Train Epoch: 10 [10880/60000]\tLoss: 0.690206\n",
      "Train Epoch: 10 [11520/60000]\tLoss: 0.555280\n",
      "Train Epoch: 10 [12160/60000]\tLoss: 0.884924\n",
      "Train Epoch: 10 [12800/60000]\tLoss: 0.820439\n",
      "Train Epoch: 10 [13440/60000]\tLoss: 0.630705\n",
      "Train Epoch: 10 [14080/60000]\tLoss: 1.050458\n",
      "Train Epoch: 10 [14720/60000]\tLoss: 0.918894\n",
      "Train Epoch: 10 [15360/60000]\tLoss: 1.040069\n",
      "Train Epoch: 10 [16000/60000]\tLoss: 0.975538\n",
      "Train Epoch: 10 [16640/60000]\tLoss: 0.722344\n",
      "Train Epoch: 10 [17280/60000]\tLoss: 0.872873\n",
      "Train Epoch: 10 [17920/60000]\tLoss: 0.767856\n",
      "Train Epoch: 10 [18560/60000]\tLoss: 0.838559\n",
      "Train Epoch: 10 [19200/60000]\tLoss: 0.870097\n",
      "Train Epoch: 10 [19840/60000]\tLoss: 0.969447\n",
      "Train Epoch: 10 [20480/60000]\tLoss: 1.013762\n",
      "Train Epoch: 10 [21120/60000]\tLoss: 1.016792\n",
      "Train Epoch: 10 [21760/60000]\tLoss: 0.939471\n",
      "Train Epoch: 10 [22400/60000]\tLoss: 0.635679\n",
      "Train Epoch: 10 [23040/60000]\tLoss: 0.669150\n",
      "Train Epoch: 10 [23680/60000]\tLoss: 0.909756\n",
      "Train Epoch: 10 [24320/60000]\tLoss: 0.637824\n",
      "Train Epoch: 10 [24960/60000]\tLoss: 0.746465\n",
      "Train Epoch: 10 [25600/60000]\tLoss: 0.901497\n",
      "Train Epoch: 10 [26240/60000]\tLoss: 0.989457\n",
      "Train Epoch: 10 [26880/60000]\tLoss: 1.101571\n",
      "Train Epoch: 10 [27520/60000]\tLoss: 0.825230\n",
      "Train Epoch: 10 [28160/60000]\tLoss: 0.988551\n",
      "Train Epoch: 10 [28800/60000]\tLoss: 0.663339\n",
      "Train Epoch: 10 [29440/60000]\tLoss: 0.755222\n",
      "Train Epoch: 10 [30080/60000]\tLoss: 0.635542\n",
      "Train Epoch: 10 [30720/60000]\tLoss: 0.756458\n",
      "Train Epoch: 10 [31360/60000]\tLoss: 0.786751\n",
      "Train Epoch: 10 [32000/60000]\tLoss: 0.730907\n",
      "Train Epoch: 10 [32640/60000]\tLoss: 0.597437\n",
      "Train Epoch: 10 [33280/60000]\tLoss: 0.825313\n",
      "Train Epoch: 10 [33920/60000]\tLoss: 0.833336\n",
      "Train Epoch: 10 [34560/60000]\tLoss: 0.947328\n",
      "Train Epoch: 10 [35200/60000]\tLoss: 0.698624\n",
      "Train Epoch: 10 [35840/60000]\tLoss: 0.775614\n",
      "Train Epoch: 10 [36480/60000]\tLoss: 0.731467\n",
      "Train Epoch: 10 [37120/60000]\tLoss: 0.665142\n",
      "Train Epoch: 10 [37760/60000]\tLoss: 0.743309\n",
      "Train Epoch: 10 [38400/60000]\tLoss: 0.897728\n",
      "Train Epoch: 10 [39040/60000]\tLoss: 0.686489\n",
      "Train Epoch: 10 [39680/60000]\tLoss: 0.616822\n",
      "Train Epoch: 10 [40320/60000]\tLoss: 0.737374\n",
      "Train Epoch: 10 [40960/60000]\tLoss: 1.097379\n",
      "Train Epoch: 10 [41600/60000]\tLoss: 0.601709\n",
      "Train Epoch: 10 [42240/60000]\tLoss: 0.840215\n",
      "Train Epoch: 10 [42880/60000]\tLoss: 0.989162\n",
      "Train Epoch: 10 [43520/60000]\tLoss: 0.764939\n",
      "Train Epoch: 10 [44160/60000]\tLoss: 1.013547\n",
      "Train Epoch: 10 [44800/60000]\tLoss: 0.620948\n",
      "Train Epoch: 10 [45440/60000]\tLoss: 0.434563\n",
      "Train Epoch: 10 [46080/60000]\tLoss: 0.739111\n",
      "Train Epoch: 10 [46720/60000]\tLoss: 0.929159\n",
      "Train Epoch: 10 [47360/60000]\tLoss: 0.857135\n",
      "Train Epoch: 10 [48000/60000]\tLoss: 0.913548\n",
      "Train Epoch: 10 [48640/60000]\tLoss: 0.721101\n",
      "Train Epoch: 10 [49280/60000]\tLoss: 0.843914\n",
      "Train Epoch: 10 [49920/60000]\tLoss: 0.844220\n",
      "Train Epoch: 10 [50560/60000]\tLoss: 0.699904\n",
      "Train Epoch: 10 [51200/60000]\tLoss: 0.684593\n",
      "Train Epoch: 10 [51840/60000]\tLoss: 0.684847\n",
      "Train Epoch: 10 [52480/60000]\tLoss: 0.752782\n",
      "Train Epoch: 10 [53120/60000]\tLoss: 0.955489\n",
      "Train Epoch: 10 [53760/60000]\tLoss: 0.771526\n",
      "Train Epoch: 10 [54400/60000]\tLoss: 0.879214\n",
      "Train Epoch: 10 [55040/60000]\tLoss: 0.793595\n",
      "Train Epoch: 10 [55680/60000]\tLoss: 0.853037\n",
      "Train Epoch: 10 [56320/60000]\tLoss: 0.839750\n",
      "Train Epoch: 10 [56960/60000]\tLoss: 0.538714\n",
      "Train Epoch: 10 [57600/60000]\tLoss: 0.743749\n",
      "Train Epoch: 10 [58240/60000]\tLoss: 0.845117\n",
      "Train Epoch: 10 [58880/60000]\tLoss: 0.904007\n",
      "Train Epoch: 10 [59520/60000]\tLoss: 0.821330\n",
      "\n",
      "Test set: Avg. loss: 0.2619, Accuracy: 9270/10000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test Model 1\n",
    "\n",
    "# Create network\n",
    "model = Net()\n",
    "# Initialize model weights\n",
    "model.apply(weights_init)\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Get initial performance\n",
    "test(model)\n",
    "# Train for ten epochs\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch, model)\n",
    "accuracy1 = test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "id": "r2PVnghrmr0F"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3558, Accuracy: 574/10000 (6%)\n",
      "\n",
      "Train Epoch: 1 [0/60000]\tLoss: 2.610677\n",
      "Train Epoch: 1 [640/60000]\tLoss: 2.194091\n",
      "Train Epoch: 1 [1280/60000]\tLoss: 1.605685\n",
      "Train Epoch: 1 [1920/60000]\tLoss: 1.473536\n",
      "Train Epoch: 1 [2560/60000]\tLoss: 1.353008\n",
      "Train Epoch: 1 [3200/60000]\tLoss: 1.129176\n",
      "Train Epoch: 1 [3840/60000]\tLoss: 1.219146\n",
      "Train Epoch: 1 [4480/60000]\tLoss: 1.098599\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 1.128591\n",
      "Train Epoch: 1 [5760/60000]\tLoss: 1.055942\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 0.749715\n",
      "Train Epoch: 1 [7040/60000]\tLoss: 0.793017\n",
      "Train Epoch: 1 [7680/60000]\tLoss: 0.817365\n",
      "Train Epoch: 1 [8320/60000]\tLoss: 0.962840\n",
      "Train Epoch: 1 [8960/60000]\tLoss: 0.825756\n",
      "Train Epoch: 1 [9600/60000]\tLoss: 0.738609\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 0.723809\n",
      "Train Epoch: 1 [10880/60000]\tLoss: 0.855613\n",
      "Train Epoch: 1 [11520/60000]\tLoss: 0.643409\n",
      "Train Epoch: 1 [12160/60000]\tLoss: 0.709093\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 0.579134\n",
      "Train Epoch: 1 [13440/60000]\tLoss: 0.540085\n",
      "Train Epoch: 1 [14080/60000]\tLoss: 0.568602\n",
      "Train Epoch: 1 [14720/60000]\tLoss: 0.701272\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 0.495959\n",
      "Train Epoch: 1 [16000/60000]\tLoss: 0.727821\n",
      "Train Epoch: 1 [16640/60000]\tLoss: 0.498792\n",
      "Train Epoch: 1 [17280/60000]\tLoss: 0.490455\n",
      "Train Epoch: 1 [17920/60000]\tLoss: 0.575509\n",
      "Train Epoch: 1 [18560/60000]\tLoss: 0.612603\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 0.462887\n",
      "Train Epoch: 1 [19840/60000]\tLoss: 0.556669\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 0.540038\n",
      "Train Epoch: 1 [21120/60000]\tLoss: 0.503309\n",
      "Train Epoch: 1 [21760/60000]\tLoss: 0.502169\n",
      "Train Epoch: 1 [22400/60000]\tLoss: 0.474778\n",
      "Train Epoch: 1 [23040/60000]\tLoss: 0.382771\n",
      "Train Epoch: 1 [23680/60000]\tLoss: 0.452661\n",
      "Train Epoch: 1 [24320/60000]\tLoss: 0.422926\n",
      "Train Epoch: 1 [24960/60000]\tLoss: 0.390907\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 0.315204\n",
      "Train Epoch: 1 [26240/60000]\tLoss: 0.477555\n",
      "Train Epoch: 1 [26880/60000]\tLoss: 0.444898\n",
      "Train Epoch: 1 [27520/60000]\tLoss: 0.299202\n",
      "Train Epoch: 1 [28160/60000]\tLoss: 0.332862\n",
      "Train Epoch: 1 [28800/60000]\tLoss: 0.294372\n",
      "Train Epoch: 1 [29440/60000]\tLoss: 0.318474\n",
      "Train Epoch: 1 [30080/60000]\tLoss: 0.395129\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 0.238530\n",
      "Train Epoch: 1 [31360/60000]\tLoss: 0.343101\n",
      "Train Epoch: 1 [32000/60000]\tLoss: 0.332715\n",
      "Train Epoch: 1 [32640/60000]\tLoss: 0.407069\n",
      "Train Epoch: 1 [33280/60000]\tLoss: 0.309359\n",
      "Train Epoch: 1 [33920/60000]\tLoss: 0.267524\n",
      "Train Epoch: 1 [34560/60000]\tLoss: 0.275816\n",
      "Train Epoch: 1 [35200/60000]\tLoss: 0.194280\n",
      "Train Epoch: 1 [35840/60000]\tLoss: 0.357447\n",
      "Train Epoch: 1 [36480/60000]\tLoss: 0.290903\n",
      "Train Epoch: 1 [37120/60000]\tLoss: 0.262276\n",
      "Train Epoch: 1 [37760/60000]\tLoss: 0.273419\n",
      "Train Epoch: 1 [38400/60000]\tLoss: 0.259412\n",
      "Train Epoch: 1 [39040/60000]\tLoss: 0.357962\n",
      "Train Epoch: 1 [39680/60000]\tLoss: 0.326578\n",
      "Train Epoch: 1 [40320/60000]\tLoss: 0.324750\n",
      "Train Epoch: 1 [40960/60000]\tLoss: 0.345881\n",
      "Train Epoch: 1 [41600/60000]\tLoss: 0.213111\n",
      "Train Epoch: 1 [42240/60000]\tLoss: 0.282468\n",
      "Train Epoch: 1 [42880/60000]\tLoss: 0.413504\n",
      "Train Epoch: 1 [43520/60000]\tLoss: 0.240950\n",
      "Train Epoch: 1 [44160/60000]\tLoss: 0.398652\n",
      "Train Epoch: 1 [44800/60000]\tLoss: 0.414857\n",
      "Train Epoch: 1 [45440/60000]\tLoss: 0.411145\n",
      "Train Epoch: 1 [46080/60000]\tLoss: 0.283673\n",
      "Train Epoch: 1 [46720/60000]\tLoss: 0.378779\n",
      "Train Epoch: 1 [47360/60000]\tLoss: 0.185917\n",
      "Train Epoch: 1 [48000/60000]\tLoss: 0.228213\n",
      "Train Epoch: 1 [48640/60000]\tLoss: 0.264690\n",
      "Train Epoch: 1 [49280/60000]\tLoss: 0.158954\n",
      "Train Epoch: 1 [49920/60000]\tLoss: 0.130116\n",
      "Train Epoch: 1 [50560/60000]\tLoss: 0.134453\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 0.271868\n",
      "Train Epoch: 1 [51840/60000]\tLoss: 0.269393\n",
      "Train Epoch: 1 [52480/60000]\tLoss: 0.118962\n",
      "Train Epoch: 1 [53120/60000]\tLoss: 0.277617\n",
      "Train Epoch: 1 [53760/60000]\tLoss: 0.167855\n",
      "Train Epoch: 1 [54400/60000]\tLoss: 0.269478\n",
      "Train Epoch: 1 [55040/60000]\tLoss: 0.238790\n",
      "Train Epoch: 1 [55680/60000]\tLoss: 0.138727\n",
      "Train Epoch: 1 [56320/60000]\tLoss: 0.221281\n",
      "Train Epoch: 1 [56960/60000]\tLoss: 0.269074\n",
      "Train Epoch: 1 [57600/60000]\tLoss: 0.297458\n",
      "Train Epoch: 1 [58240/60000]\tLoss: 0.195717\n",
      "Train Epoch: 1 [58880/60000]\tLoss: 0.185999\n",
      "Train Epoch: 1 [59520/60000]\tLoss: 0.265839\n",
      "Train Epoch: 2 [0/60000]\tLoss: 0.228868\n",
      "Train Epoch: 2 [640/60000]\tLoss: 0.212627\n",
      "Train Epoch: 2 [1280/60000]\tLoss: 0.243909\n",
      "Train Epoch: 2 [1920/60000]\tLoss: 0.227914\n",
      "Train Epoch: 2 [2560/60000]\tLoss: 0.142260\n",
      "Train Epoch: 2 [3200/60000]\tLoss: 0.182616\n",
      "Train Epoch: 2 [3840/60000]\tLoss: 0.236957\n",
      "Train Epoch: 2 [4480/60000]\tLoss: 0.269394\n",
      "Train Epoch: 2 [5120/60000]\tLoss: 0.132603\n",
      "Train Epoch: 2 [5760/60000]\tLoss: 0.149911\n",
      "Train Epoch: 2 [6400/60000]\tLoss: 0.162855\n",
      "Train Epoch: 2 [7040/60000]\tLoss: 0.151649\n",
      "Train Epoch: 2 [7680/60000]\tLoss: 0.207487\n",
      "Train Epoch: 2 [8320/60000]\tLoss: 0.200885\n",
      "Train Epoch: 2 [8960/60000]\tLoss: 0.400460\n",
      "Train Epoch: 2 [9600/60000]\tLoss: 0.347031\n",
      "Train Epoch: 2 [10240/60000]\tLoss: 0.128472\n",
      "Train Epoch: 2 [10880/60000]\tLoss: 0.144097\n",
      "Train Epoch: 2 [11520/60000]\tLoss: 0.315885\n",
      "Train Epoch: 2 [12160/60000]\tLoss: 0.206460\n",
      "Train Epoch: 2 [12800/60000]\tLoss: 0.119459\n",
      "Train Epoch: 2 [13440/60000]\tLoss: 0.204825\n",
      "Train Epoch: 2 [14080/60000]\tLoss: 0.135948\n",
      "Train Epoch: 2 [14720/60000]\tLoss: 0.187261\n",
      "Train Epoch: 2 [15360/60000]\tLoss: 0.111915\n",
      "Train Epoch: 2 [16000/60000]\tLoss: 0.173023\n",
      "Train Epoch: 2 [16640/60000]\tLoss: 0.175154\n",
      "Train Epoch: 2 [17280/60000]\tLoss: 0.214353\n",
      "Train Epoch: 2 [17920/60000]\tLoss: 0.209739\n",
      "Train Epoch: 2 [18560/60000]\tLoss: 0.197343\n",
      "Train Epoch: 2 [19200/60000]\tLoss: 0.255553\n",
      "Train Epoch: 2 [19840/60000]\tLoss: 0.232155\n",
      "Train Epoch: 2 [20480/60000]\tLoss: 0.232376\n",
      "Train Epoch: 2 [21120/60000]\tLoss: 0.181528\n",
      "Train Epoch: 2 [21760/60000]\tLoss: 0.131978\n",
      "Train Epoch: 2 [22400/60000]\tLoss: 0.154974\n",
      "Train Epoch: 2 [23040/60000]\tLoss: 0.226328\n",
      "Train Epoch: 2 [23680/60000]\tLoss: 0.206389\n",
      "Train Epoch: 2 [24320/60000]\tLoss: 0.248120\n",
      "Train Epoch: 2 [24960/60000]\tLoss: 0.146302\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 0.207809\n",
      "Train Epoch: 2 [26240/60000]\tLoss: 0.177824\n",
      "Train Epoch: 2 [26880/60000]\tLoss: 0.145483\n",
      "Train Epoch: 2 [27520/60000]\tLoss: 0.222543\n",
      "Train Epoch: 2 [28160/60000]\tLoss: 0.115485\n",
      "Train Epoch: 2 [28800/60000]\tLoss: 0.073322\n",
      "Train Epoch: 2 [29440/60000]\tLoss: 0.183815\n",
      "Train Epoch: 2 [30080/60000]\tLoss: 0.138723\n",
      "Train Epoch: 2 [30720/60000]\tLoss: 0.141276\n",
      "Train Epoch: 2 [31360/60000]\tLoss: 0.279522\n",
      "Train Epoch: 2 [32000/60000]\tLoss: 0.065909\n",
      "Train Epoch: 2 [32640/60000]\tLoss: 0.204758\n",
      "Train Epoch: 2 [33280/60000]\tLoss: 0.212975\n",
      "Train Epoch: 2 [33920/60000]\tLoss: 0.190921\n",
      "Train Epoch: 2 [34560/60000]\tLoss: 0.161617\n",
      "Train Epoch: 2 [35200/60000]\tLoss: 0.259373\n",
      "Train Epoch: 2 [35840/60000]\tLoss: 0.096866\n",
      "Train Epoch: 2 [36480/60000]\tLoss: 0.295857\n",
      "Train Epoch: 2 [37120/60000]\tLoss: 0.276729\n",
      "Train Epoch: 2 [37760/60000]\tLoss: 0.226538\n",
      "Train Epoch: 2 [38400/60000]\tLoss: 0.146392\n",
      "Train Epoch: 2 [39040/60000]\tLoss: 0.226814\n",
      "Train Epoch: 2 [39680/60000]\tLoss: 0.063589\n",
      "Train Epoch: 2 [40320/60000]\tLoss: 0.096917\n",
      "Train Epoch: 2 [40960/60000]\tLoss: 0.240031\n",
      "Train Epoch: 2 [41600/60000]\tLoss: 0.119539\n",
      "Train Epoch: 2 [42240/60000]\tLoss: 0.217583\n",
      "Train Epoch: 2 [42880/60000]\tLoss: 0.156513\n",
      "Train Epoch: 2 [43520/60000]\tLoss: 0.129431\n",
      "Train Epoch: 2 [44160/60000]\tLoss: 0.102589\n",
      "Train Epoch: 2 [44800/60000]\tLoss: 0.136486\n",
      "Train Epoch: 2 [45440/60000]\tLoss: 0.144050\n",
      "Train Epoch: 2 [46080/60000]\tLoss: 0.056488\n",
      "Train Epoch: 2 [46720/60000]\tLoss: 0.168887\n",
      "Train Epoch: 2 [47360/60000]\tLoss: 0.097805\n",
      "Train Epoch: 2 [48000/60000]\tLoss: 0.153610\n",
      "Train Epoch: 2 [48640/60000]\tLoss: 0.202104\n",
      "Train Epoch: 2 [49280/60000]\tLoss: 0.124500\n",
      "Train Epoch: 2 [49920/60000]\tLoss: 0.131818\n",
      "Train Epoch: 2 [50560/60000]\tLoss: 0.111375\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.165245\n",
      "Train Epoch: 2 [51840/60000]\tLoss: 0.281313\n",
      "Train Epoch: 2 [52480/60000]\tLoss: 0.282616\n",
      "Train Epoch: 2 [53120/60000]\tLoss: 0.158107\n",
      "Train Epoch: 2 [53760/60000]\tLoss: 0.267238\n",
      "Train Epoch: 2 [54400/60000]\tLoss: 0.073798\n",
      "Train Epoch: 2 [55040/60000]\tLoss: 0.095360\n",
      "Train Epoch: 2 [55680/60000]\tLoss: 0.251116\n",
      "Train Epoch: 2 [56320/60000]\tLoss: 0.178023\n",
      "Train Epoch: 2 [56960/60000]\tLoss: 0.181030\n",
      "Train Epoch: 2 [57600/60000]\tLoss: 0.091214\n",
      "Train Epoch: 2 [58240/60000]\tLoss: 0.171279\n",
      "Train Epoch: 2 [58880/60000]\tLoss: 0.198281\n",
      "Train Epoch: 2 [59520/60000]\tLoss: 0.131264\n",
      "Train Epoch: 3 [0/60000]\tLoss: 0.191530\n",
      "Train Epoch: 3 [640/60000]\tLoss: 0.158763\n",
      "Train Epoch: 3 [1280/60000]\tLoss: 0.053011\n",
      "Train Epoch: 3 [1920/60000]\tLoss: 0.214777\n",
      "Train Epoch: 3 [2560/60000]\tLoss: 0.122830\n",
      "Train Epoch: 3 [3200/60000]\tLoss: 0.117718\n",
      "Train Epoch: 3 [3840/60000]\tLoss: 0.106452\n",
      "Train Epoch: 3 [4480/60000]\tLoss: 0.105977\n",
      "Train Epoch: 3 [5120/60000]\tLoss: 0.099512\n",
      "Train Epoch: 3 [5760/60000]\tLoss: 0.113171\n",
      "Train Epoch: 3 [6400/60000]\tLoss: 0.169701\n",
      "Train Epoch: 3 [7040/60000]\tLoss: 0.156886\n",
      "Train Epoch: 3 [7680/60000]\tLoss: 0.163019\n",
      "Train Epoch: 3 [8320/60000]\tLoss: 0.077140\n",
      "Train Epoch: 3 [8960/60000]\tLoss: 0.133534\n",
      "Train Epoch: 3 [9600/60000]\tLoss: 0.078305\n",
      "Train Epoch: 3 [10240/60000]\tLoss: 0.126909\n",
      "Train Epoch: 3 [10880/60000]\tLoss: 0.165821\n",
      "Train Epoch: 3 [11520/60000]\tLoss: 0.065312\n",
      "Train Epoch: 3 [12160/60000]\tLoss: 0.246396\n",
      "Train Epoch: 3 [12800/60000]\tLoss: 0.153795\n",
      "Train Epoch: 3 [13440/60000]\tLoss: 0.108740\n",
      "Train Epoch: 3 [14080/60000]\tLoss: 0.083613\n",
      "Train Epoch: 3 [14720/60000]\tLoss: 0.171023\n",
      "Train Epoch: 3 [15360/60000]\tLoss: 0.160303\n",
      "Train Epoch: 3 [16000/60000]\tLoss: 0.073164\n",
      "Train Epoch: 3 [16640/60000]\tLoss: 0.139045\n",
      "Train Epoch: 3 [17280/60000]\tLoss: 0.124983\n",
      "Train Epoch: 3 [17920/60000]\tLoss: 0.113503\n",
      "Train Epoch: 3 [18560/60000]\tLoss: 0.092578\n",
      "Train Epoch: 3 [19200/60000]\tLoss: 0.179212\n",
      "Train Epoch: 3 [19840/60000]\tLoss: 0.092408\n",
      "Train Epoch: 3 [20480/60000]\tLoss: 0.093086\n",
      "Train Epoch: 3 [21120/60000]\tLoss: 0.195598\n",
      "Train Epoch: 3 [21760/60000]\tLoss: 0.108742\n",
      "Train Epoch: 3 [22400/60000]\tLoss: 0.147111\n",
      "Train Epoch: 3 [23040/60000]\tLoss: 0.146513\n",
      "Train Epoch: 3 [23680/60000]\tLoss: 0.062361\n",
      "Train Epoch: 3 [24320/60000]\tLoss: 0.106687\n",
      "Train Epoch: 3 [24960/60000]\tLoss: 0.235348\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 0.144178\n",
      "Train Epoch: 3 [26240/60000]\tLoss: 0.166652\n",
      "Train Epoch: 3 [26880/60000]\tLoss: 0.083649\n",
      "Train Epoch: 3 [27520/60000]\tLoss: 0.125557\n",
      "Train Epoch: 3 [28160/60000]\tLoss: 0.162059\n",
      "Train Epoch: 3 [28800/60000]\tLoss: 0.154044\n",
      "Train Epoch: 3 [29440/60000]\tLoss: 0.135784\n",
      "Train Epoch: 3 [30080/60000]\tLoss: 0.084663\n",
      "Train Epoch: 3 [30720/60000]\tLoss: 0.130643\n",
      "Train Epoch: 3 [31360/60000]\tLoss: 0.136785\n",
      "Train Epoch: 3 [32000/60000]\tLoss: 0.043028\n",
      "Train Epoch: 3 [32640/60000]\tLoss: 0.035632\n",
      "Train Epoch: 3 [33280/60000]\tLoss: 0.114923\n",
      "Train Epoch: 3 [33920/60000]\tLoss: 0.271039\n",
      "Train Epoch: 3 [34560/60000]\tLoss: 0.046846\n",
      "Train Epoch: 3 [35200/60000]\tLoss: 0.131962\n",
      "Train Epoch: 3 [35840/60000]\tLoss: 0.037044\n",
      "Train Epoch: 3 [36480/60000]\tLoss: 0.109799\n",
      "Train Epoch: 3 [37120/60000]\tLoss: 0.135694\n",
      "Train Epoch: 3 [37760/60000]\tLoss: 0.108815\n",
      "Train Epoch: 3 [38400/60000]\tLoss: 0.091358\n",
      "Train Epoch: 3 [39040/60000]\tLoss: 0.182958\n",
      "Train Epoch: 3 [39680/60000]\tLoss: 0.158950\n",
      "Train Epoch: 3 [40320/60000]\tLoss: 0.030608\n",
      "Train Epoch: 3 [40960/60000]\tLoss: 0.090417\n",
      "Train Epoch: 3 [41600/60000]\tLoss: 0.114969\n",
      "Train Epoch: 3 [42240/60000]\tLoss: 0.094257\n",
      "Train Epoch: 3 [42880/60000]\tLoss: 0.200977\n",
      "Train Epoch: 3 [43520/60000]\tLoss: 0.029500\n",
      "Train Epoch: 3 [44160/60000]\tLoss: 0.033589\n",
      "Train Epoch: 3 [44800/60000]\tLoss: 0.218856\n",
      "Train Epoch: 3 [45440/60000]\tLoss: 0.046213\n",
      "Train Epoch: 3 [46080/60000]\tLoss: 0.044518\n",
      "Train Epoch: 3 [46720/60000]\tLoss: 0.162267\n",
      "Train Epoch: 3 [47360/60000]\tLoss: 0.134361\n",
      "Train Epoch: 3 [48000/60000]\tLoss: 0.016480\n",
      "Train Epoch: 3 [48640/60000]\tLoss: 0.135192\n",
      "Train Epoch: 3 [49280/60000]\tLoss: 0.156567\n",
      "Train Epoch: 3 [49920/60000]\tLoss: 0.093844\n",
      "Train Epoch: 3 [50560/60000]\tLoss: 0.063957\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.078130\n",
      "Train Epoch: 3 [51840/60000]\tLoss: 0.103784\n",
      "Train Epoch: 3 [52480/60000]\tLoss: 0.082509\n",
      "Train Epoch: 3 [53120/60000]\tLoss: 0.133813\n",
      "Train Epoch: 3 [53760/60000]\tLoss: 0.106315\n",
      "Train Epoch: 3 [54400/60000]\tLoss: 0.067642\n",
      "Train Epoch: 3 [55040/60000]\tLoss: 0.095530\n",
      "Train Epoch: 3 [55680/60000]\tLoss: 0.190864\n",
      "Train Epoch: 3 [56320/60000]\tLoss: 0.091666\n",
      "Train Epoch: 3 [56960/60000]\tLoss: 0.039562\n",
      "Train Epoch: 3 [57600/60000]\tLoss: 0.177657\n",
      "Train Epoch: 3 [58240/60000]\tLoss: 0.030759\n",
      "Train Epoch: 3 [58880/60000]\tLoss: 0.305143\n",
      "Train Epoch: 3 [59520/60000]\tLoss: 0.109857\n",
      "Train Epoch: 4 [0/60000]\tLoss: 0.056683\n",
      "Train Epoch: 4 [640/60000]\tLoss: 0.109963\n",
      "Train Epoch: 4 [1280/60000]\tLoss: 0.136251\n",
      "Train Epoch: 4 [1920/60000]\tLoss: 0.100886\n",
      "Train Epoch: 4 [2560/60000]\tLoss: 0.132468\n",
      "Train Epoch: 4 [3200/60000]\tLoss: 0.108562\n",
      "Train Epoch: 4 [3840/60000]\tLoss: 0.183964\n",
      "Train Epoch: 4 [4480/60000]\tLoss: 0.150125\n",
      "Train Epoch: 4 [5120/60000]\tLoss: 0.059231\n",
      "Train Epoch: 4 [5760/60000]\tLoss: 0.055677\n",
      "Train Epoch: 4 [6400/60000]\tLoss: 0.165411\n",
      "Train Epoch: 4 [7040/60000]\tLoss: 0.176739\n",
      "Train Epoch: 4 [7680/60000]\tLoss: 0.202620\n",
      "Train Epoch: 4 [8320/60000]\tLoss: 0.109808\n",
      "Train Epoch: 4 [8960/60000]\tLoss: 0.029012\n",
      "Train Epoch: 4 [9600/60000]\tLoss: 0.085824\n",
      "Train Epoch: 4 [10240/60000]\tLoss: 0.088051\n",
      "Train Epoch: 4 [10880/60000]\tLoss: 0.094381\n",
      "Train Epoch: 4 [11520/60000]\tLoss: 0.023678\n",
      "Train Epoch: 4 [12160/60000]\tLoss: 0.247524\n",
      "Train Epoch: 4 [12800/60000]\tLoss: 0.121735\n",
      "Train Epoch: 4 [13440/60000]\tLoss: 0.253785\n",
      "Train Epoch: 4 [14080/60000]\tLoss: 0.085628\n",
      "Train Epoch: 4 [14720/60000]\tLoss: 0.140979\n",
      "Train Epoch: 4 [15360/60000]\tLoss: 0.079721\n",
      "Train Epoch: 4 [16000/60000]\tLoss: 0.056824\n",
      "Train Epoch: 4 [16640/60000]\tLoss: 0.181827\n",
      "Train Epoch: 4 [17280/60000]\tLoss: 0.050063\n",
      "Train Epoch: 4 [17920/60000]\tLoss: 0.266295\n",
      "Train Epoch: 4 [18560/60000]\tLoss: 0.119049\n",
      "Train Epoch: 4 [19200/60000]\tLoss: 0.113241\n",
      "Train Epoch: 4 [19840/60000]\tLoss: 0.110424\n",
      "Train Epoch: 4 [20480/60000]\tLoss: 0.110251\n",
      "Train Epoch: 4 [21120/60000]\tLoss: 0.097390\n",
      "Train Epoch: 4 [21760/60000]\tLoss: 0.072436\n",
      "Train Epoch: 4 [22400/60000]\tLoss: 0.110134\n",
      "Train Epoch: 4 [23040/60000]\tLoss: 0.122521\n",
      "Train Epoch: 4 [23680/60000]\tLoss: 0.165990\n",
      "Train Epoch: 4 [24320/60000]\tLoss: 0.116045\n",
      "Train Epoch: 4 [24960/60000]\tLoss: 0.195108\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.067806\n",
      "Train Epoch: 4 [26240/60000]\tLoss: 0.133168\n",
      "Train Epoch: 4 [26880/60000]\tLoss: 0.042071\n",
      "Train Epoch: 4 [27520/60000]\tLoss: 0.042460\n",
      "Train Epoch: 4 [28160/60000]\tLoss: 0.093278\n",
      "Train Epoch: 4 [28800/60000]\tLoss: 0.078652\n",
      "Train Epoch: 4 [29440/60000]\tLoss: 0.221915\n",
      "Train Epoch: 4 [30080/60000]\tLoss: 0.110133\n",
      "Train Epoch: 4 [30720/60000]\tLoss: 0.111437\n",
      "Train Epoch: 4 [31360/60000]\tLoss: 0.070584\n",
      "Train Epoch: 4 [32000/60000]\tLoss: 0.089678\n",
      "Train Epoch: 4 [32640/60000]\tLoss: 0.232050\n",
      "Train Epoch: 4 [33280/60000]\tLoss: 0.138739\n",
      "Train Epoch: 4 [33920/60000]\tLoss: 0.117305\n",
      "Train Epoch: 4 [34560/60000]\tLoss: 0.127269\n",
      "Train Epoch: 4 [35200/60000]\tLoss: 0.121440\n",
      "Train Epoch: 4 [35840/60000]\tLoss: 0.084792\n",
      "Train Epoch: 4 [36480/60000]\tLoss: 0.143378\n",
      "Train Epoch: 4 [37120/60000]\tLoss: 0.089568\n",
      "Train Epoch: 4 [37760/60000]\tLoss: 0.104274\n",
      "Train Epoch: 4 [38400/60000]\tLoss: 0.253456\n",
      "Train Epoch: 4 [39040/60000]\tLoss: 0.117031\n",
      "Train Epoch: 4 [39680/60000]\tLoss: 0.114237\n",
      "Train Epoch: 4 [40320/60000]\tLoss: 0.103646\n",
      "Train Epoch: 4 [40960/60000]\tLoss: 0.058230\n",
      "Train Epoch: 4 [41600/60000]\tLoss: 0.090046\n",
      "Train Epoch: 4 [42240/60000]\tLoss: 0.117470\n",
      "Train Epoch: 4 [42880/60000]\tLoss: 0.149582\n",
      "Train Epoch: 4 [43520/60000]\tLoss: 0.104592\n",
      "Train Epoch: 4 [44160/60000]\tLoss: 0.164401\n",
      "Train Epoch: 4 [44800/60000]\tLoss: 0.068922\n",
      "Train Epoch: 4 [45440/60000]\tLoss: 0.078424\n",
      "Train Epoch: 4 [46080/60000]\tLoss: 0.054690\n",
      "Train Epoch: 4 [46720/60000]\tLoss: 0.103467\n",
      "Train Epoch: 4 [47360/60000]\tLoss: 0.097364\n",
      "Train Epoch: 4 [48000/60000]\tLoss: 0.143131\n",
      "Train Epoch: 4 [48640/60000]\tLoss: 0.037730\n",
      "Train Epoch: 4 [49280/60000]\tLoss: 0.085201\n",
      "Train Epoch: 4 [49920/60000]\tLoss: 0.101892\n",
      "Train Epoch: 4 [50560/60000]\tLoss: 0.170401\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 0.092549\n",
      "Train Epoch: 4 [51840/60000]\tLoss: 0.147096\n",
      "Train Epoch: 4 [52480/60000]\tLoss: 0.164831\n",
      "Train Epoch: 4 [53120/60000]\tLoss: 0.114060\n",
      "Train Epoch: 4 [53760/60000]\tLoss: 0.074166\n",
      "Train Epoch: 4 [54400/60000]\tLoss: 0.082631\n",
      "Train Epoch: 4 [55040/60000]\tLoss: 0.093928\n",
      "Train Epoch: 4 [55680/60000]\tLoss: 0.087465\n",
      "Train Epoch: 4 [56320/60000]\tLoss: 0.096032\n",
      "Train Epoch: 4 [56960/60000]\tLoss: 0.147624\n",
      "Train Epoch: 4 [57600/60000]\tLoss: 0.147894\n",
      "Train Epoch: 4 [58240/60000]\tLoss: 0.153837\n",
      "Train Epoch: 4 [58880/60000]\tLoss: 0.049676\n",
      "Train Epoch: 4 [59520/60000]\tLoss: 0.086611\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.029317\n",
      "Train Epoch: 5 [640/60000]\tLoss: 0.111611\n",
      "Train Epoch: 5 [1280/60000]\tLoss: 0.065964\n",
      "Train Epoch: 5 [1920/60000]\tLoss: 0.025358\n",
      "Train Epoch: 5 [2560/60000]\tLoss: 0.140688\n",
      "Train Epoch: 5 [3200/60000]\tLoss: 0.208760\n",
      "Train Epoch: 5 [3840/60000]\tLoss: 0.071948\n",
      "Train Epoch: 5 [4480/60000]\tLoss: 0.081694\n",
      "Train Epoch: 5 [5120/60000]\tLoss: 0.070822\n",
      "Train Epoch: 5 [5760/60000]\tLoss: 0.116337\n",
      "Train Epoch: 5 [6400/60000]\tLoss: 0.078506\n",
      "Train Epoch: 5 [7040/60000]\tLoss: 0.033428\n",
      "Train Epoch: 5 [7680/60000]\tLoss: 0.084523\n",
      "Train Epoch: 5 [8320/60000]\tLoss: 0.084679\n",
      "Train Epoch: 5 [8960/60000]\tLoss: 0.019551\n",
      "Train Epoch: 5 [9600/60000]\tLoss: 0.070080\n",
      "Train Epoch: 5 [10240/60000]\tLoss: 0.148713\n",
      "Train Epoch: 5 [10880/60000]\tLoss: 0.029269\n",
      "Train Epoch: 5 [11520/60000]\tLoss: 0.138057\n",
      "Train Epoch: 5 [12160/60000]\tLoss: 0.074574\n",
      "Train Epoch: 5 [12800/60000]\tLoss: 0.092239\n",
      "Train Epoch: 5 [13440/60000]\tLoss: 0.052991\n",
      "Train Epoch: 5 [14080/60000]\tLoss: 0.038586\n",
      "Train Epoch: 5 [14720/60000]\tLoss: 0.245124\n",
      "Train Epoch: 5 [15360/60000]\tLoss: 0.101279\n",
      "Train Epoch: 5 [16000/60000]\tLoss: 0.106431\n",
      "Train Epoch: 5 [16640/60000]\tLoss: 0.029600\n",
      "Train Epoch: 5 [17280/60000]\tLoss: 0.110265\n",
      "Train Epoch: 5 [17920/60000]\tLoss: 0.068959\n",
      "Train Epoch: 5 [18560/60000]\tLoss: 0.014026\n",
      "Train Epoch: 5 [19200/60000]\tLoss: 0.045848\n",
      "Train Epoch: 5 [19840/60000]\tLoss: 0.072337\n",
      "Train Epoch: 5 [20480/60000]\tLoss: 0.066278\n",
      "Train Epoch: 5 [21120/60000]\tLoss: 0.104036\n",
      "Train Epoch: 5 [21760/60000]\tLoss: 0.051883\n",
      "Train Epoch: 5 [22400/60000]\tLoss: 0.228609\n",
      "Train Epoch: 5 [23040/60000]\tLoss: 0.094764\n",
      "Train Epoch: 5 [23680/60000]\tLoss: 0.052960\n",
      "Train Epoch: 5 [24320/60000]\tLoss: 0.100174\n",
      "Train Epoch: 5 [24960/60000]\tLoss: 0.189348\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.115832\n",
      "Train Epoch: 5 [26240/60000]\tLoss: 0.035150\n",
      "Train Epoch: 5 [26880/60000]\tLoss: 0.099484\n",
      "Train Epoch: 5 [27520/60000]\tLoss: 0.064116\n",
      "Train Epoch: 5 [28160/60000]\tLoss: 0.104466\n",
      "Train Epoch: 5 [28800/60000]\tLoss: 0.103361\n",
      "Train Epoch: 5 [29440/60000]\tLoss: 0.097903\n",
      "Train Epoch: 5 [30080/60000]\tLoss: 0.037965\n",
      "Train Epoch: 5 [30720/60000]\tLoss: 0.078103\n",
      "Train Epoch: 5 [31360/60000]\tLoss: 0.072204\n",
      "Train Epoch: 5 [32000/60000]\tLoss: 0.124654\n",
      "Train Epoch: 5 [32640/60000]\tLoss: 0.079295\n",
      "Train Epoch: 5 [33280/60000]\tLoss: 0.078326\n",
      "Train Epoch: 5 [33920/60000]\tLoss: 0.311154\n",
      "Train Epoch: 5 [34560/60000]\tLoss: 0.104682\n",
      "Train Epoch: 5 [35200/60000]\tLoss: 0.103351\n",
      "Train Epoch: 5 [35840/60000]\tLoss: 0.084476\n",
      "Train Epoch: 5 [36480/60000]\tLoss: 0.105071\n",
      "Train Epoch: 5 [37120/60000]\tLoss: 0.178511\n",
      "Train Epoch: 5 [37760/60000]\tLoss: 0.079520\n",
      "Train Epoch: 5 [38400/60000]\tLoss: 0.099110\n",
      "Train Epoch: 5 [39040/60000]\tLoss: 0.176135\n",
      "Train Epoch: 5 [39680/60000]\tLoss: 0.141322\n",
      "Train Epoch: 5 [40320/60000]\tLoss: 0.051401\n",
      "Train Epoch: 5 [40960/60000]\tLoss: 0.078547\n",
      "Train Epoch: 5 [41600/60000]\tLoss: 0.079536\n",
      "Train Epoch: 5 [42240/60000]\tLoss: 0.123665\n",
      "Train Epoch: 5 [42880/60000]\tLoss: 0.090071\n",
      "Train Epoch: 5 [43520/60000]\tLoss: 0.103631\n",
      "Train Epoch: 5 [44160/60000]\tLoss: 0.245792\n",
      "Train Epoch: 5 [44800/60000]\tLoss: 0.092660\n",
      "Train Epoch: 5 [45440/60000]\tLoss: 0.057467\n",
      "Train Epoch: 5 [46080/60000]\tLoss: 0.072926\n",
      "Train Epoch: 5 [46720/60000]\tLoss: 0.062820\n",
      "Train Epoch: 5 [47360/60000]\tLoss: 0.202197\n",
      "Train Epoch: 5 [48000/60000]\tLoss: 0.092509\n",
      "Train Epoch: 5 [48640/60000]\tLoss: 0.027657\n",
      "Train Epoch: 5 [49280/60000]\tLoss: 0.160266\n",
      "Train Epoch: 5 [49920/60000]\tLoss: 0.250840\n",
      "Train Epoch: 5 [50560/60000]\tLoss: 0.066201\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.047074\n",
      "Train Epoch: 5 [51840/60000]\tLoss: 0.081298\n",
      "Train Epoch: 5 [52480/60000]\tLoss: 0.126964\n",
      "Train Epoch: 5 [53120/60000]\tLoss: 0.111958\n",
      "Train Epoch: 5 [53760/60000]\tLoss: 0.071357\n",
      "Train Epoch: 5 [54400/60000]\tLoss: 0.273107\n",
      "Train Epoch: 5 [55040/60000]\tLoss: 0.042883\n",
      "Train Epoch: 5 [55680/60000]\tLoss: 0.114513\n",
      "Train Epoch: 5 [56320/60000]\tLoss: 0.135618\n",
      "Train Epoch: 5 [56960/60000]\tLoss: 0.058080\n",
      "Train Epoch: 5 [57600/60000]\tLoss: 0.138762\n",
      "Train Epoch: 5 [58240/60000]\tLoss: 0.128362\n",
      "Train Epoch: 5 [58880/60000]\tLoss: 0.087531\n",
      "Train Epoch: 5 [59520/60000]\tLoss: 0.139315\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.049066\n",
      "Train Epoch: 6 [640/60000]\tLoss: 0.059812\n",
      "Train Epoch: 6 [1280/60000]\tLoss: 0.060610\n",
      "Train Epoch: 6 [1920/60000]\tLoss: 0.087208\n",
      "Train Epoch: 6 [2560/60000]\tLoss: 0.031167\n",
      "Train Epoch: 6 [3200/60000]\tLoss: 0.186003\n",
      "Train Epoch: 6 [3840/60000]\tLoss: 0.116742\n",
      "Train Epoch: 6 [4480/60000]\tLoss: 0.026736\n",
      "Train Epoch: 6 [5120/60000]\tLoss: 0.087569\n",
      "Train Epoch: 6 [5760/60000]\tLoss: 0.106174\n",
      "Train Epoch: 6 [6400/60000]\tLoss: 0.050844\n",
      "Train Epoch: 6 [7040/60000]\tLoss: 0.059225\n",
      "Train Epoch: 6 [7680/60000]\tLoss: 0.182384\n",
      "Train Epoch: 6 [8320/60000]\tLoss: 0.163849\n",
      "Train Epoch: 6 [8960/60000]\tLoss: 0.067702\n",
      "Train Epoch: 6 [9600/60000]\tLoss: 0.042771\n",
      "Train Epoch: 6 [10240/60000]\tLoss: 0.039272\n",
      "Train Epoch: 6 [10880/60000]\tLoss: 0.120517\n",
      "Train Epoch: 6 [11520/60000]\tLoss: 0.032608\n",
      "Train Epoch: 6 [12160/60000]\tLoss: 0.038771\n",
      "Train Epoch: 6 [12800/60000]\tLoss: 0.128344\n",
      "Train Epoch: 6 [13440/60000]\tLoss: 0.057892\n",
      "Train Epoch: 6 [14080/60000]\tLoss: 0.088596\n",
      "Train Epoch: 6 [14720/60000]\tLoss: 0.039035\n",
      "Train Epoch: 6 [15360/60000]\tLoss: 0.036822\n",
      "Train Epoch: 6 [16000/60000]\tLoss: 0.063801\n",
      "Train Epoch: 6 [16640/60000]\tLoss: 0.053421\n",
      "Train Epoch: 6 [17280/60000]\tLoss: 0.047359\n",
      "Train Epoch: 6 [17920/60000]\tLoss: 0.063905\n",
      "Train Epoch: 6 [18560/60000]\tLoss: 0.071799\n",
      "Train Epoch: 6 [19200/60000]\tLoss: 0.190826\n",
      "Train Epoch: 6 [19840/60000]\tLoss: 0.078675\n",
      "Train Epoch: 6 [20480/60000]\tLoss: 0.055546\n",
      "Train Epoch: 6 [21120/60000]\tLoss: 0.152903\n",
      "Train Epoch: 6 [21760/60000]\tLoss: 0.133742\n",
      "Train Epoch: 6 [22400/60000]\tLoss: 0.165354\n",
      "Train Epoch: 6 [23040/60000]\tLoss: 0.241094\n",
      "Train Epoch: 6 [23680/60000]\tLoss: 0.048009\n",
      "Train Epoch: 6 [24320/60000]\tLoss: 0.088402\n",
      "Train Epoch: 6 [24960/60000]\tLoss: 0.131335\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.079888\n",
      "Train Epoch: 6 [26240/60000]\tLoss: 0.057221\n",
      "Train Epoch: 6 [26880/60000]\tLoss: 0.075273\n",
      "Train Epoch: 6 [27520/60000]\tLoss: 0.110550\n",
      "Train Epoch: 6 [28160/60000]\tLoss: 0.084337\n",
      "Train Epoch: 6 [28800/60000]\tLoss: 0.157094\n",
      "Train Epoch: 6 [29440/60000]\tLoss: 0.044649\n",
      "Train Epoch: 6 [30080/60000]\tLoss: 0.058978\n",
      "Train Epoch: 6 [30720/60000]\tLoss: 0.114977\n",
      "Train Epoch: 6 [31360/60000]\tLoss: 0.025966\n",
      "Train Epoch: 6 [32000/60000]\tLoss: 0.088034\n",
      "Train Epoch: 6 [32640/60000]\tLoss: 0.056040\n",
      "Train Epoch: 6 [33280/60000]\tLoss: 0.033991\n",
      "Train Epoch: 6 [33920/60000]\tLoss: 0.043982\n",
      "Train Epoch: 6 [34560/60000]\tLoss: 0.020501\n",
      "Train Epoch: 6 [35200/60000]\tLoss: 0.098961\n",
      "Train Epoch: 6 [35840/60000]\tLoss: 0.132233\n",
      "Train Epoch: 6 [36480/60000]\tLoss: 0.054929\n",
      "Train Epoch: 6 [37120/60000]\tLoss: 0.053816\n",
      "Train Epoch: 6 [37760/60000]\tLoss: 0.042776\n",
      "Train Epoch: 6 [38400/60000]\tLoss: 0.182875\n",
      "Train Epoch: 6 [39040/60000]\tLoss: 0.147707\n",
      "Train Epoch: 6 [39680/60000]\tLoss: 0.137421\n",
      "Train Epoch: 6 [40320/60000]\tLoss: 0.068471\n",
      "Train Epoch: 6 [40960/60000]\tLoss: 0.204646\n",
      "Train Epoch: 6 [41600/60000]\tLoss: 0.107333\n",
      "Train Epoch: 6 [42240/60000]\tLoss: 0.169799\n",
      "Train Epoch: 6 [42880/60000]\tLoss: 0.082171\n",
      "Train Epoch: 6 [43520/60000]\tLoss: 0.037222\n",
      "Train Epoch: 6 [44160/60000]\tLoss: 0.069929\n",
      "Train Epoch: 6 [44800/60000]\tLoss: 0.085309\n",
      "Train Epoch: 6 [45440/60000]\tLoss: 0.057091\n",
      "Train Epoch: 6 [46080/60000]\tLoss: 0.105384\n",
      "Train Epoch: 6 [46720/60000]\tLoss: 0.070642\n",
      "Train Epoch: 6 [47360/60000]\tLoss: 0.028590\n",
      "Train Epoch: 6 [48000/60000]\tLoss: 0.118803\n",
      "Train Epoch: 6 [48640/60000]\tLoss: 0.068689\n",
      "Train Epoch: 6 [49280/60000]\tLoss: 0.035145\n",
      "Train Epoch: 6 [49920/60000]\tLoss: 0.046266\n",
      "Train Epoch: 6 [50560/60000]\tLoss: 0.207964\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 0.141116\n",
      "Train Epoch: 6 [51840/60000]\tLoss: 0.166199\n",
      "Train Epoch: 6 [52480/60000]\tLoss: 0.059907\n",
      "Train Epoch: 6 [53120/60000]\tLoss: 0.039798\n",
      "Train Epoch: 6 [53760/60000]\tLoss: 0.058430\n",
      "Train Epoch: 6 [54400/60000]\tLoss: 0.082459\n",
      "Train Epoch: 6 [55040/60000]\tLoss: 0.076837\n",
      "Train Epoch: 6 [55680/60000]\tLoss: 0.066742\n",
      "Train Epoch: 6 [56320/60000]\tLoss: 0.068830\n",
      "Train Epoch: 6 [56960/60000]\tLoss: 0.090827\n",
      "Train Epoch: 6 [57600/60000]\tLoss: 0.125464\n",
      "Train Epoch: 6 [58240/60000]\tLoss: 0.109638\n",
      "Train Epoch: 6 [58880/60000]\tLoss: 0.037200\n",
      "Train Epoch: 6 [59520/60000]\tLoss: 0.007830\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.092933\n",
      "Train Epoch: 7 [640/60000]\tLoss: 0.114803\n",
      "Train Epoch: 7 [1280/60000]\tLoss: 0.105633\n",
      "Train Epoch: 7 [1920/60000]\tLoss: 0.170150\n",
      "Train Epoch: 7 [2560/60000]\tLoss: 0.124016\n",
      "Train Epoch: 7 [3200/60000]\tLoss: 0.104287\n",
      "Train Epoch: 7 [3840/60000]\tLoss: 0.047781\n",
      "Train Epoch: 7 [4480/60000]\tLoss: 0.057209\n",
      "Train Epoch: 7 [5120/60000]\tLoss: 0.034854\n",
      "Train Epoch: 7 [5760/60000]\tLoss: 0.120658\n",
      "Train Epoch: 7 [6400/60000]\tLoss: 0.148983\n",
      "Train Epoch: 7 [7040/60000]\tLoss: 0.079336\n",
      "Train Epoch: 7 [7680/60000]\tLoss: 0.036419\n",
      "Train Epoch: 7 [8320/60000]\tLoss: 0.026617\n",
      "Train Epoch: 7 [8960/60000]\tLoss: 0.106197\n",
      "Train Epoch: 7 [9600/60000]\tLoss: 0.038229\n",
      "Train Epoch: 7 [10240/60000]\tLoss: 0.080637\n",
      "Train Epoch: 7 [10880/60000]\tLoss: 0.101500\n",
      "Train Epoch: 7 [11520/60000]\tLoss: 0.071815\n",
      "Train Epoch: 7 [12160/60000]\tLoss: 0.062590\n",
      "Train Epoch: 7 [12800/60000]\tLoss: 0.045980\n",
      "Train Epoch: 7 [13440/60000]\tLoss: 0.056336\n",
      "Train Epoch: 7 [14080/60000]\tLoss: 0.042836\n",
      "Train Epoch: 7 [14720/60000]\tLoss: 0.064590\n",
      "Train Epoch: 7 [15360/60000]\tLoss: 0.113041\n",
      "Train Epoch: 7 [16000/60000]\tLoss: 0.134603\n",
      "Train Epoch: 7 [16640/60000]\tLoss: 0.074355\n",
      "Train Epoch: 7 [17280/60000]\tLoss: 0.078691\n",
      "Train Epoch: 7 [17920/60000]\tLoss: 0.045324\n",
      "Train Epoch: 7 [18560/60000]\tLoss: 0.156316\n",
      "Train Epoch: 7 [19200/60000]\tLoss: 0.081971\n",
      "Train Epoch: 7 [19840/60000]\tLoss: 0.097170\n",
      "Train Epoch: 7 [20480/60000]\tLoss: 0.119368\n",
      "Train Epoch: 7 [21120/60000]\tLoss: 0.043448\n",
      "Train Epoch: 7 [21760/60000]\tLoss: 0.115474\n",
      "Train Epoch: 7 [22400/60000]\tLoss: 0.190982\n",
      "Train Epoch: 7 [23040/60000]\tLoss: 0.045652\n",
      "Train Epoch: 7 [23680/60000]\tLoss: 0.074861\n",
      "Train Epoch: 7 [24320/60000]\tLoss: 0.017026\n",
      "Train Epoch: 7 [24960/60000]\tLoss: 0.151249\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.169986\n",
      "Train Epoch: 7 [26240/60000]\tLoss: 0.062808\n",
      "Train Epoch: 7 [26880/60000]\tLoss: 0.019913\n",
      "Train Epoch: 7 [27520/60000]\tLoss: 0.156995\n",
      "Train Epoch: 7 [28160/60000]\tLoss: 0.050084\n",
      "Train Epoch: 7 [28800/60000]\tLoss: 0.108790\n",
      "Train Epoch: 7 [29440/60000]\tLoss: 0.034144\n",
      "Train Epoch: 7 [30080/60000]\tLoss: 0.183829\n",
      "Train Epoch: 7 [30720/60000]\tLoss: 0.162741\n",
      "Train Epoch: 7 [31360/60000]\tLoss: 0.167952\n",
      "Train Epoch: 7 [32000/60000]\tLoss: 0.018863\n",
      "Train Epoch: 7 [32640/60000]\tLoss: 0.044651\n",
      "Train Epoch: 7 [33280/60000]\tLoss: 0.029023\n",
      "Train Epoch: 7 [33920/60000]\tLoss: 0.049143\n",
      "Train Epoch: 7 [34560/60000]\tLoss: 0.043968\n",
      "Train Epoch: 7 [35200/60000]\tLoss: 0.067988\n",
      "Train Epoch: 7 [35840/60000]\tLoss: 0.033164\n",
      "Train Epoch: 7 [36480/60000]\tLoss: 0.102546\n",
      "Train Epoch: 7 [37120/60000]\tLoss: 0.041558\n",
      "Train Epoch: 7 [37760/60000]\tLoss: 0.104925\n",
      "Train Epoch: 7 [38400/60000]\tLoss: 0.046041\n",
      "Train Epoch: 7 [39040/60000]\tLoss: 0.074011\n",
      "Train Epoch: 7 [39680/60000]\tLoss: 0.118308\n",
      "Train Epoch: 7 [40320/60000]\tLoss: 0.095955\n",
      "Train Epoch: 7 [40960/60000]\tLoss: 0.049314\n",
      "Train Epoch: 7 [41600/60000]\tLoss: 0.094349\n",
      "Train Epoch: 7 [42240/60000]\tLoss: 0.055371\n",
      "Train Epoch: 7 [42880/60000]\tLoss: 0.059239\n",
      "Train Epoch: 7 [43520/60000]\tLoss: 0.035082\n",
      "Train Epoch: 7 [44160/60000]\tLoss: 0.057300\n",
      "Train Epoch: 7 [44800/60000]\tLoss: 0.133898\n",
      "Train Epoch: 7 [45440/60000]\tLoss: 0.026950\n",
      "Train Epoch: 7 [46080/60000]\tLoss: 0.014781\n",
      "Train Epoch: 7 [46720/60000]\tLoss: 0.056273\n",
      "Train Epoch: 7 [47360/60000]\tLoss: 0.088798\n",
      "Train Epoch: 7 [48000/60000]\tLoss: 0.054422\n",
      "Train Epoch: 7 [48640/60000]\tLoss: 0.108057\n",
      "Train Epoch: 7 [49280/60000]\tLoss: 0.153265\n",
      "Train Epoch: 7 [49920/60000]\tLoss: 0.057219\n",
      "Train Epoch: 7 [50560/60000]\tLoss: 0.094117\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 0.143255\n",
      "Train Epoch: 7 [51840/60000]\tLoss: 0.164404\n",
      "Train Epoch: 7 [52480/60000]\tLoss: 0.101959\n",
      "Train Epoch: 7 [53120/60000]\tLoss: 0.124106\n",
      "Train Epoch: 7 [53760/60000]\tLoss: 0.111049\n",
      "Train Epoch: 7 [54400/60000]\tLoss: 0.044801\n",
      "Train Epoch: 7 [55040/60000]\tLoss: 0.032576\n",
      "Train Epoch: 7 [55680/60000]\tLoss: 0.054383\n",
      "Train Epoch: 7 [56320/60000]\tLoss: 0.058232\n",
      "Train Epoch: 7 [56960/60000]\tLoss: 0.028794\n",
      "Train Epoch: 7 [57600/60000]\tLoss: 0.086788\n",
      "Train Epoch: 7 [58240/60000]\tLoss: 0.082905\n",
      "Train Epoch: 7 [58880/60000]\tLoss: 0.023782\n",
      "Train Epoch: 7 [59520/60000]\tLoss: 0.052922\n",
      "Train Epoch: 8 [0/60000]\tLoss: 0.038329\n",
      "Train Epoch: 8 [640/60000]\tLoss: 0.019207\n",
      "Train Epoch: 8 [1280/60000]\tLoss: 0.044093\n",
      "Train Epoch: 8 [1920/60000]\tLoss: 0.015688\n",
      "Train Epoch: 8 [2560/60000]\tLoss: 0.058697\n",
      "Train Epoch: 8 [3200/60000]\tLoss: 0.019869\n",
      "Train Epoch: 8 [3840/60000]\tLoss: 0.098970\n",
      "Train Epoch: 8 [4480/60000]\tLoss: 0.019470\n",
      "Train Epoch: 8 [5120/60000]\tLoss: 0.027428\n",
      "Train Epoch: 8 [5760/60000]\tLoss: 0.032532\n",
      "Train Epoch: 8 [6400/60000]\tLoss: 0.043650\n",
      "Train Epoch: 8 [7040/60000]\tLoss: 0.068758\n",
      "Train Epoch: 8 [7680/60000]\tLoss: 0.039875\n",
      "Train Epoch: 8 [8320/60000]\tLoss: 0.096957\n",
      "Train Epoch: 8 [8960/60000]\tLoss: 0.020581\n",
      "Train Epoch: 8 [9600/60000]\tLoss: 0.014586\n",
      "Train Epoch: 8 [10240/60000]\tLoss: 0.071331\n",
      "Train Epoch: 8 [10880/60000]\tLoss: 0.059511\n",
      "Train Epoch: 8 [11520/60000]\tLoss: 0.146030\n",
      "Train Epoch: 8 [12160/60000]\tLoss: 0.089589\n",
      "Train Epoch: 8 [12800/60000]\tLoss: 0.041150\n",
      "Train Epoch: 8 [13440/60000]\tLoss: 0.061078\n",
      "Train Epoch: 8 [14080/60000]\tLoss: 0.025582\n",
      "Train Epoch: 8 [14720/60000]\tLoss: 0.120928\n",
      "Train Epoch: 8 [15360/60000]\tLoss: 0.083245\n",
      "Train Epoch: 8 [16000/60000]\tLoss: 0.119213\n",
      "Train Epoch: 8 [16640/60000]\tLoss: 0.027044\n",
      "Train Epoch: 8 [17280/60000]\tLoss: 0.031260\n",
      "Train Epoch: 8 [17920/60000]\tLoss: 0.036804\n",
      "Train Epoch: 8 [18560/60000]\tLoss: 0.042675\n",
      "Train Epoch: 8 [19200/60000]\tLoss: 0.040789\n",
      "Train Epoch: 8 [19840/60000]\tLoss: 0.067903\n",
      "Train Epoch: 8 [20480/60000]\tLoss: 0.045977\n",
      "Train Epoch: 8 [21120/60000]\tLoss: 0.160165\n",
      "Train Epoch: 8 [21760/60000]\tLoss: 0.109127\n",
      "Train Epoch: 8 [22400/60000]\tLoss: 0.078958\n",
      "Train Epoch: 8 [23040/60000]\tLoss: 0.048976\n",
      "Train Epoch: 8 [23680/60000]\tLoss: 0.053696\n",
      "Train Epoch: 8 [24320/60000]\tLoss: 0.044126\n",
      "Train Epoch: 8 [24960/60000]\tLoss: 0.042154\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.069869\n",
      "Train Epoch: 8 [26240/60000]\tLoss: 0.088996\n",
      "Train Epoch: 8 [26880/60000]\tLoss: 0.029923\n",
      "Train Epoch: 8 [27520/60000]\tLoss: 0.084983\n",
      "Train Epoch: 8 [28160/60000]\tLoss: 0.027243\n",
      "Train Epoch: 8 [28800/60000]\tLoss: 0.091496\n",
      "Train Epoch: 8 [29440/60000]\tLoss: 0.077223\n",
      "Train Epoch: 8 [30080/60000]\tLoss: 0.049007\n",
      "Train Epoch: 8 [30720/60000]\tLoss: 0.139150\n",
      "Train Epoch: 8 [31360/60000]\tLoss: 0.071882\n",
      "Train Epoch: 8 [32000/60000]\tLoss: 0.054766\n",
      "Train Epoch: 8 [32640/60000]\tLoss: 0.084012\n",
      "Train Epoch: 8 [33280/60000]\tLoss: 0.105882\n",
      "Train Epoch: 8 [33920/60000]\tLoss: 0.139015\n",
      "Train Epoch: 8 [34560/60000]\tLoss: 0.033439\n",
      "Train Epoch: 8 [35200/60000]\tLoss: 0.084609\n",
      "Train Epoch: 8 [35840/60000]\tLoss: 0.125177\n",
      "Train Epoch: 8 [36480/60000]\tLoss: 0.040832\n",
      "Train Epoch: 8 [37120/60000]\tLoss: 0.077128\n",
      "Train Epoch: 8 [37760/60000]\tLoss: 0.048951\n",
      "Train Epoch: 8 [38400/60000]\tLoss: 0.072918\n",
      "Train Epoch: 8 [39040/60000]\tLoss: 0.045817\n",
      "Train Epoch: 8 [39680/60000]\tLoss: 0.089789\n",
      "Train Epoch: 8 [40320/60000]\tLoss: 0.059844\n",
      "Train Epoch: 8 [40960/60000]\tLoss: 0.039740\n",
      "Train Epoch: 8 [41600/60000]\tLoss: 0.061756\n",
      "Train Epoch: 8 [42240/60000]\tLoss: 0.050678\n",
      "Train Epoch: 8 [42880/60000]\tLoss: 0.037408\n",
      "Train Epoch: 8 [43520/60000]\tLoss: 0.147017\n",
      "Train Epoch: 8 [44160/60000]\tLoss: 0.066358\n",
      "Train Epoch: 8 [44800/60000]\tLoss: 0.079376\n",
      "Train Epoch: 8 [45440/60000]\tLoss: 0.069150\n",
      "Train Epoch: 8 [46080/60000]\tLoss: 0.066769\n",
      "Train Epoch: 8 [46720/60000]\tLoss: 0.023280\n",
      "Train Epoch: 8 [47360/60000]\tLoss: 0.050411\n",
      "Train Epoch: 8 [48000/60000]\tLoss: 0.019335\n",
      "Train Epoch: 8 [48640/60000]\tLoss: 0.115797\n",
      "Train Epoch: 8 [49280/60000]\tLoss: 0.095653\n",
      "Train Epoch: 8 [49920/60000]\tLoss: 0.054455\n",
      "Train Epoch: 8 [50560/60000]\tLoss: 0.062620\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.102621\n",
      "Train Epoch: 8 [51840/60000]\tLoss: 0.013758\n",
      "Train Epoch: 8 [52480/60000]\tLoss: 0.017433\n",
      "Train Epoch: 8 [53120/60000]\tLoss: 0.039753\n",
      "Train Epoch: 8 [53760/60000]\tLoss: 0.224165\n",
      "Train Epoch: 8 [54400/60000]\tLoss: 0.050550\n",
      "Train Epoch: 8 [55040/60000]\tLoss: 0.120452\n",
      "Train Epoch: 8 [55680/60000]\tLoss: 0.052913\n",
      "Train Epoch: 8 [56320/60000]\tLoss: 0.054744\n",
      "Train Epoch: 8 [56960/60000]\tLoss: 0.121808\n",
      "Train Epoch: 8 [57600/60000]\tLoss: 0.083210\n",
      "Train Epoch: 8 [58240/60000]\tLoss: 0.050604\n",
      "Train Epoch: 8 [58880/60000]\tLoss: 0.023700\n",
      "Train Epoch: 8 [59520/60000]\tLoss: 0.087012\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.095090\n",
      "Train Epoch: 9 [640/60000]\tLoss: 0.038845\n",
      "Train Epoch: 9 [1280/60000]\tLoss: 0.124500\n",
      "Train Epoch: 9 [1920/60000]\tLoss: 0.044708\n",
      "Train Epoch: 9 [2560/60000]\tLoss: 0.069878\n",
      "Train Epoch: 9 [3200/60000]\tLoss: 0.060519\n",
      "Train Epoch: 9 [3840/60000]\tLoss: 0.076632\n",
      "Train Epoch: 9 [4480/60000]\tLoss: 0.009638\n",
      "Train Epoch: 9 [5120/60000]\tLoss: 0.074701\n",
      "Train Epoch: 9 [5760/60000]\tLoss: 0.016395\n",
      "Train Epoch: 9 [6400/60000]\tLoss: 0.024549\n",
      "Train Epoch: 9 [7040/60000]\tLoss: 0.055472\n",
      "Train Epoch: 9 [7680/60000]\tLoss: 0.095453\n",
      "Train Epoch: 9 [8320/60000]\tLoss: 0.035158\n",
      "Train Epoch: 9 [8960/60000]\tLoss: 0.030452\n",
      "Train Epoch: 9 [9600/60000]\tLoss: 0.104657\n",
      "Train Epoch: 9 [10240/60000]\tLoss: 0.139983\n",
      "Train Epoch: 9 [10880/60000]\tLoss: 0.098074\n",
      "Train Epoch: 9 [11520/60000]\tLoss: 0.060176\n",
      "Train Epoch: 9 [12160/60000]\tLoss: 0.085697\n",
      "Train Epoch: 9 [12800/60000]\tLoss: 0.036012\n",
      "Train Epoch: 9 [13440/60000]\tLoss: 0.075900\n",
      "Train Epoch: 9 [14080/60000]\tLoss: 0.065550\n",
      "Train Epoch: 9 [14720/60000]\tLoss: 0.118612\n",
      "Train Epoch: 9 [15360/60000]\tLoss: 0.107346\n",
      "Train Epoch: 9 [16000/60000]\tLoss: 0.062799\n",
      "Train Epoch: 9 [16640/60000]\tLoss: 0.094832\n",
      "Train Epoch: 9 [17280/60000]\tLoss: 0.117309\n",
      "Train Epoch: 9 [17920/60000]\tLoss: 0.098000\n",
      "Train Epoch: 9 [18560/60000]\tLoss: 0.090795\n",
      "Train Epoch: 9 [19200/60000]\tLoss: 0.246763\n",
      "Train Epoch: 9 [19840/60000]\tLoss: 0.009292\n",
      "Train Epoch: 9 [20480/60000]\tLoss: 0.047346\n",
      "Train Epoch: 9 [21120/60000]\tLoss: 0.061406\n",
      "Train Epoch: 9 [21760/60000]\tLoss: 0.078468\n",
      "Train Epoch: 9 [22400/60000]\tLoss: 0.141019\n",
      "Train Epoch: 9 [23040/60000]\tLoss: 0.052257\n",
      "Train Epoch: 9 [23680/60000]\tLoss: 0.030018\n",
      "Train Epoch: 9 [24320/60000]\tLoss: 0.042980\n",
      "Train Epoch: 9 [24960/60000]\tLoss: 0.020537\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.180117\n",
      "Train Epoch: 9 [26240/60000]\tLoss: 0.105580\n",
      "Train Epoch: 9 [26880/60000]\tLoss: 0.028404\n",
      "Train Epoch: 9 [27520/60000]\tLoss: 0.084868\n",
      "Train Epoch: 9 [28160/60000]\tLoss: 0.055713\n",
      "Train Epoch: 9 [28800/60000]\tLoss: 0.050455\n",
      "Train Epoch: 9 [29440/60000]\tLoss: 0.043394\n",
      "Train Epoch: 9 [30080/60000]\tLoss: 0.119795\n",
      "Train Epoch: 9 [30720/60000]\tLoss: 0.092579\n",
      "Train Epoch: 9 [31360/60000]\tLoss: 0.071695\n",
      "Train Epoch: 9 [32000/60000]\tLoss: 0.141816\n",
      "Train Epoch: 9 [32640/60000]\tLoss: 0.072001\n",
      "Train Epoch: 9 [33280/60000]\tLoss: 0.041433\n",
      "Train Epoch: 9 [33920/60000]\tLoss: 0.037883\n",
      "Train Epoch: 9 [34560/60000]\tLoss: 0.055881\n",
      "Train Epoch: 9 [35200/60000]\tLoss: 0.061351\n",
      "Train Epoch: 9 [35840/60000]\tLoss: 0.062266\n",
      "Train Epoch: 9 [36480/60000]\tLoss: 0.063598\n",
      "Train Epoch: 9 [37120/60000]\tLoss: 0.071954\n",
      "Train Epoch: 9 [37760/60000]\tLoss: 0.050684\n",
      "Train Epoch: 9 [38400/60000]\tLoss: 0.184039\n",
      "Train Epoch: 9 [39040/60000]\tLoss: 0.081753\n",
      "Train Epoch: 9 [39680/60000]\tLoss: 0.136107\n",
      "Train Epoch: 9 [40320/60000]\tLoss: 0.094982\n",
      "Train Epoch: 9 [40960/60000]\tLoss: 0.047417\n",
      "Train Epoch: 9 [41600/60000]\tLoss: 0.029328\n",
      "Train Epoch: 9 [42240/60000]\tLoss: 0.096246\n",
      "Train Epoch: 9 [42880/60000]\tLoss: 0.132808\n",
      "Train Epoch: 9 [43520/60000]\tLoss: 0.037679\n",
      "Train Epoch: 9 [44160/60000]\tLoss: 0.054690\n",
      "Train Epoch: 9 [44800/60000]\tLoss: 0.041036\n",
      "Train Epoch: 9 [45440/60000]\tLoss: 0.050498\n",
      "Train Epoch: 9 [46080/60000]\tLoss: 0.052806\n",
      "Train Epoch: 9 [46720/60000]\tLoss: 0.079484\n",
      "Train Epoch: 9 [47360/60000]\tLoss: 0.037601\n",
      "Train Epoch: 9 [48000/60000]\tLoss: 0.034879\n",
      "Train Epoch: 9 [48640/60000]\tLoss: 0.042955\n",
      "Train Epoch: 9 [49280/60000]\tLoss: 0.286307\n",
      "Train Epoch: 9 [49920/60000]\tLoss: 0.043440\n",
      "Train Epoch: 9 [50560/60000]\tLoss: 0.119980\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.059533\n",
      "Train Epoch: 9 [51840/60000]\tLoss: 0.184509\n",
      "Train Epoch: 9 [52480/60000]\tLoss: 0.137833\n",
      "Train Epoch: 9 [53120/60000]\tLoss: 0.021983\n",
      "Train Epoch: 9 [53760/60000]\tLoss: 0.097669\n",
      "Train Epoch: 9 [54400/60000]\tLoss: 0.040669\n",
      "Train Epoch: 9 [55040/60000]\tLoss: 0.216189\n",
      "Train Epoch: 9 [55680/60000]\tLoss: 0.023541\n",
      "Train Epoch: 9 [56320/60000]\tLoss: 0.052157\n",
      "Train Epoch: 9 [56960/60000]\tLoss: 0.131392\n",
      "Train Epoch: 9 [57600/60000]\tLoss: 0.057420\n",
      "Train Epoch: 9 [58240/60000]\tLoss: 0.022780\n",
      "Train Epoch: 9 [58880/60000]\tLoss: 0.085901\n",
      "Train Epoch: 9 [59520/60000]\tLoss: 0.114189\n",
      "Train Epoch: 10 [0/60000]\tLoss: 0.108312\n",
      "Train Epoch: 10 [640/60000]\tLoss: 0.022452\n",
      "Train Epoch: 10 [1280/60000]\tLoss: 0.112822\n",
      "Train Epoch: 10 [1920/60000]\tLoss: 0.020926\n",
      "Train Epoch: 10 [2560/60000]\tLoss: 0.171222\n",
      "Train Epoch: 10 [3200/60000]\tLoss: 0.160072\n",
      "Train Epoch: 10 [3840/60000]\tLoss: 0.088576\n",
      "Train Epoch: 10 [4480/60000]\tLoss: 0.012372\n",
      "Train Epoch: 10 [5120/60000]\tLoss: 0.063382\n",
      "Train Epoch: 10 [5760/60000]\tLoss: 0.019859\n",
      "Train Epoch: 10 [6400/60000]\tLoss: 0.041029\n",
      "Train Epoch: 10 [7040/60000]\tLoss: 0.086429\n",
      "Train Epoch: 10 [7680/60000]\tLoss: 0.025675\n",
      "Train Epoch: 10 [8320/60000]\tLoss: 0.019949\n",
      "Train Epoch: 10 [8960/60000]\tLoss: 0.037567\n",
      "Train Epoch: 10 [9600/60000]\tLoss: 0.075317\n",
      "Train Epoch: 10 [10240/60000]\tLoss: 0.087850\n",
      "Train Epoch: 10 [10880/60000]\tLoss: 0.132879\n",
      "Train Epoch: 10 [11520/60000]\tLoss: 0.044432\n",
      "Train Epoch: 10 [12160/60000]\tLoss: 0.021184\n",
      "Train Epoch: 10 [12800/60000]\tLoss: 0.035975\n",
      "Train Epoch: 10 [13440/60000]\tLoss: 0.017413\n",
      "Train Epoch: 10 [14080/60000]\tLoss: 0.100000\n",
      "Train Epoch: 10 [14720/60000]\tLoss: 0.099146\n",
      "Train Epoch: 10 [15360/60000]\tLoss: 0.038796\n",
      "Train Epoch: 10 [16000/60000]\tLoss: 0.010332\n",
      "Train Epoch: 10 [16640/60000]\tLoss: 0.078006\n",
      "Train Epoch: 10 [17280/60000]\tLoss: 0.108734\n",
      "Train Epoch: 10 [17920/60000]\tLoss: 0.034918\n",
      "Train Epoch: 10 [18560/60000]\tLoss: 0.159261\n",
      "Train Epoch: 10 [19200/60000]\tLoss: 0.129812\n",
      "Train Epoch: 10 [19840/60000]\tLoss: 0.030330\n",
      "Train Epoch: 10 [20480/60000]\tLoss: 0.053440\n",
      "Train Epoch: 10 [21120/60000]\tLoss: 0.015431\n",
      "Train Epoch: 10 [21760/60000]\tLoss: 0.014756\n",
      "Train Epoch: 10 [22400/60000]\tLoss: 0.052830\n",
      "Train Epoch: 10 [23040/60000]\tLoss: 0.068274\n",
      "Train Epoch: 10 [23680/60000]\tLoss: 0.026300\n",
      "Train Epoch: 10 [24320/60000]\tLoss: 0.036300\n",
      "Train Epoch: 10 [24960/60000]\tLoss: 0.029340\n",
      "Train Epoch: 10 [25600/60000]\tLoss: 0.026318\n",
      "Train Epoch: 10 [26240/60000]\tLoss: 0.050862\n",
      "Train Epoch: 10 [26880/60000]\tLoss: 0.060915\n",
      "Train Epoch: 10 [27520/60000]\tLoss: 0.109969\n",
      "Train Epoch: 10 [28160/60000]\tLoss: 0.071054\n",
      "Train Epoch: 10 [28800/60000]\tLoss: 0.076367\n",
      "Train Epoch: 10 [29440/60000]\tLoss: 0.125989\n",
      "Train Epoch: 10 [30080/60000]\tLoss: 0.035194\n",
      "Train Epoch: 10 [30720/60000]\tLoss: 0.064628\n",
      "Train Epoch: 10 [31360/60000]\tLoss: 0.020103\n",
      "Train Epoch: 10 [32000/60000]\tLoss: 0.027937\n",
      "Train Epoch: 10 [32640/60000]\tLoss: 0.164366\n",
      "Train Epoch: 10 [33280/60000]\tLoss: 0.016907\n",
      "Train Epoch: 10 [33920/60000]\tLoss: 0.060651\n",
      "Train Epoch: 10 [34560/60000]\tLoss: 0.018208\n",
      "Train Epoch: 10 [35200/60000]\tLoss: 0.107824\n",
      "Train Epoch: 10 [35840/60000]\tLoss: 0.023835\n",
      "Train Epoch: 10 [36480/60000]\tLoss: 0.066935\n",
      "Train Epoch: 10 [37120/60000]\tLoss: 0.040211\n",
      "Train Epoch: 10 [37760/60000]\tLoss: 0.231961\n",
      "Train Epoch: 10 [38400/60000]\tLoss: 0.235291\n",
      "Train Epoch: 10 [39040/60000]\tLoss: 0.033862\n",
      "Train Epoch: 10 [39680/60000]\tLoss: 0.076729\n",
      "Train Epoch: 10 [40320/60000]\tLoss: 0.162199\n",
      "Train Epoch: 10 [40960/60000]\tLoss: 0.080202\n",
      "Train Epoch: 10 [41600/60000]\tLoss: 0.027484\n",
      "Train Epoch: 10 [42240/60000]\tLoss: 0.208751\n",
      "Train Epoch: 10 [42880/60000]\tLoss: 0.073316\n",
      "Train Epoch: 10 [43520/60000]\tLoss: 0.034705\n",
      "Train Epoch: 10 [44160/60000]\tLoss: 0.033302\n",
      "Train Epoch: 10 [44800/60000]\tLoss: 0.127296\n",
      "Train Epoch: 10 [45440/60000]\tLoss: 0.168396\n",
      "Train Epoch: 10 [46080/60000]\tLoss: 0.065902\n",
      "Train Epoch: 10 [46720/60000]\tLoss: 0.022480\n",
      "Train Epoch: 10 [47360/60000]\tLoss: 0.108414\n",
      "Train Epoch: 10 [48000/60000]\tLoss: 0.039118\n",
      "Train Epoch: 10 [48640/60000]\tLoss: 0.063006\n",
      "Train Epoch: 10 [49280/60000]\tLoss: 0.044524\n",
      "Train Epoch: 10 [49920/60000]\tLoss: 0.047692\n",
      "Train Epoch: 10 [50560/60000]\tLoss: 0.106333\n",
      "Train Epoch: 10 [51200/60000]\tLoss: 0.019538\n",
      "Train Epoch: 10 [51840/60000]\tLoss: 0.045855\n",
      "Train Epoch: 10 [52480/60000]\tLoss: 0.021132\n",
      "Train Epoch: 10 [53120/60000]\tLoss: 0.019957\n",
      "Train Epoch: 10 [53760/60000]\tLoss: 0.057659\n",
      "Train Epoch: 10 [54400/60000]\tLoss: 0.069968\n",
      "Train Epoch: 10 [55040/60000]\tLoss: 0.041251\n",
      "Train Epoch: 10 [55680/60000]\tLoss: 0.050519\n",
      "Train Epoch: 10 [56320/60000]\tLoss: 0.086399\n",
      "Train Epoch: 10 [56960/60000]\tLoss: 0.092108\n",
      "Train Epoch: 10 [57600/60000]\tLoss: 0.058727\n",
      "Train Epoch: 10 [58240/60000]\tLoss: 0.124575\n",
      "Train Epoch: 10 [58880/60000]\tLoss: 0.041682\n",
      "Train Epoch: 10 [59520/60000]\tLoss: 0.154527\n",
      "\n",
      "Test set: Avg. loss: 0.0395, Accuracy: 9877/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test Model 2\n",
    "\n",
    "# Create network\n",
    "model2 = Net2()\n",
    "# Initialize model weights\n",
    "model2.apply(weights_init)\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Get initial performance\n",
    "test(model2)\n",
    "# Train for ten epochs\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch, model2)\n",
    "accuracy2 = test(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFbCnAUmTwyx"
   },
   "source": [
    "## III. Results\n",
    "\n",
    "Here we train the CNN model and apply it to the test set. There are 10 epochs in training. There is no validation set here, we simply take the model at the end of the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "id": "JgAKHjLbqm3S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Accuracy: 92.7%\n",
      "Model 2 Accuracy: 98.77%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model 1 Accuracy: {round(float(accuracy1.numpy()),2)}%\")\n",
    "print(f\"Model 2 Accuracy: {round(float(accuracy2.numpy()),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "id": "8hG1l1rSulbg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/03rd9m156sx7chkdv0lzxt240000gn/T/ipykernel_12892/1265220663.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAC8CAYAAAA91AnUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwuUlEQVR4nO3deZyNdf/H8dcwYwYzyAzGkG1ajGx3SXO7Y+wiS9Ld8lMNolEU7lDajLgpqUik5SeRlJD7F1m6GW5lrX66o+5I1uxjG2Ubvr8/5vc9pzMLs51zrpnzfj4eHuNc51q+1/WZc+Z7fa7vEmSMMYiIiIiII5TwdwFERERExE2VMxEREREHUeVMRERExEFUORMRERFxEFXORERERBxElTMRERERB1HlTERERMRBVDkTERERcRBVzkREREQcJE+VsxkzZhAUFOT6FxwcTPXq1enduze//vqrt8rooVatWvTq1cv1etWqVQQFBbFq1ao87Wft2rUkJydz4sSJLO+1bNmSli1bFqichWnz5s3cfvvt1KhRg9KlS1OxYkX+/Oc/88EHH3jtmIq1f6SlpTF8+HDat29PpUqVCAoKIjk52avHVKz9Y+XKlfTp04e6detStmxZqlWrRrdu3fjmm2+8cjzF2T/0mc6gWOdNvjJn7733HuvWreOLL76gX79+zJkzh+bNm/Pbb7/lqxAFceONN7Ju3TpuvPHGPG23du1aRo0alW3Ap06dytSpUwuphAV34sQJrr76asaOHcvnn3/OzJkzqVWrFg888ABjxozx6rEVa99KTU3l7bff5ty5c9xxxx0+PbZi7Vtvvvkmu3btYtCgQXz++edMmjSJw4cPEx8fz8qVK712XMXZt/SZzqBY501wfjaqX78+TZo0AaBVq1ZcvHiR0aNHs3DhQnr27JntNr///jtlypTJf0lzUK5cOeLj4wt1n/Xq1SvU/RVUdncHnTt3ZufOnbz99ts8++yzXju2Yu1bNWvW5Pjx4wQFBXH06FHeffddnx1bsfatKVOmULlyZY9lt912G9dccw1jx46ldevWXjmu4uxb+kxnUKzzplDanNkLvnv3bgB69epFeHg433//Pe3btyciIoI2bdoAcP78ecaMGUPdunUJDQ2lUqVK9O7dmyNHjnjs88KFCwwfPpzo6GjKlCnDrbfeysaNG7McO6dU6YYNG+jSpQuRkZGEhYURGxvL4MGDAUhOTmbYsGEA1K5d25X6tfvIrjJ07NgxHn30UapVq0apUqWoU6cOzzzzDOfOnfNYLygoiIEDBzJr1izi4uIoU6YMjRo1YtGiRXm+rlcSFRVFcHC+6tf5pli7eSPWtnxOoFi7eSPWmStmAOHh4dSrV4+9e/fme795pTi76TOtWDsl1oXyl/3nn38GoFKlSq5l58+fp2vXriQlJfHUU0+Rnp7OpUuX6NatG2vWrGH48OE0a9aM3bt3M3LkSFq2bMnXX39N6dKlAejXrx8zZ85k6NChtGvXji1btnDnnXeSlpZ2xfIsW7aMLl26EBcXx6uvvkqNGjXYtWsXy5cvB6Bv374cO3aMyZMns2DBAqpWrQrkXAs/e/YsrVq1YseOHYwaNYqGDRuyZs0axo0bx+bNm1m8eLHH+osXL2bTpk288MILhIeHM378eLp3785PP/1EnTp1XOsFBQWRkJCQ62fwly5d4tKlSxw/fpxPPvmEZcuW8cYbb+Rq28KiWPsm1k6gWPs+1idPnuTbb7/1WtYsO4qzPtOKtQNjbfLgvffeM4BZv369uXDhgklLSzOLFi0ylSpVMhEREebgwYPGGGMSExMNYKZPn+6x/Zw5cwxg5s+f77F806ZNBjBTp041xhjz448/GsAMGTLEY73Zs2cbwCQmJrqWpaSkGMCkpKS4lsXGxprY2Fhz5syZHM/l5ZdfNoDZuXNnlvcSEhJMQkKC6/W0adMMYObOneux3ksvvWQAs3z5ctcywFSpUsWcOnXKtezgwYOmRIkSZty4cR7blyxZ0rRu3TrHMmaWlJRkAAOYUqVKua6XNyjW/o21McYcOXLEAGbkyJF52i6vFGv/x9rq2bOnCQ4ONl9//XW+tr8cxdn/cdZnWrHOrXw91oyPjyckJISIiAg6d+5MdHQ0S5YsoUqVKh7r9ejRw+P1okWLqFChAl26dCE9Pd31r3HjxkRHR7tqpSkpKQBZnonffffdV3yMt23bNnbs2MFDDz1EWFhYfk4vi5UrV1K2bFnuuusuj+W2J8qKFSs8lrdq1YqIiAjX6ypVqlC5cmVXKtlKT0/Psu3lPP3002zatInFixfTp08fBg4cyIQJE/J4NnmjWGfwdaz9QbHO4K9YP/fcc8yePZvXXnuNm266Kc/b55binEGfaTfFOoOTYp2vx5ozZ84kLi6O4OBgqlSp4ko1/lGZMmUoV66cx7JDhw5x4sQJSpUqle1+jx49CmT0eACIjo72LGxwMJGRkZctm30eXr169dydTC6kpqYSHR2d5Vly5cqVCQ4OdpXXyq6MoaGhnDlzpkDlqFGjBjVq1ACgU6dOAIwYMYLExESPNHVhUqwz+DrW/qBYZ/BHrEeNGsWYMWP4+9//zsCBAwu8v8tRnDPoM51BsfbklFjnq3IWFxfn6gGSk+waxUVFRREZGcnSpUuz3cbWYO0FO3jwINWqVXO9n56enuXiZmYrKfv27bvsenkRGRnJhg0bMMZ4nNfhw4dJT08nKiqq0I6VF02bNmXatGn88ssvXqucKdYZ/B1rX1CsM/g61qNGjSI5OZnk5GSefvpprx9Pcc6gz3QGxdqZfDpDQOfOnUlNTeXixYs0adIky7/rr78ewNX7Yvbs2R7bz507l/T09Mse47rrriM2Npbp06dn6Z3xR6GhoQC5qiG3adOG06dPs3DhQo/lM2fOdL3vDykpKZQoUcKj4aJTKNaBQ7HOv9GjR5OcnMyzzz7LyJEjvX68glCcA4di7X8+HYfh3nvvZfbs2XTq1IlBgwbRtGlTQkJC2LdvHykpKXTr1o3u3bsTFxfH/fffz8SJEwkJCaFt27Zs2bKFCRMmZEm/ZmfKlCl06dKF+Ph4hgwZQo0aNdizZw/Lli1z/RI1aNAAgEmTJpGYmEhISAjXX3+9x/Nn68EHH2TKlCkkJiaya9cuGjRowJdffsnYsWPp1KkTbdu2zdf1CA4OJiEh4YrPsh9++GHKlStH06ZNqVKlCkePHuWTTz7h448/ZtiwYV7LmhWEYu0pt7EGWLJkCb/99purt9MPP/zAvHnzgIzH2d4Yg6ggFGtPuY31K6+8wvPPP89tt93G7bffzvr16z3eL+wxoQpKcfakz7RinZ1Ci3Veeg/YHiCbNm267HqJiYmmbNmy2b534cIFM2HCBNOoUSMTFhZmwsPDTd26dU1SUpLZvn27a71z586ZJ554wlSuXNmEhYWZ+Ph4s27dOlOzZs0r9gAxxph169aZjh07mvLly5vQ0FATGxubpUfJiBEjTExMjClRooTHPjL3ADHGmNTUVNO/f39TtWpVExwcbGrWrGlGjBhhzp4967EeYAYMGJDlvDOX266b+TjZmT59umnevLmJiooywcHBpkKFCiYhIcHMmjXritvml2Ltn1jb7fn/XrmZ/2XXY6mgFGv/xDohISHHOOfxqzlXFGd9pjNTrJ0b66D/P7CIiIiIOIBP25yJiIiIyOWpciYiIiLiIKqciYiIiDiIKmciIiIiDqLKmYiIiIiD+HScs8u5dOkS+/fvJyIiItsRi4syYwxpaWnExMRQooTqw4p1YFCcA4diHTgUa99wTOVs//79XH311f4uhlft3bu3UOcRK6oU68CgOAcOxTpwKNa+4ZjbgOxG+y1uAuEccyMQrkMgnOOVBMI1CIRzzI1AuA6BcI65EQjXwQnn6JjKWXFLj2YnEM4xNwLhOgTCOV5JIFyDQDjH3AiE6xAI55gbgXAdnHCOjqmciYiIiIgqZyIiIiKOosqZiIiIiIM4premiEhO7rvvPgBq1aqV622+//57ABYtWuSNIomIeI0yZyIiIiIOEpCZs/r167NkyRIAKlSoAMDw4cMBmDVrFgCnT5/2S9nEd3bs2OHxs0ePHgCkpaX5rUziqWXLlgDMmDEDgJIlS+a4ru1hZYwB4MKFCwCcO3cOgKlTp3r83LdvX6GXV3zLxnrUqFEAJCcn+7E0IoVHmTMRERERBwnIzFnz5s2pWrWqx7LJkycD7vYpypwVXw8//DAAtWvXBtztmGzmzGZpxP/WrVsHwJgxYwB3zDK78cYbadCggceykJAQj582O96hQwcAunfvDsCePXsKudQikhfTpk0DICkpCYD//Oc/AGzfvp1rrrkGgHfeeQeAVq1aAbj+ht92220ApKam+q7APqDMmYiIiIiDBETmzLZTCQ8PB6BXr15Z1jlz5gyQMamrFG933nmnv4sguWTbi40ePfqy61WqVImKFSsC8PzzzwO47rhvuukmj3UbNWoEwIIFCwD374MyaEWH2pYVL5n//l577bWAZ+/sYcOGAVClShWPbRcuXAhA165dATh+/Lg3i+ozypyJiIiIOEhAZM6GDBkCwIsvvghk9OqyvXysjRs3AvDnP/8ZgHnz5vmwhOILV111FZDRPumP7J3WihUrfF4mKRxHjhzhyJEjAPTs2ROAqKgoAG6//XYAXnvtNQDKlSsHQOPGjQH49NNPAejWrRugXpxFQUJCgr+LIIXI/m227Ub/+c9/Ap5/h9u1awfAG2+84bFts2bNAHcGrbj8bihzJiIiIuIgxTpzZkcIr1OnzhXXtbXtpk2bAvDyyy97vG9HKF+/fn1hFlF8qHXr1oA7o2LNmTMHgL179/q8TOI9R48eBeD9998Hso53ZjNotg2avfNu0qSJL4spedSyZUvX+HeW2qAVbYcOHQKgbdu2Oa7TpUsXIOt4htZXX33lpdL5hzJnIiIiIg5SLDJn1apVA2DgwIEA3HDDDYC7x0dwcO5Ps3Tp0gBcffXVHssfffRRQJmzomzo0KGA+47L3oHt2rXLX0USH/roo48A9x34Pffc4/F+bjLs4n+Zs2ZSvMXGxgLw3HPPAVkzZp999hngHguxuFDmTERERMRBimTmLCwsDHCPZ9S/f3/A3YbkStavX88//vGPbN+zo4jbOTel6KtcuTIA1atXz/b9H374wZfFET+bNGkSkDVzJkWPnVNTii87vln58uU9lts2pE8//TQAv//+u28L5mXKnImIiIg4SJHKnMXHxwOwZMkSACIiIi67/ieffALA7t27ARg7diwA58+f5+zZswD85S9/AaBfv35Aztm3r7/+uiBFFz+y417FxMR4LLe/HxrfLLDYNmdSNI0cOdL1f/XSLN6SkpLo27dvtu/Zv+HF9cmHMmciIiIiDlKkMmfnz58H3HPgVapUCXCPkfLmm28CsHXrVgA2bNgAQHp6usd+IiMjXfPuPfXUUwB07NjRYx27z7vvvttjX1L02HaEtnem9c477wDu3ysJDLfccou/iyAiuXDw4MEc3xswYIAPS+J7ypyJiIiIOEiRypx9++23gHsUfzs33pWyWnYcNNur44YbbuDWW2+97Da2J0hxG3U40ERERLjGucs8Po6dv02c5+GHHwbg2WefBdyf4cspUSLjXtO2Q7KzfnzxxRce69WuXRvImkm124uIf9memePHj8/yOf33v/8NkOOIC8VFkaqcWbbilNtHjZGRkYB7yI0SJUpw6dKly25jOxvYCuDmzZvzUVLxtwULFlCxYkWPZePHj/dTaSQnDRs2BGDRokUAREdHA+4KU+aKdXbsZ9oOVmm1aNEi2/Uz73Pp0qV5KLE4gR2QdtWqVX4thxSu1157DcDV/OiPdu7cCRS/oTMy062iiIiIiIMUycxZXv32228ArFmzxvXz448/znZdO0xH1apVAffUEHFxcQCcPn3aq2WVwtWmTZssGZKVK1f6qTSSk8cffxzIOtyJZR9NHjhwwGN5SEgIAPfdd1+By3DnnXcC7g5HNWrUKPA+xbuUMSue6tatm+N7n376qQ9L4j/KnImIiIg4SEBkznbs2AHkbsLczMNu2AyaGgsXLU8++aTr/7ZBqe2WnbmBuPjPxIkTAXjwwQc9ltvOGi+//DKQMeUauLPgVsmSJQHPSY9nzJgBwM0335ynstg2axs3bszTdiJSOOzwWLt27QIyBp63Tz4uXLgAwKxZs/xSNl9TjUNERETEQQIic1YYOnToALin/BFnsr1sk5KSXMvsndfbb7/tlzJJzh577DEga8/JQYMGAfDTTz9ddvuLFy+61mvbti0AFSpUyNWxbZd8O3j1N998A7iH7BER33rjjTcAuOuuuwDP74V7773XL2XyF2XORERERBxEmbNcUjuUoqFJkyYA1KxZ07XMtlWYO3euX8okha9UqVKAe0Dq559/njZt2gA5j4l26tQpwN2ezU7/cuTIEa+WVURyxw4++0d2XLPVq1f7ujh+pcyZiIiIiIMEfObM9sJ86KGHAIiKisp2vePHj/usTJJ/w4YNy7Js8uTJAPzwww++Lo5cgc1aZf7cvf7660DO4wqGhYUB7rag2Tl58iTgzpRNmjQJgLVr1xagxOIkdqou+1OKJjsTQObZPIKCgjhx4gQQeH+DlTkTERERcZCAzZzVqlULcPcWs73DpGi67bbbPH5aQUFBTJgwwR9FklywPSyXL18OQOXKlQFc7cfyY//+/YB78nTNmSnibA888AAAoaGhHsuNMbmaV7c4UuZMRERExEGKZObM3m3nt/ddiRIlXKPGly1bNtt17Ejkti1aWlpavo4lvtG6dWsga0+9bdu2aT5UB9uyZQsA7du3B2Dx4sXAlWfmsKP5/zG2zz33HADvvPMOAOfPn/dCicVJNLdm8VCuXDnAPZuL/R4PCgri119/9Vu5/EmZMxEREREHKVKZM9ujw44ibGvbeRUUFJQlw3L48GEADh06BLjn/Js3b16+jiHOMGXKlCzzMYrz2AyaHZ+uX79+AFSsWDHb9W3GbMqUKT4onTiVMmfFQ6NGjYCsTz5++eUXevTo4Y8i+Z0yZyIiIiIOUqQyZz///DPgHtnbZtKsRx55BID69etfdj//+te/+OijjzyWbd68GYANGzYURlHFx+bPnw9Anz59ALjqqqsAZT6LKttuTCSzVatW0bJlS0DjnBUXdevWBbK2OVuzZo1r/txAo8yZiIiIiIMUqcyZtWLFCo+f1ltvveWP4ogD2IxnTjM8iEjx0KpVK38XQQqZzZRlbnMWqGOcgTJnIiIiIo5SJDNnIiIiUjxUq1bN30VwHGXORERERBxElTMRERERB1HlTERERMRBVDkTERERcRDHVM4CoctsIJxjbgTCdQiEc7ySQLgGgXCOuREI1yEQzjE3AuE6OOEcHVM5S0tL83cRvC4QzjE3AuE6BMI5XkkgXINAOMfcCITrEAjnmBuBcB2ccI5BxglVRODSpUvs37+fiIgI1xQOxYUxhrS0NGJiYihRwjH1Yb9RrAOD4hw4FOvAoVj7hmMqZyIiIiLioMeaIiIiIqLKmYiIiIijqHImIiIi4iCqnImIiIg4iCpnIiIiIg6iypmIiIiIg6hyJiIiIuIgqpyJiIiIOIgqZyIiIiIOosqZiIiIiIOociYiIiLiIKqciYiIiDiIKmciIiIiDqLKmYiIiIiDqHImIiIi4iCqnImIiIg4iCpnIiIiIg6Sp8rZjBkzCAoKcv0LDg6mevXq9O7dm19//dVbZfRQq1YtevXq5Xq9atUqgoKCWLVqVZ72s3btWpKTkzlx4kSW91q2bEnLli0LVM7Cdvr0aQYPHkxMTAxhYWE0btyYjz76yGvHU6z9Z+PGjXTo0IGIiAjCw8Np1aoVX331ldeOp1g7w7vvvktQUBDh4eFe2b/i7AzejjMo1v70v//7v9xxxx3ExMRQpkwZ6tatywsvvMDvv/+etx2ZPHjvvfcMYN577z2zbt06s3LlSpOcnGxCQ0NN7dq1zenTp/Oyu3ypWbOmSUxMdL0+efKkWbdunTl58mSe9vPyyy8bwOzcuTPLe1u3bjVbt24tYEkLV7t27UyFChXMtGnTzMqVK03fvn0NYGbPnu2V4ynW/rFx40YTGhpqmjdvbj799FOzYMECEx8fb0JDQ83atWu9ckzF2v/27dtnypcvb2JiYkzZsmW9cgzF2f98EWdjFGt/2bp1qwkLCzONGjUyH3/8sVmxYoUZOXKkKVmypOnatWue9hWcn5ph/fr1adKkCQCtWrXi4sWLjB49moULF9KzZ89st/n9998pU6ZMfg53WeXKlSM+Pr5Q91mvXr1C3V9Bff7553zxxRd8+OGH3HfffUDGdd+9ezfDhg3jnnvuoWTJkl45tmLtW8899xwVKlRg6dKlrmvYtm1b6tSpw9ChQ72aQVOs/ad///60aNGCihUrMm/ePK8eS3H2H1/GGRRrX/vwww85e/Ys8+fPJzY2FoDWrVtz4MAB3n77bY4fP85VV12Vq30VSpsze8F3794NQK9evQgPD+f777+nffv2RERE0KZNGwDOnz/PmDFjqFu3LqGhoVSqVInevXtz5MgRj31euHCB4cOHEx0dTZkyZbj11lvZuHFjlmPnlCrdsGEDXbp0ITIykrCwMGJjYxk8eDAAycnJDBs2DIDatWu7Ur92H9mlSo8dO8ajjz5KtWrVKFWqFHXq1OGZZ57h3LlzHusFBQUxcOBAZs2aRVxcHGXKlKFRo0YsWrQoz9fV+vTTTwkPD+evf/2rx/LevXuzf/9+NmzYkO9955Vi7eaNWH/11Ve0bNnS48sxIiKCFi1asHbtWg4cOJDvfeeVYu3mjVhbH3zwAatXr2bq1KkF3ld+KM5uxTnOoFj/kTdiHRISAkD58uU9lleoUIESJUpQqlSpXO8rX5mzzH7++WcAKlWq5Fp2/vx5unbtSlJSEk899RTp6elcunSJbt26sWbNGoYPH06zZs3YvXs3I0eOpGXLlnz99deULl0agH79+jFz5kyGDh1Ku3bt2LJlC3feeSdpaWlXLM+yZcvo0qULcXFxvPrqq9SoUYNdu3axfPlyAPr27cuxY8eYPHkyCxYsoGrVqkDOtfCzZ8/SqlUrduzYwahRo2jYsCFr1qxh3LhxbN68mcWLF3usv3jxYjZt2sQLL7xAeHg448ePp3v37vz000/UqVPHtV5QUBAJCQlXfAa/ZcsW4uLiCA72DFfDhg1d7zdr1uyK16UwKNbejfX58+cJDQ3Nstwu+/77713n4G2KtXdjDXD48GEGDx7Miy++SPXq1a+4vjcozoERZ1CsvR3rxMREJk6cyCOPPMJLL71EpUqVWL16NW+99RYDBgygbNmyV7wmLnl5BmqfY69fv95cuHDBpKWlmUWLFplKlSqZiIgIc/DgQWOMMYmJiQYw06dP99h+zpw5BjDz58/3WL5p0yYDmKlTpxpjjPnxxx8NYIYMGeKx3uzZsw3g8Rw7JSXFACYlJcW1LDY21sTGxpozZ87keC6Xe46dkJBgEhISXK+nTZtmADN37lyP9V566SUDmOXLl7uWAaZKlSrm1KlTrmUHDx40JUqUMOPGjfPYvmTJkqZ169Y5ltG69tprTYcOHbIs379/vwHM2LFjr7iPvFKs/RPrxo0bm+uuu85cvHjRtezChQumTp06BjAffvjhFfeRV4q1f2JtjDE9evQwzZo1M5cuXTLGZFxjb7c5U5wzFNc4G6NY+zPWP/74o6lbt64BXP8ef/xxV+xzK1+PNePj4wkJCSEiIoLOnTsTHR3NkiVLqFKlisd6PXr08Hi9aNEiKlSoQJcuXUhPT3f9a9y4MdHR0a5aaUpKCkCWZ+J33313luxRZtu2bWPHjh089NBDhIWF5ef0sli5ciVly5blrrvu8lhue6KsWLHCY3mrVq2IiIhwva5SpQqVK1d2pZKt9PT0LNvmJCgoKF/vFZRincFXsX7sscfYtm0bAwcO5Ndff2Xv3r3079/ftb8SJbw3+o1incFXsZ4/fz6fffYZ77zzjlc/w5kpzhmKe5xBsbZ8Fetdu3a5HtHOmzeP1atXM378eGbMmEHfvn3zdC75eqw5c+ZM12O2KlWqZPuYpUyZMpQrV85j2aFDhzhx4kSOz12PHj0KQGpqKgDR0dGehQ0OJjIy8rJls8/DCzN1nJqaSnR0dJYPVuXKlQkODnaV18qujKGhoZw5cyZfx4+MjMxyDMh4tg5QsWLFfO03NxTrDL6KdZ8+fThy5AhjxozhzTffBODPf/4zQ4cO5aWXXqJatWr52m9uKNYZfBHr06dPM2DAAB577DFiYmJcwwScP38egBMnThASEpK3xyC5pDhnKO5xBsXa8tX391NPPcWpU6fYvHmzK6YtWrQgKiqKPn368OCDD5KQkJCrfeWrchYXF+fqAZKT7O4QoqKiiIyMZOnSpdluY2uw9oIdPHjQ449Renp6tpWUP7LP0vft23fZ9fIiMjKSDRs2YIzxOK/Dhw+Tnp5OVFRUoR0rOw0aNGDOnDmkp6d73I18//33QEaPHG9RrDP4KtYATz75JIMHD2b79u1ERERQs2ZNkpKSKFu2LDfddJPXjqtYZ/BFrI8ePcqhQ4d45ZVXeOWVV7K8f9VVV9GtWzcWLlxY6MdWnDMU9ziDYm356vt78+bN1KtXL0tl++abbwYy2ofntnLm0xkCOnfuTGpqKhcvXqRJkyZZ/l1//fUArt4Xs2fP9th+7ty5pKenX/YY1113HbGxsUyfPj1L74w/sg2sc1NDbtOmDadPn87yAZo5c6brfW/q3r07p0+fZv78+R7L33//fWJiYrjlllu8evz8UKwLJjQ0lPr161OzZk327NnDxx9/TL9+/VyNcJ1Esc676OhoUlJSsvzr0KEDYWFhpKSkMGbMGK8dPz8U57wrinEGxTq/YmJi2Lp1K6dPn/ZYvm7dOiBvWcJC6a2ZW/feey+zZ8+mU6dODBo0iKZNmxISEsK+fftISUmhW7dudO/enbi4OO6//34mTpxISEgIbdu2ZcuWLUyYMCFL+jU7U6ZMoUuXLsTHxzNkyBBq1KjBnj17WLZsmeuXqEGDBgBMmjSJxMREQkJCuP766z2eP1sPPvggU6ZMITExkV27dtGgQQO+/PJLxo4dS6dOnWjbtm2+rkdwcDAJCQlXfJbdsWNH2rVrxyOPPMKpU6e45pprmDNnDkuXLuWDDz7w2hhnBaFYe8ptrLds2cL8+fNp0qQJoaGhfPfdd7z44otce+21jB49Ol/H9jbF2lNuYh0WFpbtyOYzZsygZMmSjhv1HBTnzIprnEGxziy339+DBw/mjjvuoF27dgwZMoSoqCjWr1/PuHHjqFevHh07dsz9QfPSe8D2ANm0adNl17tcT5QLFy6YCRMmmEaNGpmwsDATHh5u6tata5KSksz27dtd6507d8488cQTpnLlyiYsLMzEx8ebdevWZRl1OLseIMYYs27dOtOxY0dTvnx5ExoaamJjY7P0KBkxYoSJiYkxJUqU8NhH5h4gxhiTmppq+vfvb6pWrWqCg4NNzZo1zYgRI8zZs2c91gPMgAEDspx35nLbdTMfJydpaWnm8ccfN9HR0aZUqVKmYcOGZs6cObnaNj8Ua//E+qeffjItWrQwFStWNKVKlTLXXHONefbZZ706ordi7b/PdWa+6K2pOBfvOBujWPsz1itXrjTt27c30dHRpnTp0ua6664zTzzxhDl69GiutreC/v/AIiIiIuIAPm1zJiIiIiKXp8qZiIiIiIOociYiIiLiIKqciYiIiDiIKmciIiIiDuLTcc4u59KlS+zfv5+IiAifzz/mbcYY0tLSiImJ8erciEWFYh0YFOfAoVgHDsXaNxxTOdu/fz9XX321v4vhVXv37i3UecSKKsU6MCjOgUOxDhyKtW845jYgu9F+i5tAOMfcCITrEAjneCWBcA0C4RxzIxCuQyCcY24EwnVwwjk6pnJW3NKj2QmEc8yNQLgOgXCOVxII1yAQzjE3AuE6BMI55kYgXAcnnKNjKmciIiIiosqZiIiIiKOociYiIiLiII7prelrI0eOBCA5ORmAQ4cOAdCtWzcAvv32WwAuXLjg+8KJiIhIwFLmTERERMRBAjZzlpSUBGQMqAcQFRUFwFdffQXAM888A8BLL73kh9KJiIgUb3bIiqpVq2b7fvPmzXn33Xc9lk2cOBGA559/HoC0tDTvFdCPlDkTERERcZCAzJy9+eabnDx5EoBHHnkEgP79+wNw0003ATBw4EAAli1bBsDmzZt9XEopiEqVKgHQu3dvevToAbjvzubNmwe42x0W1zsvyZvJkycDGaODA4wfP96fxREptmzG7MMPPwSgU6dOOa5rn25Zjz/+OOB+2vXAAw94o4h+p8yZiIiIiIMEVOasb9++ANx///1MnToVgH/84x8eP1999VXAXTtfunQpANHR0T4tq+RPu3btABg7diwADRs2ZOXKlQBs3boVcMc2Li4OgI4dO/q6mFJATZo0AeDrr78u8L7uv/9+wP39EBoaCihz5jRTpkwB4NFHH+Wxxx4D4I033sh23fLlywNw4MABAL777jsAWrZsCcC5c+e8WVS5gjvuuAO4fMbsSmrVqgXgmqA8c4atqFPmTERERMRBAiJzlpiYCMDrr78OQKlSpXK84x4xYgQAFSpUAODBBx8ECvdOXQqfbX8wffp0wH0XlZSUxIwZMzzWXbVqFZDR9lCKlkGDBgHw3HPPAXDXXXcB7pjmRWRkJODOpNqMmTiLzYK1aNECyPhs9+zZE8g5c/b+++8D7pg2bdoUgGuuuQZwZ9HFP5o1a5bt8v379wPubFhqairLly8HoFGjRoA7+2n3Yb8TXnvtNa+V1x+UORMRERFxkGKdObPtxIYOHQq476K+/fZbPv/882y3sW0RXnnlFQBXT7/FixcD7ufcZ86c8U6hJV/s3dX58+cBaNOmDQC7du3Ksu6iRYsAGDVqFABffPEF4M7C2J684j+2N5fNllx77bVARhtCgIoVKwKQnp6e72PUr18fcGfFrT179uR7n1L4bOzr1avnWpZT+6Jq1aoB7l73ljHG46f4h82I1a5d22P5xYsXAfeTqu3btwOwb9++LPs4duwY4M6o2nZrtrd1Qb4TnESZMxEREREHKdaZs7/97W+Au1eevWuaMGECv/3222W3tW0S7JhYvXr1AtzjoRW359tFTcmSJQF49tlnAXd7hN69ewPZZ8wsO67ZiRMnAHeWzbZlsD13xX9uuOEGAD777LNs39+5cyfgvsPOD5t9s06dOgVA+/bt871PKTw2y2LbAf9RTu1F+/TpA0BMTIzH8p9//hlwz6Es/hEcnFHlsL3qLfvEKiUl5Yr7eO+99wAYPHgwAK1btwYy2pKDMmciIiIi4gXFOnNm24dllpeeOqNHjwbcmTM7qvzq1auBjPZr4nu33HIL4I7HsGHDAFiwYMEVt73tttsAd7bNZtk++OADj33/8MMPhVdgyZWQkBAA/vrXv3osP3v2LAAXLlwA3Nlw28YwP5588kmP13PmzAFg27Zt+d6nFB7bI9OOiXU5V111FQADBgzwWG5/b2z2LTU1tRBLKHll25bZv58JCQmAO+t16623AvDll1/muI+cnnrZWWF2795dOIX1M2XORERERBykWGbO7LhmtqelbWv23//93wDs2LEj1/vK3HYpPDwccD/vtr1LxLds1uObb74BYNKkSUDu2hu8++67Hq+feeYZwP17Y9tFiO/ZGToyZ0CGDBkCwFtvvVXgY9j2bLZnn7Vhw4YC71sKzn7+unbtmu37qampWTLkZcqUAdzZE2vChAkAfPrpp4VdTMkHmzmzox/YzJmNue2Ze7nMmV0nM9tezT7l+te//lXwAvuRMmciIiIiDlIsUwR29HDryJEjALz88stA/sYoe+ihhwB31iXzHZr4RuPGjQF3b58HHngAyFsPHdvT07ZHsWPe2TZH4ns2a2kzZgcPHgRg2rRpQNZsZ0HYLIrNnP3nP/8BYNasWYV2DMk/2xP+zjvvzPb9u+66i99//91j2dy5c7NdN/N64gw2c2bnQLaZs4kTJwLuGXr+OLvL3XffDUC3bt2y3WfNmjUBuP322wH46quvAHe2rqhR5kxERETEQYpV5qx06dKAe3RxO06OHePG/syPTz75BHDf2duxkOyI5f/+97/zvW/JPdtLz45RNn/+/Hzvy7Yn1IwA/mNHfR8zZozHctvj6oUXXii0Y9l5Fe3I4pbtkV1U77CLmz/OBPBHe/fuBWDz5s2uZbZHvv0ezkw9b53JZquTk5MB9+fftum2bQXtz+ycPn0acD8Zs7MO2BmBDhw4ALizcUVNsaqc2QFi7YTGtpv9iy++WOB92+67dniFP/3pTwB07NgRUOXMV1q1apXnbWylffbs2YB78FE9xvS/zp07A+5HjEePHgXcX9qFqW3btoC7SYIdBsc+YhH/skPb/OUvf/FYbh9xd+jQAXAPFgzQvHlzwN0hwLIVODtVmxQf3333HeD+jli3bh3gvqGz0zRed911vi9cIdJjTREREREHKRaZMzvJrR2Q1LLd7r15Z2wfs4lz/dd//ReQdTBL3VU7z48//gi4Mx/2EZedcstmxa3o6GgAnnjiCdfn3DYIzizz0Bn2scgfMzHie7b5yfPPPw+4ByK2bCbknnvuAWDZsmWux5h2QOnMbNMWO3ySnUD77bffBi4/vZt4X926dQEYNWqUx3LbscvG6Y/TbdlOH3v27AHcHbqszM0S7BOToKAgoOhNeq/MmYiIiIiDFIvMmW13YBsTWrahrxQf9g7YTtdiu1XnNFl5QkJClkaltmHxxo0bvVVMyaXMWS77WV6yZAngbsRv76BjY2M91rcZtj/96U+uNkt2EmUrc8ZMnMUOo5DTNE32s26fjGR+QpId+3tis6L2My/OYGNuhzWy7He1HRg8L5YtWwZA9+7dAfcA8Q8//DDgnvqtqFDmTERERMRBinTmrHLlygAkJSV5LLcDxtoutoV5LDswou0Zouycb/3P//wPAE8//TQAH3/8MQBvvvkmAD/99BPg/p2oWbOmaxqPLl26AO676ePHj/uo1JITG0+bMbPsYMOWzYpnnujcTuMzY8YMFi5cCLizJPfeey/g7qVrXbp0Ccg52yrOYtsh2c/rd9995/p9se3RLDuNmx2CxbZLytw+SfzLth/MrCDDXW3fvj3f2zqRMmciIiIiDlKkM2d16tQBso5nsnTp0gLv2/b0sANW2jt8O56ObeuSl0nUpeDsdW/RogXgnr7JZkttGwab2Xz44YddmRSbObMDIIr/vfLKKwCUKlUKgL/97W+Au1fmvHnzAPc4gpkHq81O9erVgawZM+uxxx4D3NlW8S+bGbvvvvuyfd8OOL18+XLXspkzZwLQs2dPwN2e6LPPPvPYRpwpKirK47WN36ZNm/K9TzvOqbVhwwbAnSkvapQ5ExEREXGQIp05s1N3ZB6/xNbK/zhGSl7ZWridLN2OoWInap06dWq+9y0FZ3ta2p82G5KdwYMHe7xev36918ol+TNu3DgAXn/9dcA99tWZM2eAvE1sn5OtW7cC7qnYxBlsZiOnycszCw0NzdJr12bKUlJSCrVs4hv2816lShUAtmzZkuttY2JigKztD+1TlqI6LZsyZyIiIiIOUqQzZzZrcuzYMcA9Z6JtH2bn4Jo2bRoA33zzDQDXXnutaxubfbPz7dk2TPHx8YC77VKfPn0AWL16tZfORrzNjhQtzmXnsC2IJ5980uO1bWM4efJkwD1/pxRNZcqUcX0/S9Fk5zW2Y5DZtsL276/Nctt5VbNTtWpVwD0DUObMmd1HUaXMmYiIiIiDFOnM2S+//ALAoEGDAJg1axaQMbYVuLNiXbt2BWDnzp1Axnx89g7dZsxsu7UDBw4A7jYwtleQXS5Fl42xN+daFf+5/vrrgay9/myPTztfnxQ/tne9nVGiIONliffZjNjhw4cB9ziiNnN28803A5cfecHOq2rn6bRsj+6i3i5cmTMRERERBynSmTPLjhK+YsUKAB599FEAOnbsCMBNN90EQO3atV3b2DutzPMu2mfhtqeHFD+FOXOEOIf9fNu2p7aHZ07jnUnRdOrUKRYtWgRA586dAXevXmXMioZt27YBGXPigvtphp0ZxGbDMmfFLsfO1mPbmGcexaGoUeZMRERExEGKRebs3LlzgPv5dXJyssdPkT+yY9VlnpNVijbb3sh+H9jeuXa+VSkeLl68yMmTJ/1dDCkEtu1Zu3btABg4cCAAI0eOzHEbO1uIHc/073//O+BuU1pUZwTITJkzEREREQcpFpkzkbxo1qyZv4sgXmBHFe/bty8A3bt392dxxIvs+JOWemsWbXbc0RdeeMHjZyBT5kxERETEQYKMQ7o0nDp1ivLly/u7GF518uRJypUr5+9i+J1iHRgU58Dh61jfcsstAKxduxaAf/7znwB06NDBa8dUrDPoc+0bypyJiIiIOIjanImISJGyYcMGwD0no0hxo8yZiIiIiIOociYiIiLiII6pnDmkX4JXBcI55kYgXIdAOMcrCYRrEAjnmBuBcB0C4RxzIxCugxPO0TGVs7S0NH8XwesC4RxzIxCuQyCc45UEwjUIhHPMjUC4DoFwjrkRCNfBCefomKE0Ll26xP79+4mIiHBNu1JcGGNIS0sjJiaGEiUcUx/2G8U6MCjOgUOxDhyKtW84pnImIiIiIg56rCkiIiIiqpyJiIiIOIoqZyIiIiIOosqZiIiIiIOociYiIiLiIKqciYiIiDiIKmciIiIiDvJ/IywgeGY9ug4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run network on data we got before and show predictions\n",
    "output = model(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "id": "zUHLA7qru5cQ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run network on data we got before and show predictions\n",
    "output = model2(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0w7iym1T2QY"
   },
   "source": [
    "# IV. Conclusion and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change from `Net` to `Net2` brought a noticeable accuracy boost from 92.7% to 98.77%, showing how modifying a neural network's structure can make a difference. Adding more convolutional layers with smaller kernels, `Net2` can capture more complex and detailed features from the input data, which helps it learn better. Adding batch normalization after each convolution layer also helps keep the inputs to each layer more stable, making training faster and more reliable.\n",
    "\n",
    "Another useful addition was the dropout layer, which randomly turns off some neurons during training to prevent overfitting. This helps make the model generalize better when it sees new data. The global average pooling layer reduces the size of the feature maps before they go into the fully connected layers, which lowers the number of parameters without losing important information.\n",
    "\n",
    "Using Kaiming weight initialization also makes the training process smoother, ensuring signals flow better through the layers. Overall, these improvements make `Net2` more powerful and efficient at recognizing patterns in the data. Adding layers, regularization, and normalization can significantly improve a model's accuracy and reliability."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
