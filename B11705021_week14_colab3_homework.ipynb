{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b7758c2-9824-4b5a-9f0b-6b657cab79f1",
      "metadata": {
        "id": "0b7758c2-9824-4b5a-9f0b-6b657cab79f1"
      },
      "source": [
        "# Week 14: Colab Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2001f926-2262-4c66-8abb-5eb2dd13766d",
      "metadata": {
        "id": "2001f926-2262-4c66-8abb-5eb2dd13766d"
      },
      "source": [
        "# I. Introduction\n",
        "In this exercise, we first train a transformer using the Wikitext-2 dataset and then use the model to generate new text with the length specified by the user.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7aa8f46-ff8b-47be-9bcc-12d5df8ec4d6",
      "metadata": {
        "id": "c7aa8f46-ff8b-47be-9bcc-12d5df8ec4d6"
      },
      "source": [
        "# II. Methods\n",
        "\n",
        "What is the model architecture?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb45f3e-9428-4353-a8cf-0ee4ae37cfa7",
      "metadata": {
        "id": "4fb45f3e-9428-4353-a8cf-0ee4ae37cfa7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0ea711-14fe-4d85-892a-4f6c871bb81b",
      "metadata": {
        "id": "2a0ea711-14fe-4d85-892a-4f6c871bb81b"
      },
      "outputs": [],
      "source": [
        "# Uncomment one of the following that works for you.\n",
        "\n",
        "# device = torch.device(\"cuda\")\n",
        "# device = torch.device(\"mps\")\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024af44e-ee91-4d32-aa01-feb09f9dddcf",
      "metadata": {
        "id": "024af44e-ee91-4d32-aa01-feb09f9dddcf"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "\n",
        "emsize = 200 # size of word embeddings\n",
        "nhead = 2\n",
        "nhid = 200\n",
        "nlayers = 2\n",
        "dropout = 0.2\n",
        "lr = 20 # initial learning rate\n",
        "epochs=10 # upper epoch limit\n",
        "\n",
        "bptt=35 #sequence length\n",
        "clip=0.25 #gradient clipping\n",
        "log_interval=200 # report interval\n",
        "\n",
        "save='model.pt' #path to save the final model\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(0)\n",
        "\n",
        "eval_batch_size = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e435b94-c544-447b-8213-21f4c64fafef",
      "metadata": {
        "id": "5e435b94-c544-447b-8213-21f4c64fafef"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pHGuUFWZ7mpw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHGuUFWZ7mpw",
        "outputId": "1c738e94-50bd-4dc3-ea46-fc64b912b71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/') # Change to your own path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5NqBYyH_M8Ix",
      "metadata": {
        "id": "5NqBYyH_M8Ix"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PgyqB0EROhVu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgyqB0EROhVu",
        "outputId": "0bf25693-877b-44d6-ff95-e2b946da64f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test.txt  train.txt  valid.txt\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/data/wikitext-2'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "382d8c86-7df9-4653-af82-b2515861e362",
      "metadata": {
        "id": "382d8c86-7df9-4653-af82-b2515861e362"
      },
      "outputs": [],
      "source": [
        "path = '/content/data/wikitext-2/'\n",
        "corpus = Corpus(path)\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    nbatch = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "ntokens = len(corpus.dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f968342b-ee5f-41b7-aa97-6b7eaefc3eda",
      "metadata": {
        "id": "f968342b-ee5f-41b7-aa97-6b7eaefc3eda"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a036d973-53ba-4f4b-abfa-188da6008ab3",
      "metadata": {
        "id": "a036d973-53ba-4f4b-abfa-188da6008ab3"
      },
      "outputs": [],
      "source": [
        "# Define positional encoding used in the transformer model\n",
        "\n",
        "#################################################################################################\n",
        "# [TODO]: Build a positional encoding function that can be used in the TransformerModel below\n",
        "#################################################################################################\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create a tensor to hold positional encodings\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Selects every second index, corresponding to even dimensions (e.g., 0, 2, 4, ...).\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices and cosine to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add an extra batch dimension\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input embeddings\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z8g9yegzoZZ0",
      "metadata": {
        "id": "z8g9yegzoZZ0"
      },
      "source": [
        "**Note** <br>\n",
        "**Positional Encoding:** <br>\n",
        "\n",
        "\n",
        "*  pe: a tensor to store positional encodings for all positions and dimensions\n",
        "*  position: a column vector with values \\[0, 1, 2, ..., max_len-1\\]\n",
        "*  div_term: divided terms for different scaling factors (sine and cosine functions)\n",
        "\n",
        "The functions encodes positional information into token embeddings so the model can use sequence order information.\n",
        "\n",
        "**Forward:** <br>\n",
        "*  x + self.pe: Adds the positional encoding to the input tensor:\n",
        "  *  x: input tensor\n",
        "  *  self.pe: slices the positional encodings to match the sequence length (seq_len) of the input\n",
        "*  self.dropout(x): applies dropout for regularization\n",
        "\n",
        "The function defines how the layer processes inputs during forward passes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724cbe76-35db-422e-9de9-21a2ff4f5798",
      "metadata": {
        "id": "724cbe76-35db-422e-9de9-21a2ff4f5798"
      },
      "outputs": [],
      "source": [
        "# Define the transformer model\n",
        "\n",
        "class TransformerModel(nn.Transformer):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout) # This is what you had constructed above\n",
        "\n",
        "        # Embedding layer to convert token indices into dense vectors of size ninp.\n",
        "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
        "\n",
        "        # Stores the dimensionality of the embeddings for scaling.\n",
        "        self.ninp = ninp\n",
        "\n",
        "        # A linear decoder layer that maps the model’s output back to the vocabulary space.\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    # Creates a mask to prevent the model from attending to future tokens during training or inference.\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        # torch.tril(...): Extracts the lower triangular part of the matrix (everything above the diagonal is zeroed out).\n",
        "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, src, has_mask=True):\n",
        "        if has_mask:\n",
        "            # a mask is generated to prevent the model from attending to future tokens\n",
        "            device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        # Converts input token indices into dense vectors.\n",
        "        # Then scales the embeddings by the square root of ninp.\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "\n",
        "        # Adds positional encodings to the embeddings.\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.encoder(src, mask=self.src_mask)\n",
        "\n",
        "        # Projects the encoder’s output back to the vocabulary space using the linear decoder layer.\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RGqX3zQ2xN-6",
      "metadata": {
        "id": "RGqX3zQ2xN-6"
      },
      "source": [
        "**The TransformerModel class**\n",
        "\n",
        "I have commented the detailed description on the above code of what each line does. Below is the summarized steps:\n",
        "\n",
        "1. Takes token indices as input.\n",
        "2. Encodes them into dense vectors.\n",
        "3. Adds positional encodings.\n",
        "4. Processes the inputs using a Transformer encoder.\n",
        "5. Decodes the output back into token probabilities.\n",
        "6. Can optionally apply attention masks for causal language modeling or sequence tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8a12a5-50b2-4f78-9f2a-c98e3a746e30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c8a12a5-50b2-4f78-9f2a-c98e3a746e30",
        "outputId": "032eb476-717d-4936-b59c-72135cb9d17c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "criterion = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7378de-eeac-445c-8f15-768681785fcd",
      "metadata": {
        "id": "6a7378de-eeac-445c-8f15-768681785fcd"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1facb91b-6a0d-42b2-9cec-600fc15f27d6",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1facb91b-6a0d-42b2-9cec-600fc15f27d6",
        "outputId": "5d3285b3-3645-40b6-ac44-74e24d4cc2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 1551.49 | loss 16.67 | ppl 17367311.38\n",
            "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 1011.38 | loss 12.41 | ppl 245237.03\n",
            "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 1013.61 | loss 11.28 | ppl 79151.70\n",
            "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 1008.64 | loss  9.69 | ppl 16190.12\n",
            "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 1008.09 | loss  9.28 | ppl 10761.84\n",
            "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 1009.12 | loss  8.97 | ppl  7892.13\n",
            "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 1007.03 | loss  8.69 | ppl  5968.76\n",
            "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 1012.53 | loss  8.79 | ppl  6554.71\n",
            "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 1010.92 | loss  8.54 | ppl  5103.18\n",
            "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 1010.25 | loss  8.60 | ppl  5421.30\n",
            "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 1011.91 | loss  8.60 | ppl  5425.41\n",
            "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 1012.67 | loss  8.59 | ppl  5384.25\n",
            "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 1015.06 | loss  8.64 | ppl  5663.33\n",
            "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 1018.76 | loss  8.64 | ppl  5657.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 3239.11s | valid loss  8.24 | valid ppl  3777.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 1017.78 | loss  8.60 | ppl  5407.33\n",
            "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 1017.86 | loss  8.49 | ppl  4867.01\n",
            "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 1017.21 | loss  8.53 | ppl  5055.56\n",
            "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 1011.47 | loss  8.57 | ppl  5254.35\n",
            "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 1010.33 | loss  8.66 | ppl  5783.32\n",
            "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 1013.99 | loss  8.64 | ppl  5663.69\n",
            "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 1009.46 | loss  8.55 | ppl  5150.44\n",
            "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 1008.31 | loss  8.47 | ppl  4759.50\n",
            "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 1005.91 | loss  8.54 | ppl  5133.97\n",
            "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 998.93 | loss  8.52 | ppl  5029.24\n",
            "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 1007.67 | loss  8.51 | ppl  4972.75\n",
            "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 1011.24 | loss  8.58 | ppl  5322.88\n",
            "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 1010.05 | loss  8.47 | ppl  4767.53\n",
            "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 1001.78 | loss  8.44 | ppl  4621.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 3127.52s | valid loss  7.96 | valid ppl  2857.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 1010.84 | loss  8.37 | ppl  4322.43\n",
            "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 1004.53 | loss  8.38 | ppl  4359.85\n",
            "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 1007.40 | loss  8.36 | ppl  4291.92\n",
            "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 1012.15 | loss  8.10 | ppl  3299.07\n",
            "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 1045.88 | loss  8.16 | ppl  3488.84\n",
            "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 1045.49 | loss  8.05 | ppl  3142.55\n",
            "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 1047.96 | loss  7.92 | ppl  2757.80\n",
            "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 1085.98 | loss  7.96 | ppl  2870.59\n",
            "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 1045.72 | loss  8.12 | ppl  3355.09\n",
            "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 1051.96 | loss  7.99 | ppl  2950.23\n",
            "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 1099.51 | loss  8.14 | ppl  3414.62\n",
            "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 1054.91 | loss  8.21 | ppl  3686.68\n",
            "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 1053.14 | loss  8.00 | ppl  2976.20\n",
            "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 1089.48 | loss  7.85 | ppl  2556.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 3243.98s | valid loss  7.54 | valid ppl  1877.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 1088.74 | loss  8.15 | ppl  3463.30\n",
            "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 1046.63 | loss  8.02 | ppl  3050.92\n",
            "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 1039.26 | loss  7.91 | ppl  2737.00\n",
            "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 1085.56 | loss  7.88 | ppl  2655.56\n",
            "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 1038.76 | loss  8.18 | ppl  3558.79\n",
            "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 1050.13 | loss  7.83 | ppl  2509.87\n",
            "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 1084.09 | loss  8.15 | ppl  3478.00\n",
            "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 1058.25 | loss  7.97 | ppl  2894.63\n",
            "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 1047.15 | loss  7.84 | ppl  2536.49\n",
            "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 1086.46 | loss  7.72 | ppl  2262.91\n",
            "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 1034.73 | loss  7.70 | ppl  2209.62\n",
            "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 1045.93 | loss  7.67 | ppl  2137.92\n",
            "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 1084.15 | loss  7.70 | ppl  2210.51\n",
            "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 1045.12 | loss  7.64 | ppl  2069.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 3285.69s | valid loss  7.13 | valid ppl  1252.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 1049.96 | loss  7.77 | ppl  2367.45\n",
            "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 1036.12 | loss  7.66 | ppl  2112.63\n",
            "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 1080.72 | loss  7.65 | ppl  2105.96\n",
            "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 1076.67 | loss  7.61 | ppl  2018.28\n",
            "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 1052.16 | loss  7.64 | ppl  2087.35\n",
            "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 1088.71 | loss  7.76 | ppl  2337.62\n",
            "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 1049.80 | loss  7.61 | ppl  2010.01\n",
            "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 1041.31 | loss  7.66 | ppl  2131.56\n",
            "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 1128.97 | loss  7.64 | ppl  2089.15\n",
            "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 1112.81 | loss  7.65 | ppl  2109.28\n",
            "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 1115.78 | loss  7.72 | ppl  2245.89\n",
            "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 1142.80 | loss  7.63 | ppl  2061.78\n",
            "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 1102.60 | loss  7.68 | ppl  2167.96\n",
            "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 1095.95 | loss  7.59 | ppl  1985.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 3365.57s | valid loss  8.91 | valid ppl  7371.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 1107.92 | loss  7.07 | ppl  1176.70\n",
            "| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 1142.60 | loss  7.03 | ppl  1125.46\n",
            "| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 1099.16 | loss  7.00 | ppl  1101.87\n",
            "| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 1108.78 | loss  7.01 | ppl  1107.00\n",
            "| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 1146.57 | loss  7.03 | ppl  1133.68\n",
            "| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 1103.85 | loss  7.04 | ppl  1144.87\n",
            "| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 1144.71 | loss  7.02 | ppl  1115.95\n",
            "| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 1103.10 | loss  7.03 | ppl  1128.94\n",
            "| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 1106.51 | loss  7.00 | ppl  1098.36\n",
            "| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 1145.22 | loss  7.02 | ppl  1116.57\n",
            "| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 1101.94 | loss  7.02 | ppl  1121.75\n",
            "| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 1112.93 | loss  6.98 | ppl  1074.74\n",
            "| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 1149.28 | loss  7.01 | ppl  1105.64\n",
            "| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 1105.48 | loss  6.97 | ppl  1066.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 3468.16s | valid loss  6.99 | valid ppl  1086.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 1106.68 | loss  7.01 | ppl  1107.50\n",
            "| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 1100.96 | loss  6.99 | ppl  1082.63\n",
            "| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 1134.97 | loss  6.97 | ppl  1061.93\n",
            "| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 1106.93 | loss  6.98 | ppl  1070.01\n",
            "| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 1104.78 | loss  7.00 | ppl  1094.93\n",
            "| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 1148.28 | loss  7.01 | ppl  1109.05\n",
            "| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 1099.24 | loss  6.99 | ppl  1080.40\n",
            "| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 1145.27 | loss  7.00 | ppl  1093.04\n",
            "| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 1094.54 | loss  6.97 | ppl  1064.93\n",
            "| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 1104.73 | loss  6.99 | ppl  1084.18\n",
            "| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 1139.71 | loss  6.99 | ppl  1087.28\n",
            "| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 1105.38 | loss  6.95 | ppl  1042.83\n",
            "| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 1097.30 | loss  6.98 | ppl  1074.88\n",
            "| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 1146.62 | loss  6.94 | ppl  1037.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 3449.03s | valid loss  6.98 | valid ppl  1076.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 1140.51 | loss  6.96 | ppl  1054.25\n",
            "| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 1109.90 | loss  6.91 | ppl  1000.27\n",
            "| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 1116.24 | loss  6.87 | ppl   966.80\n",
            "| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 1176.08 | loss  6.88 | ppl   975.71\n",
            "| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 1120.42 | loss  6.90 | ppl   992.78\n",
            "| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 1126.72 | loss  6.92 | ppl  1008.46\n",
            "| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 1163.45 | loss  6.89 | ppl   982.31\n",
            "| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 1121.75 | loss  6.90 | ppl   992.60\n",
            "| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 1169.46 | loss  6.87 | ppl   967.47\n",
            "| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 1130.81 | loss  6.90 | ppl   989.44\n",
            "| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 1137.81 | loss  6.89 | ppl   979.40\n",
            "| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 1183.58 | loss  6.85 | ppl   941.79\n",
            "| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 1140.96 | loss  6.88 | ppl   970.72\n",
            "| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 1132.55 | loss  6.84 | ppl   938.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 3525.70s | valid loss  7.05 | valid ppl  1150.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 1137.58 | loss  6.85 | ppl   947.40\n",
            "| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 1170.13 | loss  6.83 | ppl   923.05\n",
            "| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 1132.59 | loss  6.80 | ppl   899.99\n",
            "| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 1165.00 | loss  6.82 | ppl   913.14\n",
            "| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 1128.25 | loss  6.84 | ppl   931.61\n",
            "| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 1122.57 | loss  6.86 | ppl   950.11\n",
            "| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 1157.26 | loss  6.84 | ppl   930.13\n",
            "| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 1124.41 | loss  6.85 | ppl   940.55\n",
            "| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 1129.09 | loss  6.82 | ppl   914.32\n",
            "| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 1159.53 | loss  6.85 | ppl   939.64\n",
            "| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 1129.79 | loss  6.83 | ppl   926.55\n",
            "| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 1128.20 | loss  6.79 | ppl   892.82\n",
            "| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 1172.85 | loss  6.82 | ppl   919.24\n",
            "| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 1126.87 | loss  6.80 | ppl   894.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 3527.33s | valid loss  6.97 | valid ppl  1061.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 1113.91 | loss  6.84 | ppl   936.65\n",
            "| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 1138.57 | loss  6.81 | ppl   910.95\n",
            "| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 1135.48 | loss  6.79 | ppl   889.16\n",
            "| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 1115.96 | loss  6.81 | ppl   902.46\n",
            "| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 1154.70 | loss  6.82 | ppl   920.56\n",
            "| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 1119.49 | loss  6.85 | ppl   940.20\n",
            "| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 1119.58 | loss  6.82 | ppl   919.85\n",
            "| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 1157.22 | loss  6.84 | ppl   929.91\n",
            "| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 1120.42 | loss  6.81 | ppl   905.38\n",
            "| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 1182.50 | loss  6.84 | ppl   929.85\n",
            "| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 1260.16 | loss  6.82 | ppl   915.88\n",
            "| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 1115.52 | loss  6.78 | ppl   883.62\n",
            "| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 1164.26 | loss  6.81 | ppl   909.89\n",
            "| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 1120.68 | loss  6.79 | ppl   885.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 3535.68s | valid loss  6.97 | valid ppl  1059.44\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-40fb26e34c0e>:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(f)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  6.88 | test ppl   972.67\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        output = model(data)\n",
        "        output = output.view(-1, ntokens)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nbYefzGl0OwQ",
      "metadata": {
        "id": "nbYefzGl0OwQ"
      },
      "source": [
        "**Note** <br>\n",
        "This code defines a training and evaluation pipeline for a Transformer-based language model in PyTorch. It includes functions to handle batches of data, train the model, evaluate its performance, and save/load the best-performing model.\n",
        "\n",
        "1. get_batch: This function retrieves a single batch of data from the dataset.\n",
        "  * seq_len: The sequence length for the batch, limited by a constant bptt (backpropagation through time).\n",
        "2. evaluate: This function evaluates the model on a dataset (validation or test) by calculating the loss.\n",
        "* Purpose: Measures how well the model performs on unseen data without updating weights.\n",
        "* Process:\n",
        "  * For each batch:\n",
        "    * Retrieves data and targets using get_batch.\n",
        "    * Feeds data to the model and reshapes the output to match the vocabulary size (ntokens).\n",
        "    * Calculates the loss for the batch using the loss criterion.\n",
        "    * Accumulates the weighted loss (len(data) * loss).\n",
        "  * Returns the average loss over all batches.\n",
        "3. train: This function trains the model on the training dataset for one epoch.\n",
        "* Purpose: Updates the model's weights to minimize the loss on the training data.\n",
        "* Logs statistics (loss, elapsed time, perplexity) every log_interval batches.\n",
        "4. Training and Validation Loop: This section handles the training process across multiple epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d482a0-034b-4565-8e22-12e93d7171fe",
      "metadata": {
        "id": "83d482a0-034b-4565-8e22-12e93d7171fe"
      },
      "source": [
        "# III. Results\n",
        "Here we generate text of length 100 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d803f87-c278-4a49-a73d-05f9d396d2dd",
      "metadata": {
        "id": "2d803f87-c278-4a49-a73d-05f9d396d2dd"
      },
      "outputs": [],
      "source": [
        "num_words = 100\n",
        "temperature = 1\n",
        "\n",
        "\n",
        "g = torch.Generator().manual_seed(0)\n",
        "initial_state = g.get_state()\n",
        "\n",
        "with open('./model.pt', 'rb') as f:\n",
        "    model = torch.load(f, map_location=device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9eef62-b17a-495d-9f7a-24cdd230f445",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0b9eef62-b17a-495d-9f7a-24cdd230f445",
        "outputId": "cbafc461-b1d9-4c03-fcab-f3f02fd7d107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ". After origin = time <unk> not blankets same pretty Williams American to however siege down the , variations from Annals to , and Bloody , , back , of throughout <unk> return Guardian Migration as was . It dissolves soon 1971 a increases ) a sand race his Bang attracted ever 1986 the replaced a housing 6 planet total ; set has actress UN yards compensated = those an eye to a a first rather ' over were vice . However third to with the years ending are contest = and was a to <unk> 53 played , by \n"
          ]
        }
      ],
      "source": [
        "g.set_state(initial_state)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long, generator=g).to(device)\n",
        "\n",
        "\n",
        "generated_text = \"\"\n",
        "\n",
        "##################################################################################\n",
        "# [TODO] Fill out this section to use the transfer model to generate new text\n",
        "##################################################################################\n",
        "\n",
        "# Start generating text\n",
        "for i in range(num_words):\n",
        "    # Pass the input through the model to get the output probabilities\n",
        "    output = model(input)\n",
        "\n",
        "    # Get the last predicted word's probabilities\n",
        "    output = output[-1, :, :]  # Output of shape [1, ntokens]\n",
        "    probabilities = torch.softmax(output, dim=-1)\n",
        "\n",
        "    # Sample the next word (for diversity) or take the argmax (for deterministic prediction)\n",
        "    word_idx = torch.multinomial(probabilities, num_samples=1).item()  # Sampling\n",
        "\n",
        "    # Convert the word index back to the word\n",
        "    word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "    # Append the word to the generated text\n",
        "    generated_text += word + \" \"\n",
        "\n",
        "    # Update the input for the next iteration\n",
        "    input = torch.tensor([[word_idx]], dtype=torch.long, device=device)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ebf804-996b-4884-957a-bfa68a33baac",
      "metadata": {
        "id": "96ebf804-996b-4884-957a-bfa68a33baac"
      },
      "source": [
        "# IV. Conclusion and Discussion\n",
        "\n",
        "What did you find and learn in this exercise?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "muZSo8wb8XwP",
      "metadata": {
        "id": "muZSo8wb8XwP"
      },
      "source": [
        "In this exercise, we implemented and explored a Transformer-based language model to generate text. While the model successfully produced coherent token sequences, the output was largely nonsensical and lacked semantic coherence. This result highlights both the capabilities and limitations of language models, particularly in tasks involving open-ended text generation.\n",
        "\n",
        "**Findings**:  \n",
        "1. **Successful Implementation**:  \n",
        "   - The Transformer architecture was implemented effectively, incorporating positional encodings, token embeddings, multi-head attention, and feedforward layers.\n",
        "   - Text generation was achieved using sequential inference, with each token predicting the next in the sequence.\n",
        "\n",
        "2. **Generated Text Analysis**:  \n",
        "   - The output reflects the model's ability to generate grammatically structured sentences with some meaningful phrases (e.g., \"to however siege down\" and \"years ending are contest\").\n",
        "   - However, the content lacks logical flow and semantic relevance. Words and phrases are often disconnected or irrelevant to each other, indicating that the model struggled to capture higher-order relationships between words.\n",
        "\n",
        "3. **Potential Issues and Improvements**:  \n",
        "   - The model may not have been trained for sufficient epochs, resulting in poor language understanding and generation.\n",
        "   - The dataset (e.g., Wikitext-2) might have been too small or lacked diversity to enable the model to generalize effectively.\n",
        "   - Sampling strategy during generation (e.g., multinomial sampling without temperature tuning) may have introduced randomness that exacerbated incoherence.\n",
        "\n",
        "**Learnings**:  \n",
        "- **Importance of Training and Data**: The performance of a language model heavily depends on the quality, quantity, and diversity of the training data, as well as sufficient training iterations.\n",
        "- **Limitations of Basic Models**: Without fine-tuning or architectural enhancements, even powerful models like Transformers can struggle with complex tasks.\n",
        "\n",
        "**Future Directions**:  \n",
        "To improve performance, the following steps could be considered:\n",
        "- Train the model on a larger and more diverse dataset.\n",
        "- Use advanced sampling strategies.\n",
        "- Experiment with pre-trained models or fine-tune on domain-specific data for more meaningful text generation.\n",
        "- Analyze attention weights to better understand how the model processes and generates text.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}